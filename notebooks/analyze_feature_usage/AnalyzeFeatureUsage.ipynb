{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Feature Usage\n",
    "\n",
    "In this notebook we'll use Certifai to generate counterfactual explanations of two models' predictions for a given\n",
    "dataset.  By construction these will be data points optimized to be close to the model's decision boundary under\n",
    "a normalizing constraint of sparsity in changed features over the corresponding original data points.\n",
    "\n",
    "We will then analyze these counterfactual differences to obtain a frequency of occurrence of each feature in them,\n",
    "which is plotted as a histogram for each of two comparative models built on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue,\n",
    "                                      CertifaiFeatureRestriction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special import - \n",
    "# for multiprocessing to work in a Notebook,  pickled classes must be in a separate package or notebook\n",
    "# hence, the encoder class has to be somewhere other than the current notebook\n",
    "# from ipynb.fs.defs.cat_encoder import CatEncoder # <- doesn't work on Azure Notebooks\n",
    "# %run cat_encoder.py # <- doesn't work because code doesn't remain external\n",
    "\n",
    "# Azure Notebooks workaround - \n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from cat_encoder import CatEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example will use a simple logistic classifier on the German Credit dataset\n",
    "base_path = '..'\n",
    "all_data_file = f\"{base_path}/datasets/german_credit_eval.csv\"\n",
    "\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "cat_columns = [\n",
    "    'checkingstatus',\n",
    "    'history',\n",
    "    'purpose',\n",
    "    'savings',\n",
    "    'employ',\n",
    "    'status',\n",
    "    'others',\n",
    "    'property',\n",
    "    'age',\n",
    "    'otherplans',\n",
    "    'housing',\n",
    "    'job',\n",
    "    'telephone',\n",
    "    'foreign'\n",
    "    ]\n",
    "\n",
    "label_column = 'outcome'\n",
    "\n",
    "# Separate outcome\n",
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Note - to support python multi-processing in the context of a notebook the encoder MUST\n",
    "# be in a separate file, which is why `CatEncoder` is defined outside of this notebook\n",
    "encoder = CatEncoder(cat_columns, X)\n",
    "\n",
    "# Fit a classification models (note - no train/test split here currently as I'm just using the same data as\n",
    "# the scan will)\n",
    "logistic_model = LogisticRegression(random_state=0, solver=\"lbfgs\", max_iter=1000)\n",
    "logistic_model.fit(encoder(X_train.values), y_train.values)\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "dtree_model.fit(encoder(X_train.values), y_train.values)\n",
    "\n",
    "# Assess on the test set\n",
    "logistic_accuracy = logistic_model.score(encoder(X_test.values), y_test.values)\n",
    "print(f\"Logistic classifier model accuracy on test data is {logistic_accuracy}\")\n",
    "dtree_accuracy = dtree_model.score(encoder(X_test.values), y_test.values)\n",
    "print(f\"Decision tree model accuracy on test data is {dtree_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model up for use by Certifai as a local model\n",
    "logistic_model_proxy = CertifaiPredictorWrapper(logistic_model, encoder=encoder)\n",
    "dtree_model_proxy = CertifaiPredictorWrapper(dtree_model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scan to extract counterfactual explanations of the model predictions and build\n",
    "# a histogram of the feature usage in those counterfactuals\n",
    "\n",
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='Loan granted', favorable=True),\n",
    "        CertifaiOutcomeValue(2, name='Loan denied')\n",
    "    ]),\n",
    "    prediction_description='Determine whether a loan should be granted')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "# Add our local models\n",
    "first_model = CertifaiModel('logistic',\n",
    "                            local_predictor=logistic_model_proxy)\n",
    "scan.add_model(first_model)\n",
    "second_model = CertifaiModel('dtree',\n",
    "                            local_predictor=dtree_model_proxy)\n",
    "scan.add_model(second_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.csv(all_data_file))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "# don't allow changes to the nationality status\n",
    "scan.add_feature_restriction('foreign', CertifaiFeatureRestriction.constant())\n",
    "\n",
    "# Setup an evaluation for explanation on the above dataset using the model\n",
    "scan.add_evaluation_type('explanation')\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "# For this analysis we'll generate explanations for the entire dataset so we have a good number\n",
    "# on which to base statistical measures\n",
    "scan.explanation_dataset_id = 'evaluation'\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = 'outcome'\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a dictionary keyed on analysis, containing reports keyed on model id\n",
    "# The console app is the recommended way to view these, by saving the results to file\n",
    "# (see previous cell), but programmatic analysis of the result here is also possible\n",
    "\n",
    "# Here we'll extract the frequency of feature usage in generated counterfactuals for each model\n",
    "def get_feature_frequency(model_id):\n",
    "    # Extract the information for fairness of a particular model id\n",
    "    local_model_explanation_info = result['explanation'][model_id]\n",
    "    # Extract the full set of counterfactuals for this\n",
    "    all_counterfactuals = [ind for r in local_model_explanation_info['explanations'] for ind in r['bestIndividuals']]\n",
    "\n",
    "    def features_changed(counterfactual):\n",
    "        # Each feature has an entry saying how it changed.  This will be one of:\n",
    "        #   'unchanged'\n",
    "        #   'changed' (categorical change)\n",
    "        #   <numeric> (differnce from original value for numeric feaure)\n",
    "        def no_change(diff):\n",
    "            return (diff == 'unchanged') or diff == 0\n",
    "\n",
    "        diffs = counterfactual['diff']\n",
    "        return [idx for idx in range(len(diffs)) if not no_change(diffs[idx])]\n",
    "\n",
    "\n",
    "    # Get the full list of model features from the schema\n",
    "    features = local_model_explanation_info['model_schema']['feature_schemas']\n",
    "    num_model_features = len(features)\n",
    "\n",
    "    feature_names = np.array([f['name'] for f in features])\n",
    "    \n",
    "    # Count the changes for each feature across the dataset\n",
    "    all_changes = np.zeros(num_model_features)\n",
    "    for cf in all_counterfactuals:\n",
    "        changed = features_changed(cf)\n",
    "        for idx in changed:\n",
    "            all_changes[idx] += 1\n",
    "    return all_changes, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of frequency of occurrence of changes to each feature in counterfactuals\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(ax, model_id):\n",
    "    all_changes, feature_names = get_feature_frequency(model_id)\n",
    "    indexes = np.arange(len(all_changes))\n",
    "    order = np.argsort(-all_changes)\n",
    "\n",
    "    ax.bar(indexes,all_changes[order])\n",
    "\n",
    "    ax.ylabel = 'Frequency'\n",
    "    ax.set_title(f'Model: {model_id}')\n",
    "    ax.set_xticks(indexes)\n",
    "    ax.set_xticklabels(feature_names[order], rotation=90)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[15,6])\n",
    "fig.suptitle('Feature occurrence frequency by model', fontsize=20)\n",
    "\n",
    "plot_histogram(ax1, 'logistic')\n",
    "plot_histogram(ax2, 'dtree')\n",
    "\n",
    "# Put them both on the same scale\n",
    "ylim = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "ax1.set_ylim(top=ylim)\n",
    "ax2.set_ylim(top=ylim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
