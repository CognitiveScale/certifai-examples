{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Cortex Certifai Scan on Azure ML Model with authentication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENTS\n",
    "\n",
    "1. In this tutorial we will create sklearn models to classify [german credit loan risk](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29) (predict whether loan will be granted or not)\n",
    "\n",
    "2. Register model and deploy as webservice in ACI (AZURE CONTAINER INSTANCE) with authentication\n",
    "\n",
    "3. Test the deployed webservice\n",
    "\n",
    "4. Run Certifai Scan on the model deployed as webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the \n",
    "[configuration-notebook](https://github.com/Azure/MachineLearningNotebooks/blob/c520bd1d4130d9a01ee46e0937459e2de95d15ec/configuration.ipynb) to create an Azure workspace. Creating local and remote environments/dependencies will be covered in the notebook\n",
    "\n",
    "**PleaseNote**: to step through this notebook, make sure you have necessary dependencies installed locally\n",
    "\n",
    "- python>=3.6.2,<3.7\n",
    "- scikit-learn=0.20.3\n",
    "- numpy=1.16.2\n",
    "- pandas\n",
    "- azureml-sdk=1.4.0\n",
    "- ipython\n",
    "- matplotlib\n",
    "- jupyter\n",
    "\n",
    "You can also use [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) to create the local environment using the `certifai_azure_model_env.yml` file provided with the notebook\n",
    "\n",
    "Open your favorite terminal and cd into folder where this notebook is located to execute the below commands\n",
    "\n",
    "- `conda env create -f certifai_azure_model_env.yml` : will create local conda env with the necessary python packages for working through the notebook\n",
    "- `jupyter-notebook` : to launch jupyter notebook sesssion. \n",
    "\n",
    "\n",
    "**Note**: Installing `Cortex-Certifai` packages will be covered separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Cortex Certifai Toolkit path\n",
    "- update the `certifai_toolkit_path` to point your downloaded toolkit\n",
    "- this will be used later to install cortex certifai python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "certifai_toolkit_path = f'{home}/Downloads/certifai_toolkit_v1.2.13'\n",
    "certifai_toolkit_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a [german credit](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29) prediction model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports for model building and persistance \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test to confirm correct version of scikit-learn and numpy are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sklearn_version_test\n",
    "assert sklearn_version_test.__version__ == '0.20.3', 'scikit-learn version mismatch, `pip install scikit-learn==0.20.3` to install right sklearn version for this notebook'\n",
    "assert np.__version__                   == '1.16.2', 'numpy version mismatch, `pip install numpy==1.16.2` to install right numpy version for this notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special import - \n",
    "# for multiprocessing to work in a Notebook,  pickled classes must be in a separate package or notebook\n",
    "# hence, the model encoder class has to be somewhere other than the current notebook\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from cat_encoder import CatEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into memory\n",
    "df = pd.read_csv('../datasets/german_credit_eval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\n",
    "    'checkingstatus',\n",
    "    'history',\n",
    "    'purpose',\n",
    "    'savings',\n",
    "    'employ',\n",
    "    'status',\n",
    "    'others',\n",
    "    'property',\n",
    "    'age',\n",
    "    'otherplans',\n",
    "    'housing',\n",
    "    'job',\n",
    "    'telephone',\n",
    "    'foreign'\n",
    "    ]\n",
    "\n",
    "label_column = 'outcome'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset into the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode and scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CatEncoder(cat_columns, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build and train model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data, name, model_family, test=None):\n",
    "    if test is None:\n",
    "        test = data\n",
    "        \n",
    "    if model_family == 'SVM':\n",
    "        parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.1, .5, 1, 2, 4, 10], 'gamma':['auto']}\n",
    "        m = svm.SVC()\n",
    "    elif model_family == 'logistic':\n",
    "        parameters = {'C': (0.5, 1.0, 2.0), 'solver': ['lbfgs'], 'max_iter': [1000]}\n",
    "        m = LogisticRegression()\n",
    "    model = GridSearchCV(m, parameters, cv=3)\n",
    "    model.fit(data[0], data[1])\n",
    "\n",
    "    # Assess on the test data\n",
    "    accuracy = model.score(test[0], test[1].values)\n",
    "    print(f\"Model '{name}' accuracy is {accuracy}\")\n",
    "    return model\n",
    "\n",
    "svm_model_name      = 'german_credit_svm'\n",
    "logistic_model_name = 'german_credit_logit'\n",
    "\n",
    "svm_model = build_model((encoder(X_train.values), y_train),\n",
    "                        svm_model_name,\n",
    "                        'SVM',\n",
    "                        test=(encoder(X_test.values), y_test))\n",
    "\n",
    "logistic_model = build_model((encoder(X_train.values), y_train),\n",
    "                        logistic_model_name,\n",
    "                        'logistic',\n",
    "                        test=(encoder(X_test.values), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dump the trained models (along with corresponding encoder object) to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder object is dumped(along with trained model) to apply same transformation during prediction\n",
    "def dump_model(model_name,model_obj,encoder_obj=encoder):\n",
    "    model_path = f'{model_name}.pkl'\n",
    "    model_obj = {\n",
    "        \"model\":model_obj,\n",
    "        \"encoder\":encoder_obj\n",
    "    }\n",
    "    joblib.dump(value=model_obj, filename=model_path)\n",
    "    print(f'model saved on disk {model_obj}')\n",
    "    return model_path\n",
    "\n",
    "# persist models to disk\n",
    "svm_model_disk_path      = dump_model(svm_model_name,svm_model)\n",
    "logistic_model_disk_path = dump_model(logistic_model_name,logistic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the section below we will:\n",
    "\n",
    "1. Configure Azure workspace\n",
    "2. Register models (built above) to the workspace\n",
    "3. Create a prediction environment in the remote Azure workspace (created above) and\n",
    "4. Deploy models (predict) as web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Initialize Azure workspace\n",
    "\n",
    "- Follow the instructions listed here [creating and managing azure-ml workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace) to create an azure-ml workspace\n",
    "\n",
    "**Once you have the workspace created easiest way to run through remaining steps is to download the `config.json` to the current directory and replace the exisiting config.json**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace%28class%29?view=azure-ml-py) object from the persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register models to created  workspace\n",
    "\n",
    "Register a file or folder as a model by calling [Model.register()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-).\n",
    "\n",
    "In addition to the content of the model file itself (model + scaler object), our registered model will also store model metadata like model description, tags, etc. -- that will be useful when managing and deploying models in our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "logistic_model_azure = Model.register(model_path=logistic_model_disk_path,\n",
    "                       model_name=logistic_model_name,\n",
    "                       tags={'area': \"banking credit risk\", 'type': \"classification\"},\n",
    "                       description=\"Logistic Classifier model to predict credit loan approval\",\n",
    "                       workspace=ws)\n",
    "\n",
    "svm_model_azure = Model.register(model_path=svm_model_disk_path,\n",
    "                       model_name=svm_model_name,\n",
    "                       tags={'area': \"banking credit risk\", 'type': \"classification\"},\n",
    "                       description=\"Support Vector Machine Classifier model to predict credit loan approval\",\n",
    "                       workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom prediction environment inside azure-ml workspace\n",
    "\n",
    "If we want control over how our model is run, if it uses another framework, or if it has special runtime requirements, we can instead specify our own environment and scoring method. Custom environments can be used for any model we want to deploy.\n",
    "\n",
    "Specify the model's runtime environment by creating an [Environment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment%28class%29?view=azure-ml-py) object and providing the [CondaDependencies](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py) needed by the model\n",
    "\n",
    "In this example we will create a conda environment for our german credit model from file **myenv.yml** and register it to our workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"myenv.yml\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "environment = Environment(\"german-credit-env\")\n",
    "environment.python.conda_dependencies = CondaDependencies(\"myenv.yml\")\n",
    "environment.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create Inference Configuration and deploy webservice\n",
    "\n",
    "**Inference Configuration** will contain:\n",
    "\n",
    "1. Scoring script\n",
    "2. Environment (created above)\n",
    "\n",
    "We create the scoring script, called **score.py**. The web service call uses this script to show how to use the model.\n",
    "\n",
    "We include below two required functions in the scoring script:\n",
    "\n",
    "1. The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started.\n",
    "\n",
    "2. The `run(data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are also supported.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Deploy the registered model in the custom environment by providing an [InferenceConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py) object to [Model.deploy()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config--deployment-config-none--deployment-target-none-). In this case we are also using the [AciWebservice.deploy_configuration()](https://docs.microsoft.com/python/api/azureml-core/azureml.core.webservice.aci.aciwebservice#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none--dns-name-label-none--) method to generate a custom deploy configuration\n",
    "        \n",
    "**Note**: This step can take several minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../utils/svm_score.py') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "inference_config_logistic = InferenceConfig(entry_script=\"logistic_score.py\",\n",
    "                                   environment=environment,source_directory=\"../utils\")\n",
    "inference_config_svm = InferenceConfig(entry_script=\"svm_score.py\",\n",
    "                                   environment=environment,source_directory=\"../utils\")\n",
    "\n",
    "logistic_service_name = 'german-credit-logistic-service'\n",
    "svm_service_name = 'german-credit-svm-service'\n",
    "\n",
    "aci_deployment_config = AciWebservice.deploy_configuration(auth_enabled=True)\n",
    "\n",
    "# Remove any existing services under the same name.\n",
    "try:\n",
    "    Webservice(ws, logistic_service_name).delete()\n",
    "except WebserviceException:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    Webservice(ws, svm_service_name).delete()\n",
    "except WebserviceException:\n",
    "    pass\n",
    "\n",
    "\n",
    "service_logistic = Model.deploy(ws, logistic_service_name, [logistic_model_azure],inference_config=inference_config_logistic,deployment_config=aci_deployment_config)\n",
    "service_svm      = Model.deploy(ws, svm_service_name,      [svm_model_azure],     inference_config=inference_config_svm,     deployment_config=aci_deployment_config)\n",
    "service_logistic.wait_for_deployment(show_output=True)\n",
    "service_svm.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice\n",
    "\n",
    "1. Get the webservice endpoint using `service.scoring_uri` :: string\n",
    "2. Get the authentication headers usinh `service.get_keys()` :: tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_logistic_uri  = service_logistic.scoring_uri\n",
    "service_logistic_keys = service_logistic.get_keys()\n",
    "\n",
    "service_svm_uri       = service_svm.scoring_uri\n",
    "service_svm_keys      = service_svm.get_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json test data sample(from csv)\n",
    "\n",
    "import json\n",
    "sample_input = json.dumps({\n",
    "\"payload\": {\n",
    "    \"instances\": [\n",
    "        [\n",
    "            \"... < 0 DM\",\n",
    "            6,\n",
    "            \"critical account/ other credits existing (not at this bank)\",\n",
    "            \"radio/television\",\n",
    "            1169,\n",
    "            \"unknown/ no savings account\",\n",
    "            \".. >= 7 years\",\n",
    "            4,\n",
    "            \"male : single\",\n",
    "            \"others - none\",\n",
    "            4,\n",
    "            \"real estate\",\n",
    "            \"> 25 years\",\n",
    "            \"none\",\n",
    "            \"own\",\n",
    "            2,\n",
    "            \"skilled employee / official\",\n",
    "            1,\n",
    "            \"phone - yes, registered under the customers name\",\n",
    "            \"foreign - yes\"\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "})\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {service_svm_keys[0]}'          \n",
    "          }\n",
    "\n",
    "response = requests.post(\n",
    "    service_svm_uri, data=sample_input, headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.elapsed)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Cortex Certifai Scan locally\n",
    "\n",
    "1. Install the cortex certifai packages required to initiate model scan\n",
    "\n",
    "2. Configure scan details and execute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Cortex Certifai python packages\n",
    "\n",
    "initiating a Cortex Certifai scan requires following python packages to be installed in the current local environment\n",
    "\n",
    "`required-packages`\n",
    "\n",
    "- cortex-certifai-scanner\n",
    "- cortex-certifai-engine\n",
    "- cortex-certifai-common\n",
    "\n",
    "`optional-packages`\n",
    "\n",
    "- cortex-certifai-client\n",
    "- cortex-certifai-console\n",
    "\n",
    "Download [certifai toolkit](https://www.cognitivescale.com/download-certifai) and follow instructions in the `Readme.md` to install the python-packages in the current environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required certifai packages (optional packages are left for user to install)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find $certifai_toolkit_path/packages/all       -type f ! -name \"*console-*\" ! -name \"*client-*\" | xargs -I % sh -c 'pip install % ' ;\n",
    "!find $certifai_toolkit_path/packages/python3.6 -type f   -name \"*engine-*\"                      | xargs -I % sh -c 'pip install % ' ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure certifai package was installed correctly\n",
    "from certifai.scanner.version import get_version\n",
    "get_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cortex Certifai python-package to launch a scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for creating a scan\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define cortex certifai task type\n",
    "\n",
    "- `CertifaiTaskOutcomes` : cortex certifai supports classification as well as regression models. here we have an example of binary-classification (e.g. predict whether loan should be granted or not)\n",
    "- `CertifaiOutcomeValue` : define the different outcomes possible from the model predictions. here we have a model that predicts either 1(loan granted) or 2(loan denied)\n",
    "\n",
    "**Note**: Please refer to [Certifai Api Docs](https://cognitivescale.github.io/cortex-certifai/certifai-api-ref/certifai.scanner.builder.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scan object from scratch using the ScanBuilder class with tasks and outcomes\n",
    "\n",
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='Loan granted', favorable=True),\n",
    "        CertifaiOutcomeValue(2, name='Loan denied')\n",
    "    ]),\n",
    "    prediction_description='Determine whether a loan should be granted')\n",
    "\n",
    "#  create a certifai scan object and add the certifai task created above\n",
    "scan = CertifaiScanBuilder.create('model_auth_demo',\n",
    "                                  prediction_task=task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add logistic and svm models (created above) to scan object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Certifai Model Object using the web service (from earlier) by passing the deployed web service url\n",
    "first_model = CertifaiModel('SVM',\n",
    "                            predict_endpoint=service_svm_uri)\n",
    "scan.add_model(first_model)\n",
    "\n",
    "second_model = CertifaiModel('logistic',\n",
    "                            predict_endpoint=service_logistic_uri)\n",
    "scan.add_model(second_model)\n",
    "\n",
    "# Add corresponding model headers for service authentication and content-type\n",
    "\n",
    "# add the default headers applicable to all models\n",
    "scan.add_model_header(header_name='Content-Type',header_value='application/json')\n",
    "\n",
    "# add defined headers corresponding to auth keys for respective model services\n",
    "scan.add_model_header(header_name='Authorization', header_value=f'Bearer {service_svm_keys[0]}', model_id='SVM')\n",
    "scan.add_model_header(header_name='Authorization', header_value=f'Bearer {service_logistic_keys[0]}', model_id='logistic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the evaluation dataset to scan object\n",
    "\n",
    "- `evaluation dataset` dataset to be used by cortex certifai to evaluate the model against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an evaluation object and pass the evaluation dataset(csv) here \n",
    "german_credit_eval_data_file = \"../utils/german_credit_eval.csv\"\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.csv(german_credit_eval_data_file))\n",
    "scan.add_dataset(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating model fairness \n",
    "\n",
    "- add `fairness` as evaluation type to scan object\n",
    "- create an `evaluation_dataset_id` to refer to added evaluation datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup an evaluation for fairness on the above dataset using the model\n",
    "# We'll look at disparity between groups defined by marital status and age\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('status'))\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.evaluation_dataset_id = 'evaluation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = 'outcome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analayze Fairness Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a dictionary keyed on analysis, containing reports keyed on model id\n",
    "# The console app is the recommended way to view these, by saving the results to file\n",
    "# (see previous cell), but programmatic analysis of the result here is also possible\n",
    "print(type(result['fairness']['SVM']['fairness']['secondary_scores']))\n",
    "df = construct_scores_dataframe(scores('fairness', result), include_confidence=False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  fairness by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many scores also come with 95% confidence bounds, which we omitted above for the sake of brevity, but\n",
    "# we can include those also.  In the example here we include the confidence bounds but only display the scores\n",
    "# to a reduced level of detail to keep a smallish table for display purposes\n",
    "df = construct_scores_dataframe(scores('fairness', result, max_depth=1))\n",
    "display(df)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Let's chart the fairness measure by feature for each model together with its confidence bounds for\n",
    "# easier visual comparison:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['Feature (status)', 'Feature (age)']\n",
    "feature_scores = df[features]\n",
    "feature_lower_bounds = df[[f + ' lower bound' for f in features]]\n",
    "feature_upper_bounds = df[[f + ' upper bound' for f in features]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "ax.set_title('Feature fairness by model', fontsize=20)\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "ax.set_xticks(np.arange(len(features))+width)\n",
    "ax.set_xticklabels(features)\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    central_values = list(feature_scores.iloc[idx])\n",
    "    lower_bounds = list(feature_lower_bounds.iloc[idx])\n",
    "    upper_bounds = list(feature_upper_bounds.iloc[idx])\n",
    "    lower_errors = [central_values[i] - lower_bounds[i] for i in range(len(central_values))]\n",
    "    upper_errors = [upper_bounds[i] - central_values[i] for i in range(len(central_values))]\n",
    "\n",
    "    ax.bar([width/2+idx*width+f_idx for f_idx in range(len(features))],\n",
    "            central_values,\n",
    "            width,\n",
    "            yerr=[lower_errors, upper_errors],\n",
    "            color=colors[idx],\n",
    "            label=df.index[idx],\n",
    "            capsize=10)\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Resource Cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this tutorial we\n",
    " - created and registered `logistic_model_azure` and `svm_model_azure` models to our Azure workspace\n",
    " - created `german-credit-logistic-service` and `german-credit-svm-service` ACI (Azure Container Instance) webservices \n",
    "\n",
    "- Once Cortex Certifai evaluation is complete, make sure to clear all azure resources in order to avoid cost\n",
    "- Follow the [Azure Ml resource cleanup docs][1] to remove all resources created above\n",
    "\n",
    "[1]:https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-train#clean-up-resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
