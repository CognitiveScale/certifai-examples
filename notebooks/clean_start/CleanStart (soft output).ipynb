{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a scan programatically\n",
    "\n",
    "In this notebook we'll build up a scan definition from first principles, against a local model trained within the\n",
    "notebook.  We will then run that scan and save its results.  Finally we will extract the scan defintion as YAML, which could be used to run the same scan (potentially on revised models or datasets) via the Certifai stand-alone scanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from copy import copy\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special import - \n",
    "# for multiprocessing to work in a Notebook,  pickled classes must be in a separate package or notebook\n",
    "# hence, the encoder class has to be somewhere other than the current notebook\n",
    "# from ipynb.fs.defs.cat_encoder import CatEncoder # <- doesn't work on Azure Notebooks\n",
    "# %run cat_encoder.py # <- doesn't work because code doesn't remain external\n",
    "\n",
    "# Azure Notebooks workaround - \n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from cat_encoder import CatEncoder\n",
    "from sklearn_soft_wrapper import SkLearnSoftWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example will use a simple logistic classifier on the German Credit dataset\n",
    "base_path = '..'\n",
    "all_data_file = f\"{base_path}/datasets/german_credit_eval.csv\"\n",
    "\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "cat_columns = [\n",
    "    'checkingstatus',\n",
    "    'history',\n",
    "    'purpose',\n",
    "    'savings',\n",
    "    'employ',\n",
    "    'status',\n",
    "    'others',\n",
    "    'property',\n",
    "    'age',\n",
    "    'otherplans',\n",
    "    'housing',\n",
    "    'job',\n",
    "    'telephone',\n",
    "    'foreign'\n",
    "    ]\n",
    "\n",
    "label_column = 'outcome'\n",
    "\n",
    "# Separate outcome\n",
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Note - to support python multi-processing in the context of a notebook the encoder MUST\n",
    "# be in a separate file, which is why `CatEncoder` is defined outside of this notebook\n",
    "encoder = CatEncoder(cat_columns, X)\n",
    "\n",
    "def build_model(data, name, model_family, test=None):\n",
    "    if test is None:\n",
    "        test = data\n",
    "        \n",
    "    if model_family == 'SVM':\n",
    "        parameters = {'probability': [True], 'kernel':('linear', 'rbf', 'poly'), 'C':[0.1, .5, 1, 2, 4, 10], 'gamma':['auto']}\n",
    "        m = svm.SVC()\n",
    "    elif model_family == 'logistic':\n",
    "        parameters = {'C': (0.5, 1.0, 2.0), 'solver': ['lbfgs'], 'max_iter': [1000]}\n",
    "        m = LogisticRegression()\n",
    "    model = GridSearchCV(m, parameters, cv=3)\n",
    "    model.fit(data[0], data[1])\n",
    "\n",
    "    # Assess on the test data\n",
    "    accuracy = model.score(test[0], test[1].values)\n",
    "    print(f\"Model '{name}' accuracy is {accuracy}\")\n",
    "    return model\n",
    "\n",
    "svm_model = build_model((encoder(X_train.to_numpy()), y_train),\n",
    "                        'Support Vector Machine',\n",
    "                        'SVM',\n",
    "                        test=(encoder(X_test.to_numpy()), y_test))\n",
    "\n",
    "logistic_model = build_model((encoder(X_train.to_numpy()), y_train),\n",
    "                        'Logistic classifier',\n",
    "                        'logistic',\n",
    "                        test=(encoder(X_test.to_numpy()), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the models using their soft outputs for Certifai\n",
    "\n",
    "def _wrap_sklearn_classifier(classifier, encoder=None) -> CertifaiPredictorWrapper:\n",
    "    return CertifaiPredictorWrapper(SkLearnSoftWrapper(classifier),\n",
    "                                    soft_predictions=True,\n",
    "                                    encoder=encoder,\n",
    "                                    label_ordering=classifier.classes_)\n",
    "\n",
    "svm_model_proxy = _wrap_sklearn_classifier(svm_model, encoder=encoder)\n",
    "logistic_model_proxy = _wrap_sklearn_classifier(logistic_model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scan object from scratch using the ScanBuilder class\n",
    "\n",
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='Loan granted', favorable=True),\n",
    "        CertifaiOutcomeValue(2, name='Loan denied')\n",
    "    ]),\n",
    "    prediction_description='Determine whether a loan should be granted')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "\n",
    "# Add our local models\n",
    "first_model = CertifaiModel('SVM',\n",
    "                            local_predictor=svm_model_proxy)\n",
    "scan.add_model(first_model)\n",
    "second_model = CertifaiModel('logistic',\n",
    "                            local_predictor=logistic_model_proxy)\n",
    "scan.add_model(second_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.csv(all_data_file))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "# Setup an evaluation for fairness on the above dataset using the model\n",
    "# We'll look at disparity between groups defined by marital status and age\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('status'))\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = 'outcome'\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a dictionary keyed on analysis, containing reports keyed on model id\n",
    "# The console app is the recommended way to view these, by saving the results to file\n",
    "# (see previous cell), but programmatic analysis of the result here is also possible\n",
    "print(result['fairness']['SVM'].keys())\n",
    "df = construct_scores_dataframe(scores('fairness', result), include_confidence=False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many scores also come with 95% confidence bounds, which we omitted above for the sake of brevity, but\n",
    "# we can include those also.  In the example here we include the confidence bounds but only display the scores\n",
    "# to a reduced level of detail to keep a smallish table for display purposes\n",
    "df = construct_scores_dataframe(scores('fairness', result, max_depth=1))\n",
    "\n",
    "display(df)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Let's chart the fairness measure by feature for each model together with its confidence bounds for\n",
    "# easier visual comparison:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['Feature (status)', 'Feature (age)']\n",
    "feature_scores = df[features]\n",
    "feature_lower_bounds = df[[f + ' lower bound' for f in features]]\n",
    "feature_upper_bounds = df[[f + ' upper bound' for f in features]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "ax.set_title('Feature fairness by model', fontsize=20)\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "ax.set_xticks(np.arange(len(features))+width)\n",
    "ax.set_xticklabels(features)\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    central_values = list(feature_scores.iloc[idx])\n",
    "    lower_bounds = list(feature_lower_bounds.iloc[idx])\n",
    "    upper_bounds = list(feature_upper_bounds.iloc[idx])\n",
    "    lower_errors = [central_values[i] - lower_bounds[i] for i in range(len(central_values))]\n",
    "    upper_errors = [upper_bounds[i] - central_values[i] for i in range(len(central_values))]\n",
    "\n",
    "    ax.bar([width/2+idx*width+f_idx for f_idx in range(len(features))],\n",
    "            central_values,\n",
    "            width,\n",
    "            yerr=[lower_errors, upper_errors],\n",
    "            color=colors[idx],\n",
    "            label=df.index[idx],\n",
    "            capsize=10)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also persist the template we built and reuse it later as illustrated here\n",
    "import tempfile\n",
    "import yaml\n",
    "\n",
    "temp_file = tempfile.mktemp()\n",
    "with open(temp_file, \"w\") as f:\n",
    "    scan.save(f)\n",
    "    print(f\"Saved template to: {temp_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = CertifaiScanBuilder.from_file(temp_file)\n",
    "\n",
    "# The persisted scan won't contain the model itself, so we'll need to reconnect that\n",
    "for model in reloaded.models:\n",
    "    if model.id == 'logistic':\n",
    "        model.local_predictor=logistic_model_proxy\n",
    "    elif model.id == 'SVM':\n",
    "        model.local_predictor=svm_model_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = reloaded.run(write_reports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_scores_dataframe(scores('fairness', result), include_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also process dataframes we have locally already loaded in Pandas rather than\n",
    "# requiring them to initially be saved off as CSV/JSON files\n",
    "df2 = pd.read_csv(all_data_file)\n",
    "\n",
    "# Add this already loaded data as a new dataset and switch to evaluate it\n",
    "loaded_dataset = CertifaiDataset('loaded', CertifaiDatasetSource.dataframe(df2))\n",
    "reloaded.add_dataset(loaded_dataset)\n",
    "reloaded.evaluation_dataset_id = 'loaded'\n",
    "reloaded.output_path = '../local_reports'\n",
    "\n",
    "result = reloaded.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_scores_dataframe(scores('fairness', result), include_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having developed a scan definition we'll eventually want to be able to export it for use in\n",
    "# scans of a deployed model.  The main differences then will be:\n",
    "#  * The model won't be local to the scanner, but will be accessed via the network\n",
    "#  * The dataset won't be in a local DataFrame but will always be read from (possibly network attached) storage\n",
    "# This means that the scan template used by the scanner will need dataset sourcing details filling in, and model\n",
    "# endpoint address specifying.  This can either be done by a post-hoc hand edit of the produced YAML (the file we\n",
    "# saved off earlier to illustrate persisting the template), or we can add those details programatically prior to\n",
    "# exporting as illustrated below\n",
    "exportable_scan = CertifaiScanBuilder.from_file(temp_file)  # reload the scan we've been using in the notebook\n",
    "exportable_scan.models[0].predict_endpoint = 'http://mymodel/predict'\n",
    "exportable_scan.datasets[0].source = CertifaiDatasetSource.csv('somefile.csv')\n",
    "\n",
    "# Export the updated scan as YAML for use by the scanner (we won't actually save it here - just print it)\n",
    "scan_yaml = exportable_scan.extract_yaml() # could use the method `save` to write direct to a file\n",
    "print(scan_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
