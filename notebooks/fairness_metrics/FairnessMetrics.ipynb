{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using different fairness metrics\n",
    "\n",
    "In this notebook we'll build up a scan definition to perform multiple fairness analyses other than the burden-based\n",
    "default used by the Certifai counterfcatual framework.\n",
    "\n",
    "Specifically we'll look at two widely used measures of fairness detailed below.  For the purposes of discussion we will define things in terms of the following random variables:\n",
    "\n",
    "* `X` - the input to the model (i.e. - the features)\n",
    "* `C` - the class predicted by the model\n",
    "* `Y` - the actual (ground truth) class\n",
    "* `G` - the protected group membership.  Note that if the protected grouping feature is included in the data given to the model then for some deterministic `f`, `G = f(X)`.  However, this need not be the case (when it is not the model is said the be trained with 'fairness by unawareness' (which has no theoretical guarantees in regard to actual achieved fairness)\n",
    "\n",
    "## Demographic Parity\n",
    "\n",
    "A predictor is said the be fair under *Demographic Parity* if the chances of a random member of each protected group\n",
    "has the same probability of the favorable prediction.  For example a predictor of whether some one is going to be\n",
    "offered a job, would be fair with respect to demographic parity and (simple binary) gender-based protected groups is it awarded jobs to men and women with equal likelihood.\n",
    "\n",
    "Techanically this may be stated as `C ⟂ G`\n",
    "\n",
    "This is a very intuitive measure, but note that it takes no account of ground truth (i.e. - whether under some objective measure a person *should* be awarded the job in this case).  As a result a 'perfect'predictor which is 100%\n",
    "accurate according to ground truth may not necessarily be fair under this metric.\n",
    "\n",
    "## Equal opportunity\n",
    "\n",
    "A predictor is said to be fair under *Equality of Opportunity* if the chances of a random member of each protected group *who should receive the favorable prediction according to ground truth* has the same probability of the favorable prediction.\n",
    "\n",
    "Technically this may be stated as `C ⟂ G | Y=1`\n",
    "\n",
    "This measure determines whether the predictive model is unbiased relative to a perfect decision process, and is always satisfied by a 100% accurate predictor.\n",
    "\n",
    "Both of the above measures are binary definitions of what it measn to be completely fair (under the respective definitions of fairness).  We will turn them into soft measures by looking at how close the model is to achieving them.  Specifically we will look at the measure for each class. These are empirical rate estimates of the probabilities, so for the two definitions above respectively:\n",
    "\n",
    "`P(C | G=g)` for each `g ∈ G`; and\n",
    "\n",
    "`P(C | Y=1, G=g)`\n",
    "\n",
    "The result is a list of values, one for each protected group, which we then measure the disparity between to provide\n",
    "a soft measure of fairness (all of them being equal implies perfect fairness).  The actual formula used is to turn the disparity into a [0, 1] measure (1 being perfectly fair) is:\n",
    "\n",
    "`(max(P) - min(P))/max(P ∪ (1 - P))`, using `P` as shorthand for the relevant conditional probabilities above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from copy import copy\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special import - \n",
    "# for multiprocessing to work in a Notebook,  pickled classes must be in a separate package or notebook\n",
    "# hence, the encoder class has to be somewhere other than the current notebook\n",
    "# from ipynb.fs.defs.cat_encoder import CatEncoder # <- doesn't work on Azure Notebooks\n",
    "# %run cat_encoder.py # <- doesn't work because code doesn't remain external\n",
    "\n",
    "# Azure Notebooks workaround - \n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('.')))\n",
    "from cat_encoder import CatEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example will use a simple logistic classifier on the German Credit dataset\n",
    "base_path = '..'\n",
    "all_data_file = f\"{base_path}/datasets/german_credit_eval.csv\"\n",
    "\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "cat_columns = [\n",
    "    'checkingstatus',\n",
    "    'history',\n",
    "    'purpose',\n",
    "    'savings',\n",
    "    'employ',\n",
    "    'status',\n",
    "    'others',\n",
    "    'property',\n",
    "    'age',\n",
    "    'otherplans',\n",
    "    'housing',\n",
    "    'job',\n",
    "    'telephone',\n",
    "    'foreign'\n",
    "    ]\n",
    "\n",
    "label_column = 'outcome'\n",
    "\n",
    "# Separate outcome\n",
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Note - to support python multi-processing in the context of a notebook the encoder MUST\n",
    "# be in a separate file, which is why `CatEncoder` is defined outside of this notebook\n",
    "encoder = CatEncoder(cat_columns, X)\n",
    "\n",
    "def build_model(data, name, model_family, test=None):\n",
    "    if test is None:\n",
    "        test = data\n",
    "        \n",
    "    if model_family == 'SVM':\n",
    "        parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.1, .5, 1, 2, 4, 10], 'gamma':['auto']}\n",
    "        m = svm.SVC()\n",
    "    elif model_family == 'logistic':\n",
    "        parameters = {'C': (0.5, 1.0, 2.0), 'solver': ['lbfgs'], 'max_iter': [1000]}\n",
    "        m = LogisticRegression()\n",
    "    model = GridSearchCV(m, parameters, cv=3)\n",
    "    model.fit(data[0], data[1])\n",
    "\n",
    "    # Assess on the test data\n",
    "    accuracy = model.score(test[0], test[1].values)\n",
    "    print(f\"Model '{name}' accuracy is {accuracy}\")\n",
    "    return model\n",
    "\n",
    "svm_model = build_model((encoder(X_train.values), y_train),\n",
    "                        'Support Vector Machine',\n",
    "                        'SVM',\n",
    "                        test=(encoder(X_test.values), y_test))\n",
    "\n",
    "logistic_model = build_model((encoder(X_train.values), y_train),\n",
    "                        'Logistic classifier',\n",
    "                        'logistic',\n",
    "                        test=(encoder(X_test.values), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model up for use by Certifai as a local model\n",
    "svm_model_proxy = CertifaiPredictorWrapper(svm_model, encoder=encoder)\n",
    "logistic_model_proxy = CertifaiPredictorWrapper(logistic_model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scan object\n",
    "\n",
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='Loan granted', favorable=True),\n",
    "        CertifaiOutcomeValue(2, name='Loan denied')\n",
    "    ]),\n",
    "    prediction_description='Determine whether a loan should be granted')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "\n",
    "# Add our local models\n",
    "first_model = CertifaiModel('SVM',\n",
    "                            local_predictor=svm_model_proxy)\n",
    "scan.add_model(first_model)\n",
    "second_model = CertifaiModel('logistic',\n",
    "                            local_predictor=logistic_model_proxy)\n",
    "scan.add_model(second_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.csv(all_data_file))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "# Setup an evaluation for fairness on the above dataset using the model\n",
    "# We'll look at disparity between groups defined by marital status and age\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('status'))\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "\n",
    "# Rather than Certifai's default burden-based fairness analysis of counterfactuals, we'll specify instead\n",
    "# some other measures of fairness.\n",
    "scan.add_fairness_metric('demographic parity')\n",
    "scan.add_fairness_metric('equal opportunity')\n",
    "\n",
    "# The primary metric selects the one that would be used as the fairness component in the ATX score\n",
    "scan.primary_fairness_metric = 'demographic parity'\n",
    "\n",
    "# For the purposes of this example we'll drop th default analysis that uses counterfactual burden\n",
    "scan.remove_fairness_metric('burden')\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = 'outcome'\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Extraction\n",
    "\n",
    "Having run the analysis, we now extract the detailed scores from the results as a DataFrame.  The columns have the following interpretation:\n",
    "\n",
    "* overall fairness - fairness of the model across all protected features (this is just a min over protected feature fairness - i.e. - `min(Feature(age), Feature(status)` in this case)\n",
    "* individual feature fairness (e.g. - `Feature (age)` - soft fairness measure with respect to a particular protected feature\n",
    "* Group details (&lt;group&gt;) - the actual empirical probability (rate) relevant to the fairness metric concerned for a specific group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the scores\n",
    "score_df = construct_scores_dataframe(scores('fairness', result), include_confidence=True)\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We can extract and plot summary and details.  Here we look at the soft fairness measure for each feature, and overall for the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize this to see how the two different measures compare for these models\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[12,4])\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "def plot_model_results(ax, title, data, lower, upper, add_labels=False):\n",
    "    ax.set_title(title, fontsize=20)\n",
    "   \n",
    "    metrics = data['type']\n",
    "    all_values = data.drop('type', axis=1)\n",
    "\n",
    "    ax.set_yticks(range(0,100,20))\n",
    "    ax.set_xticks(np.arange(len(all_values.columns))+width)\n",
    "    ax.set_xticklabels(all_values.columns)\n",
    "    \n",
    "    for idx in range(len(data)):\n",
    "        values = list(all_values.iloc[idx])\n",
    "        lowers = list(lower.iloc[idx])\n",
    "        uppers = list(upper.iloc[idx])\n",
    "        lower_errs = [values[i] - lowers[i] for i in range(len(lowers))]\n",
    "        upper_errs = [uppers[i] - values[i] for i in range(len(uppers))]\n",
    "        metric = metrics.iloc[idx]\n",
    "\n",
    "        ax.bar([width/2+idx*width+f_idx for f_idx in range(len(all_values.columns))],\n",
    "               values,\n",
    "               width,\n",
    "               color=colors[idx],\n",
    "               label=metric if add_labels else None,\n",
    "               yerr=[lower_errs, upper_errs],\n",
    "               capsize=10)\n",
    "\n",
    "\n",
    "displayed_stats = ['overall fairness', 'Feature (age)', 'Feature (status)']\n",
    "plot_columns = score_df[['type'] + displayed_stats]\n",
    "plot_columns.columns = ['type', 'overall', 'Age', 'Status']\n",
    "column_lower_bounds = score_df[[f + ' lower bound' for f in displayed_stats]]\n",
    "column_upper_bounds = score_df[[f + ' upper bound' for f in displayed_stats]]\n",
    "plot_model_results(ax1,\n",
    "                   \"Logistic Model\",\n",
    "                   plot_columns[score_df['context'] == 'logistic'],\n",
    "                   column_lower_bounds[score_df['context'] == 'logistic'],\n",
    "                   column_upper_bounds[score_df['context'] == 'logistic'],\n",
    "                   add_labels=True)\n",
    "plot_model_results(ax2,\n",
    "                   \"SVM Model\",\n",
    "                   plot_columns[score_df['context'] == 'SVM'],\n",
    "                   column_lower_bounds[score_df['context'] == 'SVM'],\n",
    "                   column_upper_bounds[score_df['context'] == 'SVM'])\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8)   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentary\n",
    "\n",
    "Both models fair reasonably well, with the SVM having a slight edge.  It is interesting to note that fairness is largely the same in terms of both the protected features we analyzed here for the SVM, but that the logistic regression model is considerably more biased on `status` than it is on `age`\n",
    "\n",
    "## Visualization of details\n",
    "\n",
    "Let's look at the detailed rates that lie behind the demographic parity measure.  for this binary clasification task this is essentially just the positive predictive rate of the model for each protected group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score details for each fairness type tell us the relevant rates (empirical probabilities) associated\n",
    "# with each group. For instance, for demographic parity this will be the positive prediction rate for the group\n",
    "\n",
    "# Let's plot this for the 'status' feature\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.3\n",
    "\n",
    "def plot_detail_results(ax, label, data, lower, upper, idx = 0):\n",
    "    all_values = data\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(all_values.columns))+width)\n",
    "    ax.set_xticklabels(all_values.columns)\n",
    "    \n",
    "    values = list(all_values.iloc[0])\n",
    "    lowers = list(lower.iloc[0])\n",
    "    uppers = list(upper.iloc[0])\n",
    "    lower_errs = [values[i] - lowers[i] for i in range(len(lowers))]\n",
    "    upper_errs = [uppers[i] - values[i] for i in range(len(uppers))]\n",
    "    \n",
    "    ax.bar([c + idx*width for c in range(len(all_values.columns))],\n",
    "           values,\n",
    "           width,\n",
    "           color=colors[idx],\n",
    "           label=label,\n",
    "           yerr = [lower_errs, upper_errs],\n",
    "           capsize=10)\n",
    "\n",
    "def plot_ground_truth(ax, data, feature_column, groups, offset, color, label):\n",
    "    rates = []\n",
    "    for g in groups:\n",
    "        group_data = df[df[feature_column] == g]\n",
    "        rates.append(sum(group_data['outcome'] == 1)/len(group_data))\n",
    "\n",
    "    ax.bar([c + offset for c in range(len(rates))],\n",
    "            rates,\n",
    "            width,\n",
    "            color=color,\n",
    "            label=label,\n",
    "            capsize=10)\n",
    "    \n",
    "ax.set_title(\"Positive predictive rates (status)\", fontsize=20)\n",
    "\n",
    "status_groups = [\n",
    "    'female : divorced/separated/married',\n",
    "    'male : divorced/separated',\n",
    "    'male : married/widowed',\n",
    "    'male : single'\n",
    "]\n",
    "plot_columns = score_df[[f'Group details ({x})' for x in status_groups]]\n",
    "plot_columns.columns = ['female', 'male prev married', 'male married', 'male single']\n",
    "column_lower_bounds = score_df[[f'Group details ({x}) lower bound' for x in status_groups]]\n",
    "column_upper_bounds = score_df[[f'Group details ({x}) upper bound' for x in status_groups]]\n",
    "plot_detail_results(ax,\n",
    "                    \"Logistic Model\",\n",
    "                    plot_columns[(score_df['context'] == 'logistic') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_lower_bounds[(score_df['context'] == 'logistic') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_upper_bounds[(score_df['context'] == 'logistic') & (score_df['type'] == 'demographic parity')])\n",
    "plot_detail_results(ax,\n",
    "                    \"SVM Model\",\n",
    "                    plot_columns[(score_df['context'] == 'SVM') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_lower_bounds[(score_df['context'] == 'SVM') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_upper_bounds[(score_df['context'] == 'SVM') & (score_df['type'] == 'demographic parity')],\n",
    "                    idx=1) \n",
    "plot_ground_truth(ax,\n",
    "                  df,\n",
    "                  'status',\n",
    "                  status_groups,\n",
    "                  width*2,\n",
    "                  colors[2],\n",
    "                  'ground truth')\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8)   \n",
    "\n",
    "plt.show()\n",
    "\n",
    "# And similarly for the 'age' feature\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "    \n",
    "ax.set_title(\"Positive predictive rates (age)\", fontsize=20)\n",
    "\n",
    "status_groups = [\n",
    "    '<= 25 years',\n",
    "    '> 25 years'\n",
    "]\n",
    "plot_columns = score_df[[f'Group details ({x})' for x in status_groups]]\n",
    "plot_columns.columns = status_groups\n",
    "column_lower_bounds = score_df[[f'Group details ({x}) lower bound' for x in status_groups]]\n",
    "column_upper_bounds = score_df[[f'Group details ({x}) upper bound' for x in status_groups]]\n",
    "plot_detail_results(ax,\n",
    "                    \"Logistic Model\",\n",
    "                    plot_columns[(score_df['context'] == 'logistic') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_lower_bounds[(score_df['context'] == 'logistic') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_upper_bounds[(score_df['context'] == 'logistic') & (score_df['type'] == 'demographic parity')])\n",
    "plot_detail_results(ax,\n",
    "                    \"SVM Model\",\n",
    "                    plot_columns[(score_df['context'] == 'SVM') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_lower_bounds[(score_df['context'] == 'SVM') & (score_df['type'] == 'demographic parity')],\n",
    "                    column_upper_bounds[(score_df['context'] == 'SVM') & (score_df['type'] == 'demographic parity')],\n",
    "                    idx=1) \n",
    "plot_ground_truth(ax,\n",
    "                  df,\n",
    "                  'age',\n",
    "                  status_groups,\n",
    "                  width*2,\n",
    "                  colors[2],\n",
    "                  'ground truth')\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8)   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "Some Interesting take-outs from the above:\n",
    "\n",
    "* We can readily see that the model if significantly more likely to give a positive predictions for the both the older age group, and males who have not gone through a divorce.\n",
    "\n",
    "* The positive rates for the ground truth are also not even, so this is a case where the perfect predictor would not itself be fair under demographic parity.  This implies that a major source of bias (under this definition) is the training data itself.\n",
    "\n",
    "* Both models systemically over-estimate the positive rate for *all* groups.  This is not in itself a fairness issue, but it is telling us something about a pathology in our modelling.\n",
    "\n",
    "* The discrepancy in positive predictive rates for both models is greater than that exhibited by the ground truth, so in both cases the models are amplifying the bias displayed by the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
