{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Cortex Certifai scan programatically\n",
    "\n",
    "In this notebook we'll build up a scan definition from first principles, against a local model trained within the notebook. We will then run that scan and save its results. \n",
    "\n",
    "- Load the dataset and model from disk\n",
    "- Create scan defintion using Certifai Builder Api\n",
    "- Refer to the [Cortex Certifai documentation](https://cognitivescale.github.io/cortex-certifai/docs/about) for detailed information about Cortex Certifai.\n",
    "- Refer to [Cortex Certifai Examples Github](https://github.com/CognitiveScale/cortex-certifai-examples) for more self start tutorials\n",
    "\n",
    "*Please Note*: this notebook assumes: \n",
    "- trained model and dataset is available within the the environment\n",
    "- Cortex Certifai toolkit and model dependencies are installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue, CertifaiModelMetric,\n",
    "                                      CertifaiFeatureDataType,CertifaiFeatureSchema, CertifaiDataSchema,CertifaiFeatureRestriction)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe\n",
    "from certifai.common.utils.encoding import CatEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version is 0.23.2\n",
      "numpy version is 1.19.1 \n",
      "Certifai version: 1.3.3\n",
      "Scanner build: 1.3.3-205-g3e430a75\n",
      "Console build: 1.3.3-283-g4906ac8\n"
     ]
    }
   ],
   "source": [
    "# check required packages and their version\n",
    "import sklearn as scikit\n",
    "print(f'sklearn version is {scikit.__version__}')\n",
    "print(f'numpy version is {np.__version__} ')\n",
    "!certifai --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load pre-trained model from disk along with encoder (if present)\n",
    "\n",
    "- replace model_path variable below to point to stored model binary on disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models/german_credit_multiclass.joblib\n",
      "Pipeline(steps=[('full_pipeline',\n",
      "                 Pipeline(steps=[('scaler', StandardScaler())])),\n",
      "                ('model',\n",
      "                 GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
      "                              param_grid={'C': (0.5, 1.0, 2.0),\n",
      "                                          'max_iter': [1000],\n",
      "                                          'solver': ['lbfgs']}))])\n"
     ]
    }
   ],
   "source": [
    "# load model \n",
    "model_path = 'models/german_credit_multiclass.joblib'\n",
    "print(f'loading {model_path}')\n",
    "try:\n",
    "    model = joblib.load(model_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(f'model `{model_path}` not found. Looks like model has not been trained or file location is wrong')\n",
    "    raise Exception(str(e))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../..'\n",
    "all_data_file = f\"{base_path}/datasets/german_credit_eval_multiclass_encoded.csv\"\n",
    "\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "one_hot_encoded_cat_cols = [\n",
    "         'checkingstatus',\n",
    "         'others',\n",
    "         'age',\n",
    "         'job',\n",
    "         'employ',\n",
    "         'property',\n",
    "         'foreign',\n",
    "         'history',\n",
    "         'savings',\n",
    "         'purpose',\n",
    "         'housing'       \n",
    "        ]\n",
    "\n",
    "target_encoded_cat_cols = ['otherplans', 'status', 'telephone']\n",
    "value_encoded_cat_cols =  []\n",
    "label_column = 'outcome'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  target_encoded string categorical to floating point encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and create one-hot feature-value mappings dict\n",
    "\n",
    "- our dataset has one-hot feature values encoded as dataset columns along with target_encoded categoricals\n",
    "- these one-hot encoded features present as column have column names delimited using `_` e.g. `age_<= 25 years`  `age_> 25 years` etc.\n",
    "- we create one hot value mappings for each  one-hot encoded feature below\n",
    "(e.g. `feature`: {`column_name_correponding_to_feature`: `features_value_in_column_for_given_feature`} )\n",
    "    ```\n",
    "    'age': {'age_<= 25 years': '<= 25 years', \n",
    "    'age_> 25 years': '> 25 years'}\n",
    "   ```\n",
    "- `one_hot_value_mappings` created above will be used later used to create `CertifaiDatasetSchema` \n",
    "> **Please Note**: this is only needed in-case dataset already has one-hot encoded feature as columns. Also if you already have the persisted mappings with same schema you can use that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "mappings = defaultdict(list)\n",
    "for col in df.columns.to_list():\n",
    "    col_list = []\n",
    "    feature_name,feature_value = col.split('_')[0], col.split('_')[1:]\n",
    "    if feature_name in one_hot_encoded_cat_cols:\n",
    "        if feature_value:\n",
    "            mappings[feature_name].append('_'.join(feature_value))\n",
    "\n",
    "# create a mapping from {feature -> {1-hot_column_name_in_csv: feature_value }} using the feature `mapping` list\n",
    "\n",
    "\"\"\"\n",
    "workclass:\n",
    "     workclass_Local-gov -> Local-gov\n",
    "     workclass_Self-emp-inc -> Self-emp-inc\n",
    "\"\"\"\n",
    "one_hot_value_mappings = {}\n",
    "for k,v in mappings.items():\n",
    "    one_hot_value_mappings[k] = { f'{k}_{cols}':cols  for cols in v} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct target encoder mappings \n",
    "- to generate target_mappings using `category_encoders.target_encoder.TargetEncoder` (from category-encoders package) `te` object we can use the below snippet\n",
    "```\n",
    "target_mappings = {}\n",
    "for feature in target_encode_cat_cols:\n",
    "    feature_mappings = {}\n",
    "    for ordinal_mapping in te.ordinal_encoder.category_mapping:\n",
    "        if ordinal_mapping['col'] == feature:\n",
    "            mapping = ordinal_mapping['mapping']\n",
    "            for idx, ordinal in enumerate(mapping):\n",
    "                label = mapping.index[idx]\n",
    "                if not (isinstance(label, float) and np.isnan(label)):\n",
    "                    feature_mappings[label] = te.mapping[feature][ordinal]\n",
    "            break\n",
    "    target_mappings[feature] = feature_mappings\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['one_hot_encoded_mappings', 'target_encoded_mappings'])\n",
      "---------\n",
      "target encoded value mappings ->\n",
      "{'status': {'male : single': 1.594890510948905, 'female : divorced/separated/married': 1.706451612903226, 'male : married/widowed': 1.608695652173913, 'male : divorced/separated': 1.72}, 'telephone': {'phone - none': 1.7080536912751678, 'phone - yes, registered under the customers name': 1.5321782178217822}, 'otherplans': {'none': 1.628992628992629, 'stores': 1.6808510638297873, 'bank': 1.669064748201439}}\n"
     ]
    }
   ],
   "source": [
    "# above cell demonstrates how to create the mappings\n",
    "# we use the persisted value mappings created using same code from above\n",
    "# we also get the target_encoded mappings\n",
    "import json\n",
    "with open('../dataset_generation/cat_mappings.json', 'r') as fl:\n",
    "    mappings = json.load(fl)   \n",
    "print(mappings.keys())\n",
    "one_hot_value_mappings = mappings.get('one_hot_encoded_mappings')\n",
    "target_mappings = mappings.get('target_encoded_mappings')\n",
    "print(f'---------\\ntarget encoded value mappings ->\\n{target_mappings}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certifai Evaluation Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Certifai model proxy\n",
    "\n",
    "- [CertifaiPredictorWrapper](https://cognitivescale.github.io/cortex-certifai/certifai-api-ref-1.3.2/certifai.scanner.builder.html?highlight=certifaipredictorwrapper#certifai.scanner.builder.CertifaiPredictorWrapper) api is used to wrap model objects with encoder/decoder callables\n",
    "- if model has encoding, decoding capabilities built into it, `encoder`/`decoder` kwargs need not be provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proxy = CertifaiPredictorWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test wrapped model_proxy predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test to assert wrapped certifai model predicts == raw model predicts\n",
    "assert (model_proxy.model.predict(df.drop(label_column,axis=1)[:10].values) == \n",
    "        model.predict(df.drop(label_column, axis=1)[:10].values)).all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### define  Certifai task type\n",
    "\n",
    "- CertifaiTaskOutcomes : Cortex Certifai supports classification as well as regression models. Here we have an example of multiclass classificaton (e.g. Determine whether a loan should be granted)\n",
    "- CertifaiOutcomeValue : define the outcomes possible from the model predictions. here we have a multiclass classification model that predicts whether loan will be \n",
    "- `granted`,\n",
    "- `denied` or sent for\n",
    "- `further inspection`\n",
    "\n",
    "\n",
    "Note: Please refer to Certifai Api [docs](https://cognitivescale.github.io/cortex-certifai/certifai-api-ref/certifai.scanner.builder.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Certifai evaluation task and add that to scan object\n",
    "# First define the possible prediction outcomes\n",
    "\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='Loan granted', favorable=True),\n",
    "        CertifaiOutcomeValue(2, name='Loan denied', favorable=False),\n",
    "        CertifaiOutcomeValue(3, name='further inspection', favorable=False)\n",
    "    ],\n",
    "    favorable_outcome_group_name = 'Loan Granted',\n",
    "    unfavorable_outcome_group_name = 'Loan Denied or subject to futher inspection'\n",
    "),\n",
    "                     \n",
    "    prediction_description='Determine whether a loan should be granted')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('german_credit_multiclass',\n",
    "                                  prediction_task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the model to be evaluated from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = CertifaiModel('german_credit_multiclass', local_predictor=model_proxy)\n",
    "scan.add_model(first_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create `CertifaiFeatureSchema` using categorical mappings dict from above\n",
    "\n",
    "- for one-hot encoded features we use `one_hot_value_mappings` from above to let Certifai know the different features values along with data types for that particular feature\n",
    "- for target encoded and value encoded features we let Certifai know their unique sets of values\n",
    "- define [CertifaiFeatureDataType](https://cognitivescale.github.io/cortex-certifai/certifai-api-ref-1.3.2/certifai.scanner.builder.html?highlight=certifaifeaturedatatype#certifai.scanner.builder.CertifaiFeatureDataType) for categorical features\n",
    "- define [CertifaiFeatureSchema](https://cognitivescale.github.io/cortex-certifai/certifai-api-ref-1.3.2/certifai.scanner.builder.html?highlight=certifaifeatureschema#certifai.scanner.builder.CertifaiFeatureSchema) for the datatype created above\n",
    "- add the schema to [CertifaiDataSchema](https://cognitivescale.github.io/cortex-certifai/certifai-api-ref-1.3.2/certifai.scanner.builder.html?highlight=certifaidataschema#certifai.scanner.builder.CertifaiDataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = []\n",
    "for feature in one_hot_encoded_cat_cols + target_encoded_cat_cols + value_encoded_cat_cols:\n",
    "    if feature in one_hot_value_mappings:\n",
    "        data_type = CertifaiFeatureDataType.categorical(value_columns=one_hot_value_mappings[feature].items())\n",
    "        feature_schema = CertifaiFeatureSchema(name=feature,\n",
    "                                               data_type=data_type)\n",
    "        cat_features.append(feature_schema)\n",
    "\n",
    "    elif feature in target_encoded_cat_cols:\n",
    "        data_type = CertifaiFeatureDataType.categorical(values=list(target_mappings[feature].keys()),\n",
    "                                                        target_encodings=list(target_mappings[feature].values()))\n",
    "        feature_schema = CertifaiFeatureSchema(name=feature,\n",
    "                                               data_type=data_type)\n",
    "        cat_features.append(feature_schema)\n",
    "\n",
    "    elif feature in df.columns:\n",
    "        data_type = CertifaiFeatureDataType.categorical(values=sorted(df[feature].unique().tolist()))\n",
    "        feature_schema = CertifaiFeatureSchema(name=feature,\n",
    "                                               data_type=data_type)\n",
    "        cat_features.append(feature_schema)\n",
    "        \n",
    "# certifai dataset schema combining numerical categorical features and 1-hot features\n",
    "schema = CertifaiDataSchema(features=cat_features)\n",
    "scan.dataset_schema = schema   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Certifai Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# certifai evaluation setup\n",
    "\n",
    "# Add the explanation dataset. Here we run explanations for first 100 rows from the dataset\n",
    "\n",
    "expn_dataset = CertifaiDataset('explanation',\n",
    "                               CertifaiDatasetSource.dataframe(df[:100]))\n",
    "scan.add_dataset(expn_dataset)\n",
    "scan.explanation_dataset_id = 'explanation'\n",
    "\n",
    "\n",
    "# add the evaluation for performance, explainability, robustness, fairness\n",
    "# scan.add_evaluation_type('explanation')\n",
    "# scan.add_evaluation_type('explainability')\n",
    "# scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.add_metric(CertifaiModelMetric('accuracy', certifai_metric='accuracy'))\n",
    "scan.atx_performance_metric = 'accuracy'\n",
    "scan.add_evaluation_type('performance')\n",
    "\n",
    "\n",
    "# add fairness features\n",
    "\n",
    "# set fairness features\n",
    "fairness_fields = ['status', 'age']\n",
    "\n",
    "for feature in fairness_fields:\n",
    "    scan.add_fairness_grouping_feature(CertifaiGroupingFeature(feature))\n",
    "\n",
    "# add the evaluation dataset from dataframe loaded at the start of the notebook        \n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                           CertifaiDatasetSource.dataframe(df))\n",
    "scan.add_dataset(eval_dataset)\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "scan.test_dataset_id = 'evaluation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specify target column if present in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = label_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_schema:\r\n",
      "  feature_schemas:\r\n",
      "  - category_values:\r\n",
      "    - '... < 0 DM'\r\n",
      "    - '... >= 200 DM / salary assignments for at least 1 year'\r\n",
      "    - 0 <= ... < 200 DM\r\n",
      "    - no checking account\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: checkingstatus\r\n",
      "    one_hot_columns:\r\n",
      "    - name: checkingstatus_... < 0 DM\r\n",
      "      value: '... < 0 DM'\r\n",
      "    - name: checkingstatus_... >= 200 DM / salary assignments for at least 1 year\r\n",
      "      value: '... >= 200 DM / salary assignments for at least 1 year'\r\n",
      "    - name: checkingstatus_0 <= ... < 200 DM\r\n",
      "      value: 0 <= ... < 200 DM\r\n",
      "    - name: checkingstatus_no checking account\r\n",
      "      value: no checking account\r\n",
      "  - category_values:\r\n",
      "    - co-applicant\r\n",
      "    - guarantor\r\n",
      "    - others - none\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: others\r\n",
      "    one_hot_columns:\r\n",
      "    - name: others_co-applicant\r\n",
      "      value: co-applicant\r\n",
      "    - name: others_guarantor\r\n",
      "      value: guarantor\r\n",
      "    - name: others_others - none\r\n",
      "      value: others - none\r\n",
      "  - category_values:\r\n",
      "    - <= 25 years\r\n",
      "    - '> 25 years'\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: age\r\n",
      "    one_hot_columns:\r\n",
      "    - name: age_<= 25 years\r\n",
      "      value: <= 25 years\r\n",
      "    - name: age_> 25 years\r\n",
      "      value: '> 25 years'\r\n",
      "  - category_values:\r\n",
      "    - management/ self-employed/highly qualified employee/ officer\r\n",
      "    - skilled employee / official\r\n",
      "    - unemployed/ unskilled - non-resident\r\n",
      "    - unskilled - resident\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: job\r\n",
      "    one_hot_columns:\r\n",
      "    - name: job_management/ self-employed/highly qualified employee/ officer\r\n",
      "      value: management/ self-employed/highly qualified employee/ officer\r\n",
      "    - name: job_skilled employee / official\r\n",
      "      value: skilled employee / official\r\n",
      "    - name: job_unemployed/ unskilled - non-resident\r\n",
      "      value: unemployed/ unskilled - non-resident\r\n",
      "    - name: job_unskilled - resident\r\n",
      "      value: unskilled - resident\r\n",
      "  - category_values:\r\n",
      "    - .. >= 7 years\r\n",
      "    - '... < 1 year'\r\n",
      "    - 1 <= ... < 4 years\r\n",
      "    - 4 <= ... < 7 years\r\n",
      "    - unemployed\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: employ\r\n",
      "    one_hot_columns:\r\n",
      "    - name: employ_.. >= 7 years\r\n",
      "      value: .. >= 7 years\r\n",
      "    - name: employ_... < 1 year\r\n",
      "      value: '... < 1 year'\r\n",
      "    - name: employ_1 <= ... < 4 years\r\n",
      "      value: 1 <= ... < 4 years\r\n",
      "    - name: employ_4 <= ... < 7 years\r\n",
      "      value: 4 <= ... < 7 years\r\n",
      "    - name: employ_unemployed\r\n",
      "      value: unemployed\r\n",
      "  - category_values:\r\n",
      "    - building society savings agreement/ life insurance\r\n",
      "    - car or other, not in attribute 6\r\n",
      "    - real estate\r\n",
      "    - unknown / no property\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: property\r\n",
      "    one_hot_columns:\r\n",
      "    - name: property_building society savings agreement/ life insurance\r\n",
      "      value: building society savings agreement/ life insurance\r\n",
      "    - name: property_car or other, not in attribute 6\r\n",
      "      value: car or other, not in attribute 6\r\n",
      "    - name: property_real estate\r\n",
      "      value: real estate\r\n",
      "    - name: property_unknown / no property\r\n",
      "      value: unknown / no property\r\n",
      "  - category_values:\r\n",
      "    - foreign - no\r\n",
      "    - foreign - yes\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: foreign\r\n",
      "    one_hot_columns:\r\n",
      "    - name: foreign_foreign - no\r\n",
      "      value: foreign - no\r\n",
      "    - name: foreign_foreign - yes\r\n",
      "      value: foreign - yes\r\n",
      "  - category_values:\r\n",
      "    - all credits at this bank paid back duly\r\n",
      "    - critical account/ other credits existing (not at this bank)\r\n",
      "    - delay in paying off in the past\r\n",
      "    - existing credits paid back duly till now\r\n",
      "    - no credits taken/ all credits paid back duly\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: history\r\n",
      "    one_hot_columns:\r\n",
      "    - name: history_all credits at this bank paid back duly\r\n",
      "      value: all credits at this bank paid back duly\r\n",
      "    - name: history_critical account/ other credits existing (not at this bank)\r\n",
      "      value: critical account/ other credits existing (not at this bank)\r\n",
      "    - name: history_delay in paying off in the past\r\n",
      "      value: delay in paying off in the past\r\n",
      "    - name: history_existing credits paid back duly till now\r\n",
      "      value: existing credits paid back duly till now\r\n",
      "    - name: history_no credits taken/ all credits paid back duly\r\n",
      "      value: no credits taken/ all credits paid back duly\r\n",
      "  - category_values:\r\n",
      "    - .. >= 1000 DM\r\n",
      "    - '... < 100 DM'\r\n",
      "    - 100 <= ... < 500 DM\r\n",
      "    - 500 <= ... < 1000 DM\r\n",
      "    - unknown/ no savings account\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: savings\r\n",
      "    one_hot_columns:\r\n",
      "    - name: savings_.. >= 1000 DM\r\n",
      "      value: .. >= 1000 DM\r\n",
      "    - name: savings_... < 100 DM\r\n",
      "      value: '... < 100 DM'\r\n",
      "    - name: savings_100 <= ... < 500 DM\r\n",
      "      value: 100 <= ... < 500 DM\r\n",
      "    - name: savings_500 <= ... < 1000 DM\r\n",
      "      value: 500 <= ... < 1000 DM\r\n",
      "    - name: savings_unknown/ no savings account\r\n",
      "      value: unknown/ no savings account\r\n",
      "  - category_values:\r\n",
      "    - business\r\n",
      "    - car (new)\r\n",
      "    - car (used)\r\n",
      "    - domestic appliances\r\n",
      "    - education\r\n",
      "    - furniture/equipment\r\n",
      "    - purpose - others\r\n",
      "    - radio/television\r\n",
      "    - repairs\r\n",
      "    - retraining\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: purpose\r\n",
      "    one_hot_columns:\r\n",
      "    - name: purpose_business\r\n",
      "      value: business\r\n",
      "    - name: purpose_car (new)\r\n",
      "      value: car (new)\r\n",
      "    - name: purpose_car (used)\r\n",
      "      value: car (used)\r\n",
      "    - name: purpose_domestic appliances\r\n",
      "      value: domestic appliances\r\n",
      "    - name: purpose_education\r\n",
      "      value: education\r\n",
      "    - name: purpose_furniture/equipment\r\n",
      "      value: furniture/equipment\r\n",
      "    - name: purpose_purpose - others\r\n",
      "      value: purpose - others\r\n",
      "    - name: purpose_radio/television\r\n",
      "      value: radio/television\r\n",
      "    - name: purpose_repairs\r\n",
      "      value: repairs\r\n",
      "    - name: purpose_retraining\r\n",
      "      value: retraining\r\n",
      "  - category_values:\r\n",
      "    - for free\r\n",
      "    - own\r\n",
      "    - rent\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: housing\r\n",
      "    one_hot_columns:\r\n",
      "    - name: housing_for free\r\n",
      "      value: for free\r\n",
      "    - name: housing_own\r\n",
      "      value: own\r\n",
      "    - name: housing_rent\r\n",
      "      value: rent\r\n",
      "  - category_values:\r\n",
      "    - none\r\n",
      "    - stores\r\n",
      "    - bank\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: otherplans\r\n",
      "    target_encodings:\r\n",
      "    - 1.628992628992629\r\n",
      "    - 1.6808510638297873\r\n",
      "    - 1.669064748201439\r\n",
      "  - category_values:\r\n",
      "    - 'male : single'\r\n",
      "    - 'female : divorced/separated/married'\r\n",
      "    - 'male : married/widowed'\r\n",
      "    - 'male : divorced/separated'\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: status\r\n",
      "    target_encodings:\r\n",
      "    - 1.594890510948905\r\n",
      "    - 1.706451612903226\r\n",
      "    - 1.608695652173913\r\n",
      "    - 1.72\r\n",
      "  - category_values:\r\n",
      "    - phone - none\r\n",
      "    - phone - yes, registered under the customers name\r\n",
      "    data_type: categorical\r\n",
      "    feature_name: telephone\r\n",
      "    target_encodings:\r\n",
      "    - 1.7080536912751678\r\n",
      "    - 1.5321782178217822\r\n",
      "  outcome_column: outcome\r\n",
      "datasets:\r\n",
      "- dataset_id: explanation\r\n",
      "  delimiter: ','\r\n",
      "  file_type: loaded\r\n",
      "  has_header: true\r\n",
      "  quote_character: '\"'\r\n",
      "- dataset_id: evaluation\r\n",
      "  delimiter: ','\r\n",
      "  file_type: loaded\r\n",
      "  has_header: true\r\n",
      "  quote_character: '\"'\r\n",
      "evaluation:\r\n",
      "  evaluation_dataset_id: evaluation\r\n",
      "  evaluation_types:\r\n",
      "  - fairness\r\n",
      "  - performance\r\n",
      "  explanation_dataset_id: explanation\r\n",
      "  fairness_grouping_features:\r\n",
      "  - name: status\r\n",
      "  - name: age\r\n",
      "  favorable_outcome_group_name: Loan Granted\r\n",
      "  name: german_credit_multiclass\r\n",
      "  prediction_description: Determine whether a loan should be granted\r\n",
      "  prediction_favorability: explicit\r\n",
      "  prediction_values:\r\n",
      "  - favorable: true\r\n",
      "    name: Loan granted\r\n",
      "    value: 1\r\n",
      "  - favorable: false\r\n",
      "    name: Loan denied\r\n",
      "    value: 2\r\n",
      "  - favorable: false\r\n",
      "    name: further inspection\r\n",
      "    value: 3\r\n",
      "  test_dataset_id: evaluation\r\n",
      "  unfavorable_outcome_group_name: Loan Denied or subject to futher inspection\r\n",
      "model_use_case:\r\n",
      "  atx_performance_metric_name: accuracy\r\n",
      "  model_use_case_id: german_credit_multiclass\r\n",
      "  name: german_credit_multiclass\r\n",
      "  performance_metrics:\r\n",
      "  - metric: Accuracy\r\n",
      "    name: accuracy\r\n",
      "  task_type: multiclass-classification\r\n",
      "models:\r\n",
      "- model_id: german_credit_multiclass\r\n",
      "  name: german_credit_multiclass\r\n"
     ]
    }
   ],
   "source": [
    "# generate and save the scan definition\n",
    "generated_scan_def = scan.extract_yaml()\n",
    "local_scan_definition_file = 'target_encoded_multiclass_scan_def.yaml'\n",
    "\n",
    "with open(local_scan_definition_file, 'w') as f:\n",
    "    scan.save(f)\n",
    "!cat $local_scan_definition_file    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the scan\n",
    "result = scan.run(base_path='.' , write_reports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_fairness = construct_scores_dataframe(scores('fairness', result), include_confidence=True)\n",
    "display(df_fairness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (fairness) status burdens by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "display(df_fairness)\n",
    "\n",
    "# since status is a target encoded variable;\n",
    "# for plotting values in non-encoded space we make use of the `target_mappings` dict\n",
    "decimals=3\n",
    "\n",
    "groups_disp, groups  = list(target_mappings['status'].keys()), list(map(lambda x: round(x,decimals),target_mappings['status'].values()))\n",
    "\n",
    "\n",
    "\n",
    "feature_scores = df_fairness[[f\"Group details ({g})\" for g in groups]]\n",
    "feature_lower_bounds = df_fairness[[f + ' lower bound' for f in feature_scores]]\n",
    "feature_upper_bounds = df_fairness[[f + ' upper bound' for f in feature_scores]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "ax.set_title('status burdens by model', fontsize=20)\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "ax.set_xticks(np.arange(len(groups))+width)\n",
    "ax.set_xticklabels(groups_disp)\n",
    "\n",
    "for idx in range(len(df_fairness)):\n",
    "    central_values = list(feature_scores.iloc[idx])\n",
    "    lower_bounds = list(feature_lower_bounds.iloc[idx])\n",
    "    upper_bounds = list(feature_upper_bounds.iloc[idx])\n",
    "    lower_errors = [central_values[i] - lower_bounds[i] for i in range(len(central_values))]\n",
    "    upper_errors = [upper_bounds[i] - central_values[i] for i in range(len(central_values))]\n",
    "\n",
    "    ax.bar([width/2+idx*width+f_idx for f_idx in range(len(groups))],\n",
    "            central_values,\n",
    "            width,\n",
    "            yerr=[lower_errors, upper_errors],\n",
    "            color=colors[idx],\n",
    "            label=df_fairness.index[idx],\n",
    "            capsize=10)\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (fairness) age burdens by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "display(df_fairness)\n",
    "# since age is a one-hot encoded variable;\n",
    "# for plotting values in non-encoded space we make use of the `one_hot_value_mappings` dict\n",
    "\n",
    "groups_disp, groups  = list(one_hot_value_mappings['age'].keys()), list(one_hot_value_mappings['age'].values())\n",
    "\n",
    "feature_scores = df_fairness[[f\"Group details ({g})\" for g in groups]]\n",
    "feature_lower_bounds = df_fairness[[f + ' lower bound' for f in feature_scores]]\n",
    "feature_upper_bounds = df_fairness[[f + ' upper bound' for f in feature_scores]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "ax.set_title('age burdens by model', fontsize=20)\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "ax.set_xticks(np.arange(len(groups))+width)\n",
    "ax.set_xticklabels(groups_disp)\n",
    "\n",
    "for idx in range(len(df_fairness)):\n",
    "    central_values = list(feature_scores.iloc[idx])\n",
    "    lower_bounds = list(feature_lower_bounds.iloc[idx])\n",
    "    upper_bounds = list(feature_upper_bounds.iloc[idx])\n",
    "    lower_errors = [central_values[i] - lower_bounds[i] for i in range(len(central_values))]\n",
    "    upper_errors = [upper_bounds[i] - central_values[i] for i in range(len(central_values))]\n",
    "\n",
    "    ax.bar([width/2+idx*width+f_idx for f_idx in range(len(groups))],\n",
    "            central_values,\n",
    "            width,\n",
    "            yerr=[lower_errors, upper_errors],\n",
    "            color=colors[idx],\n",
    "            label=df_fairness.index[idx],\n",
    "            capsize=10)\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Fairness by model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_fairness)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Let's chart the fairness measure by feature for each model together with its confidence bounds for\n",
    "# easier visual comparison:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['Feature (age)', 'Feature (status)']\n",
    "feature_scores = df_fairness[features]\n",
    "feature_lower_bounds = df_fairness[[f + ' lower bound' for f in features]]\n",
    "feature_upper_bounds = df_fairness[[f + ' upper bound' for f in features]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "ax.set_title('Feature fairness by model', fontsize=20)\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "ax.set_xticks(np.arange(len(features))+width)\n",
    "ax.set_xticklabels(features)\n",
    "\n",
    "for idx in range(len(df_fairness)):\n",
    "    central_values = list(feature_scores.iloc[idx])\n",
    "    lower_bounds = list(feature_lower_bounds.iloc[idx])\n",
    "    upper_bounds = list(feature_upper_bounds.iloc[idx])\n",
    "    lower_errors = [central_values[i] - lower_bounds[i] for i in range(len(central_values))]\n",
    "    upper_errors = [upper_bounds[i] - central_values[i] for i in range(len(central_values))]\n",
    "\n",
    "    ax.bar([width/2+idx*width+f_idx for f_idx in range(len(features))],\n",
    "            central_values,\n",
    "            width,\n",
    "            yerr=[lower_errors, upper_errors],\n",
    "            color=colors[idx],\n",
    "            label=df_fairness.index[idx],\n",
    "            capsize=10)\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness by model\n",
    "\n",
    "- The following cell shows how to extract Certifai Robustness Score from the scan results object\n",
    "- The high Robustness score indicates the given model needs to change larger no. of features to alter the prediction outcomes\n",
    "- please refer to official documentation for [Certifai Robustness](https://cognitivescale.github.io/cortex-certifai/docs/factors/robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robustness = construct_scores_dataframe(scores('robustness', result), include_confidence=True)\n",
    "display(df_robustness)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['robustness']\n",
    "feature_scores = df_robustness[features]\n",
    "feature_lower_bounds = df_robustness[[f + ' lower bound' for f in features]]\n",
    "feature_upper_bounds = df_robustness[[f + ' upper bound' for f in features]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "ax.set_title('Robustness', fontsize=20)\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "ax.set_xticks(np.arange(len(features))+width)\n",
    "ax.set_xticklabels(features)\n",
    "\n",
    "for idx in range(len(df_robustness)):\n",
    "    central_values = list(feature_scores.iloc[idx])\n",
    "    lower_bounds = list(feature_lower_bounds.iloc[idx])\n",
    "    upper_bounds = list(feature_upper_bounds.iloc[idx])\n",
    "    lower_errors = [central_values[i] - lower_bounds[i] for i in range(len(central_values))]\n",
    "    upper_errors = [upper_bounds[i] - central_values[i] for i in range(len(central_values))]\n",
    "\n",
    "    ax.errorbar([width/2+idx*width+f_idx for f_idx in range(len(features))],\n",
    "            central_values,\n",
    "            yerr=[lower_errors, upper_errors],\n",
    "            color=colors[idx],\n",
    "            label=df_robustness.index[idx],\n",
    "            capsize=10)\n",
    "    \n",
    "    ax.scatter([width/2+idx*width+f_idx for f_idx in range(len(features))], central_values,s=500)\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability by model \n",
    "\n",
    "- the following cell shows how to extract Certifai Explainability Score from the scan results object\n",
    "- please refer to official documentation for [Certifai Explainability](https://cognitivescale.github.io/cortex-certifai/docs/factors/explainability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_explainability= construct_scores_dataframe(scores('explainability', result), include_confidence=True)\n",
    "display(df_explainability)\n",
    "\n",
    "\n",
    "features = ['Num features (1)', 'Num features (2)', 'Num features (3)', 'Num features (4)', 'Num features (5)', 'Num features (6)', 'Num features (7)', 'Num features (8)', 'Num features (9)', 'Num features (10)']\n",
    "features_disp = [i for i in range(1,11)]\n",
    "feature_scores = df_explainability[features]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,4])\n",
    "ax.set_title('Explainability by no of features', fontsize=20)\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink']\n",
    "width = 0.4\n",
    "\n",
    "ax.set_xticks(np.arange(len(features))+width)\n",
    "ax.set_xticklabels(features_disp)\n",
    "ax.set_xlabel(\"Num. of features\")\n",
    "ax.set_ylabel(\"Explainability score\")\n",
    "\n",
    "for idx in range(len(df_explainability)):\n",
    "    central_values = list(feature_scores.iloc[idx])\n",
    "\n",
    "    ax.bar([width/2+idx*width+f_idx for f_idx in range(len(features))],\n",
    "            central_values,\n",
    "            width,align='edge',\n",
    "            color=colors[idx],\n",
    "            label=df_explainability.index[idx],\n",
    "            capsize=10)\n",
    "\n",
    "fig.legend(fontsize=14, bbox_to_anchor=(1.1,.6))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
