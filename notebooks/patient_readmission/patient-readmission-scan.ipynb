{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90d890b7e4ff4dbbb27e2b59e50e942c7bb0078c"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This is the second notebook in this example of how to scan models using Certifai. If you have not already done so, please run the [first notebook](patient-readmission-train) to train the models to be explained.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Create a Certifai scan object with the information Certifai needs to explain the models\n",
    "2. Run the explanations scan and save its definition for future use\n",
    "3. Run a second scan, this time to get the trust scores (fairness, explainability, robustness)\n",
    "4. View the results in the Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "fba09007b76132d605daa699b39a5fc92ff87ee1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel,\n",
    "                                      CertifaiDataset, CertifaiDatasetSource, CertifaiGroupingFeature,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue,\n",
    "                                      CertifaiFeatureDataType, CertifaiFeatureSchema, CertifaiDataSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4490db86a8bb5bdc7bc81d17da9f0a3b690b90c"
   },
   "source": [
    "# Creating the Certifai Scan object\n",
    "\n",
    "In this section, we create a Certifai scan object containing with the information Certifai needs to run a scan that explains the models. This information consists of:\n",
    "* Metadata about the prediction task being performed\n",
    "* What evaluations to run\n",
    "* The models to be scanned\n",
    "* The datasets to be used\n",
    "* Metadata about the datasets that is needed for the scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Certifai scan object, providing metadata about the prediction task that is performed by the models. Define the evaluations to be performed, which in this case is just 'explanation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(0, name='Not Readmitted', favorable=True),\n",
    "        CertifaiOutcomeValue(1, name='Readmitted')\n",
    "    ]),\n",
    "    prediction_description='Determine whether a patient will be readmitted')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('readmission',\n",
    "                                  prediction_task=task)\n",
    "scan.add_evaluation_type('explanation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the two models we saved in the first notebook, and wrap them so that they can be called by Certifai. Add these models into the scan object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "\n",
    "for model_name in ['logit', 'mlp']:\n",
    "    with open(f'readmission_{model_name}.pkl', 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "        model = CertifaiPredictorWrapper(saved.get('model'), encoder=Encoder())\n",
    "        scan.add_model(CertifaiModel(model_name, local_predictor=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two datasets to the scan. The evaluation dataset is used by Certifai to create an initial population for the genetic algorithm used in the scan, and needs to be a representative sample of the expected data (minimum c. 1K rows, ideally 10-50K rows, larger is OK). The explanation dataset contains the points to be explained. Note the time to run the scan will depend linearly on the size of the explanation dataset, so it is best to keep this relatively small (in this case, 100 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.csv('diabetic_data_processed.csv'))\n",
    "scan.add_dataset(eval_dataset)\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "\n",
    "df = pd.read_csv('diabetic_data_processed.csv')\n",
    "explan_dataset = CertifaiDataset('explanation',\n",
    "                                CertifaiDatasetSource.dataframe(df.sample(100)))\n",
    "scan.add_dataset(explan_dataset)\n",
    "scan.explanation_dataset_id = 'explanation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the metadata about one-hot encoding that we saved in the first notebook and use this to define the feature schema in the scan object. This lets Certifai know the value mappings to columns for both the analysis and when presenting explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cat_value_mappings.pkl', 'rb') as f:\n",
    "    cat_value_mappings = pickle.load(f)\n",
    "\n",
    "cat_features = []\n",
    "for feature, value_columns in cat_value_mappings.items():\n",
    "    data_type = CertifaiFeatureDataType.categorical(value_columns=value_columns.items())\n",
    "    feature_schema = CertifaiFeatureSchema(name=feature, data_type=data_type)\n",
    "    cat_features.append(feature_schema)\n",
    "schema = CertifaiDataSchema(features=cat_features)\n",
    "scan.dataset_schema = schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell Certifai about the label/outcome column in the dataset, so that it won't be passed in the predict calls or used in the genetic algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.dataset_schema.outcome_feature_name = 'readmitted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Explanations Scan\n",
    "\n",
    "Run the scan, saving the results in the `reports` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'readmission' and scan_id: '9e5292a83fae', total estimated time is 2 minutes\n",
      "[--------------------] 2020-10-03 20:06:54.056010 - 0 of 2 reports (0.0% complete) - Running explanation evaluation for model: logit, estimated time is 45 seconds\n",
      "[##########----------] 2020-10-03 20:07:37.539557 - 1 of 2 reports (50.0% complete) - Running explanation evaluation for model: mlp, estimated time is 60 seconds\n",
      "[####################] 2020-10-03 20:08:32.690934 - 2 of 2 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "results = scan.run(write_reports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan definition as a yaml file so that it can be rerun in the future, either in a notebook or from the CLI. This is useful for example to get explanations for additional datapoints, for updated models, or for a model that has been deployed as a service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('explain-scan-def.yaml', \"w\") as f:\n",
    "    scan.save(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scan definition can be loaded into a new notebook using `CertifaiScanBuilder.from_file('explain-scan-def.yaml')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the Results\n",
    "\n",
    "The results can be viewed in the Certifai console using the CLI command `certifai console`, run from this folder. \n",
    "Go to `http://localhost:8000` in your browser. \n",
    "\n",
    "The results can also be analyzed in this notebook; or analyzed later in a separate notebook.\n",
    "TODO LINK TO ANALYSIS NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run a Trust Scan\n",
    "\n",
    "In this Section, we're now going to modify the scan definition to run a scan for fairness, explainability and robustness. We could include explanations in this new scan, but for now we'll omit them. \n",
    "\n",
    "First, remove the explanation evaluation and add fairness, explainability and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.remove_evaluation_type('explanation')\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('explainability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this use case, 'fairness' is less about with bias against protected groups, and more to do with detecting how specific groups may be more biased to unfavorable outcomes. To scan for these disparities, we need to provide some additional information on the groups to be scanned. We'll choose race, gender and age.\n",
    "\n",
    "These results need to be interpreted with care.  [Racial/Ethnic Disparities in Readmissions in US Hospitals: The Role of Insurance Coverage](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5946640/) shows that lower readmission rates may not always be construed as a good outcome, and could relate to a lack of insurance coverage and poor access to care. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('gender'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running a fairness analysis, it is important that there are sufficient samples of each class within the grouping features. We can check this and other potential issues using a preflight analysis. It will also give us a time estimate for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preflight Scan\n",
      "[--------------------] 2020-10-03 20:08:39.280893 - 0 of 8 checks (0.0% complete) - Running model nondeterminism preflight check for model logit\n",
      "[##------------------] 2020-10-03 20:08:39.322757 - 1 of 8 checks (12.5% complete) - Running scan time estimate preflight check for model logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-03 20:08:41,630 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#####---------------] 2020-10-03 20:09:42.265016 - 2 of 8 checks (25.0% complete) - Running unknown outcome class preflight check for model logit\n",
      "[#######-------------] 2020-10-03 20:09:42.287747 - 3 of 8 checks (37.5% complete) - Running fairness class samples preflight check for model logit\n",
      "[##########----------] 2020-10-03 20:09:42.771684 - 4 of 8 checks (50.0% complete) - Finished all preflight checks for model logit\n",
      "[##########----------] 2020-10-03 20:09:42.771800 - 4 of 8 checks (50.0% complete) - Running model nondeterminism preflight check for model mlp\n",
      "[############--------] 2020-10-03 20:09:42.813925 - 5 of 8 checks (62.5% complete) - Running scan time estimate preflight check for model mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-03 20:09:45,318 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###############-----] 2020-10-03 20:11:00.715962 - 6 of 8 checks (75.0% complete) - Running unknown outcome class preflight check for model mlp\n",
      "[#################---] 2020-10-03 20:11:00.739645 - 7 of 8 checks (87.5% complete) - Running fairness class samples preflight check for model mlp\n",
      "[####################] 2020-10-03 20:11:01.182076 - 8 of 8 checks (100.0% complete) - Finished all preflight checks for model mlp\n",
      "{'logit': {'errors': [],\n",
      "           'messages': ['Passed model non determinism check',\n",
      "                        'Expected time for fairness analysis is 529 seconds',\n",
      "                        'Expected time for robustness analysis is 62 seconds',\n",
      "                        'Expected time for explainability analysis is 59 '\n",
      "                        'seconds',\n",
      "                        'Model logit passed time estimation check',\n",
      "                        'Passed unknown outcome classes check'],\n",
      "           'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                        \"size for 'Unknown/Invalid' with 3 examples\",\n",
      "                        \"Fairness grouping attribute 'gender' has small sample \"\n",
      "                        \"size for 'nan' with 0 examples\",\n",
      "                        \"Fairness grouping attribute 'age' has small sample \"\n",
      "                        \"size for 'nan' with 0 examples\"]},\n",
      " 'mlp': {'errors': [],\n",
      "         'messages': ['Passed model non determinism check',\n",
      "                      'Expected time for fairness analysis is 634 seconds',\n",
      "                      'Expected time for robustness analysis is 72 seconds',\n",
      "                      'Expected time for explainability analysis is 72 seconds',\n",
      "                      'Model mlp passed time estimation check',\n",
      "                      'Passed unknown outcome classes check'],\n",
      "         'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                      \"size for 'Unknown/Invalid' with 3 examples\",\n",
      "                      \"Fairness grouping attribute 'gender' has small sample \"\n",
      "                      \"size for 'nan' with 0 examples\",\n",
      "                      \"Fairness grouping attribute 'age' has small sample size \"\n",
      "                      \"for 'nan' with 0 examples\"]}}\n"
     ]
    }
   ],
   "source": [
    "preflight_result = scan.run_preflight()\n",
    "pprint.pprint(preflight_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the warnings, 'gender' has a small sample size (3) for 'Unknown/Invalid'. We will address this by dropping those rows, given they are a tiny proportion and are not a useful class for analysis. \n",
    "\n",
    "In other cases (e.g. for underrepresented age ranges), a better approach is to combine smaller classes into one larger class using bucketing. This is illustrated in the [Practical Issues](cortex-certifai-examples/notebooks/practical_issues/PracticalIssues.ipynb) notebook.\n",
    "\n",
    "There are also warnings about 'nan' classes in 'age' and 'gender' with 0 examples. This is an artifact of including a nan column for one-hot encodings with no null values. These warnings can be ignored as a class with 0 examples will not be considered in the analysis. However, it would make sense to change the data pipeline to eliminate these columns and not pass them to the models. \n",
    "\n",
    "Drop rows with 'gender_Unknown/Invalid' and replace the evaluation dataset in the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.remove_dataset('evaluation')\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                                CertifaiDatasetSource.dataframe(df[df['gender_Unknown/Invalid'] == 0]))\n",
    "scan.add_dataset(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan definition and run the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------] 2020-10-03 20:11:06.961680 - 0 of 6 reports (0.0% complete) - Starting scan with model_use_case_id: 'readmission' and scan_id: 'f9ec20548880', total estimated time is 24 minutes\n",
      "[--------------------] 2020-10-03 20:11:06.961851 - 0 of 6 reports (0.0% complete) - Running fairness evaluation for model: logit, estimated time is 529 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-03 20:28:08,070 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2020-10-03 20:29:36,562 root   WARNING  Examples of protected class ('age', '[10-20)') exhausted before convergence after 691 samples\n",
      "2020-10-03 20:47:40,876 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2020-10-03 20:49:48,971 root   WARNING  Examples of protected class ('age', '[20-30)') exhausted before convergence after 1657 samples\n"
     ]
    }
   ],
   "source": [
    "with open('trust-scan-def.yaml', \"w\") as f:\n",
    "    scan.save(f)\n",
    "results = scan.run(write_reports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scan.add_fairness_metric('demographic_parity')\n",
    "scan.fairness_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scan.add_fairness_metric('demographic_parity')\n",
    "#scan.primary_fairness_metric = 'demographic_parity'\n",
    "#scan.remove_fairness_metric('burden')\n",
    "#scan.remove_evaluation_type('robustness')\n",
    "#scan.remove_evaluation_type('explainability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = scan.run(write_reports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
