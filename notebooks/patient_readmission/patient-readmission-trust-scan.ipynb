{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90d890b7e4ff4dbbb27e2b59e50e942c7bb0078c"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This fourth notebook shows how to scan models for their trust scores using Certifai, using a previously created scan definition as the starting point. \n",
    "\n",
    "If you have not already done so, please run the [first notebook](patient-readmission-train.ipynb) to train the models to be explained and the [second notebook](patient-readmission-explain-scan.ipynb) to create the `explain-scan-def.yaml` scan definition.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load the previously saved `explain-scan-def.yaml` scan definition and models\n",
    "2. Modify this scan definition to scan for the trust scores (fairness, explainability, robustness)\n",
    "4. View the results in the Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "fba09007b76132d605daa699b39a5fc92ff87ee1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiGroupingFeature, \n",
    "                                      CertifaiPredictorWrapper, CertifaiModel, \n",
    "                                      CertifaiDataset, CertifaiDatasetSource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4490db86a8bb5bdc7bc81d17da9f0a3b690b90c"
   },
   "source": [
    "# Loading the Certifai Scan object\n",
    "\n",
    "In this section, we load the previously defined scan definition to use as a starting point. This is a convenience that avoids us having to recreate information about the prediction task, datasets and feature schema. \n",
    "\n",
    "Load the scan definition from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = CertifaiScanBuilder.from_file('explain-scan-def.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're running with local models in the notebook, we need to reload the models and reassociate them with the scan. This isn't necessary if the models are running externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "\n",
    "for model_name in ['logit', 'mlp']:\n",
    "    with open(f'readmission_{model_name}.pkl', 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "        model = CertifaiPredictorWrapper(saved.get('model'), encoder=Encoder())\n",
    "        scan.remove_model(model_name)\n",
    "        scan.add_model(CertifaiModel(model_name, local_predictor=model))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run the Trust Scan\n",
    "\n",
    "In this Section, we're now going to modify the scan definition to run a scan for fairness, explainability and robustness. We could include explanations in this new scan, but for now we'll omit them. \n",
    "\n",
    "First, remove the explanation evaluation and add fairness, explainability and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.remove_evaluation_type('explanation')\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('explainability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this use case, 'fairness' is less about with bias against protected groups, and more to do with detecting how specific groups may be more biased to unfavorable outcomes. To scan for these disparities, we need to provide some additional information on the groups to be scanned. We'll choose race, gender and age.\n",
    "\n",
    "These results need to be interpreted with care.  [Racial/Ethnic Disparities in Readmissions in US Hospitals: The Role of Insurance Coverage](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5946640/) shows that lower readmission rates may not always be construed as a good outcome, and could relate to a lack of insurance coverage and poor access to care. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('gender'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running a fairness analysis, it is important that there are sufficient samples of each class within the grouping features. We can check this and other potential issues using a preflight analysis. It will also give us a time estimate for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preflight Scan\n",
      "[--------------------] 2020-10-04 12:23:04.361895 - 0 of 8 checks (0.0% complete) - Running model nondeterminism preflight check for model logit\n",
      "[##------------------] 2020-10-04 12:23:04.408516 - 1 of 8 checks (12.5% complete) - Running scan time estimate preflight check for model logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-04 12:23:06,515 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#####---------------] 2020-10-04 12:24:04.340559 - 2 of 8 checks (25.0% complete) - Running unknown outcome class preflight check for model logit\n",
      "[#######-------------] 2020-10-04 12:24:04.362417 - 3 of 8 checks (37.5% complete) - Running fairness class samples preflight check for model logit\n",
      "[##########----------] 2020-10-04 12:24:04.760288 - 4 of 8 checks (50.0% complete) - Finished all preflight checks for model logit\n",
      "[##########----------] 2020-10-04 12:24:04.760464 - 4 of 8 checks (50.0% complete) - Running model nondeterminism preflight check for model mlp\n",
      "[############--------] 2020-10-04 12:24:04.808456 - 5 of 8 checks (62.5% complete) - Running scan time estimate preflight check for model mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-04 12:24:07,055 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###############-----] 2020-10-04 12:25:21.620083 - 6 of 8 checks (75.0% complete) - Running unknown outcome class preflight check for model mlp\n",
      "[#################---] 2020-10-04 12:25:21.643150 - 7 of 8 checks (87.5% complete) - Running fairness class samples preflight check for model mlp\n",
      "[####################] 2020-10-04 12:25:22.039223 - 8 of 8 checks (100.0% complete) - Finished all preflight checks for model mlp\n",
      "{'logit': {'errors': [],\n",
      "           'messages': ['Passed model non determinism check',\n",
      "                        'Expected time for fairness analysis is 540 seconds',\n",
      "                        'Expected time for robustness analysis is 63 seconds',\n",
      "                        'Expected time for explainability analysis is 58 '\n",
      "                        'seconds',\n",
      "                        'Model logit passed time estimation check',\n",
      "                        'Passed unknown outcome classes check'],\n",
      "           'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                        \"size for 'Unknown/Invalid' with 3 examples\"]},\n",
      " 'mlp': {'errors': [],\n",
      "         'messages': ['Passed model non determinism check',\n",
      "                      'Expected time for fairness analysis is 659 seconds',\n",
      "                      'Expected time for robustness analysis is 75 seconds',\n",
      "                      'Expected time for explainability analysis is 75 seconds',\n",
      "                      'Model mlp passed time estimation check',\n",
      "                      'Passed unknown outcome classes check'],\n",
      "         'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                      \"size for 'Unknown/Invalid' with 3 examples\"]}}\n"
     ]
    }
   ],
   "source": [
    "preflight_result = scan.run_preflight()\n",
    "pprint.pprint(preflight_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the warnings, 'gender' has a small sample size (3) for 'Unknown/Invalid'. We will address this by dropping those rows, given they are a tiny proportion and are not a useful class for analysis. \n",
    "\n",
    "In other cases (e.g. for underrepresented age ranges), a better approach is to combine smaller classes into one larger class using bucketing. This is illustrated in the [Practical Issues](cortex-certifai-examples/notebooks/practical_issues/PracticalIssues.ipynb) notebook.\n",
    "\n",
    "Drop rows with 'gender_Unknown/Invalid' and replace the evaluation dataset in the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetic_data_processed.csv')\n",
    "scan.remove_dataset('evaluation')\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                                CertifaiDatasetSource.dataframe(df[df['gender_Unknown/Invalid'] == 0]))\n",
    "scan.add_dataset(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan definition and run the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'readmission' and scan_id: 'e9ffd5d26bb7', total estimated time is 25 minutes\n",
      "[--------------------] 2020-10-04 12:25:28.110357 - 0 of 6 reports (0.0% complete) - Running fairness evaluation for model: logit, estimated time is 540 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-04 12:43:27,292 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2020-10-04 12:44:57,492 root   WARNING  Examples of protected class ('age', 15) exhausted before convergence after 691 samples\n",
      "2020-10-04 13:03:59,213 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2020-10-04 13:06:08,001 root   WARNING  Examples of protected class ('age', 25) exhausted before convergence after 1657 samples\n",
      "2020-10-04 13:09:45,528 root   WARNING  Examples of protected class ('race', 'Hispanic') exhausted before convergence after 2037 samples\n",
      "2020-10-04 13:10:39,866 root   WARNING  Examples of protected class ('race', 'nan') exhausted before convergence after 2271 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###-----------------] 2020-10-04 13:11:21.181134 - 1 of 6 reports (16.67% complete) - Running robustness evaluation for model: logit, estimated time is 63 seconds\n",
      "[######--------------] 2020-10-04 13:13:07.149930 - 2 of 6 reports (33.33% complete) - Running explainability evaluation for model: logit, estimated time is 58 seconds\n",
      "[##########----------] 2020-10-04 13:14:29.533358 - 3 of 6 reports (50.0% complete) - Running fairness evaluation for model: mlp, estimated time is 659 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-04 13:22:14,746 root   WARNING  Examples of protected class ('age', 5) exhausted before convergence after 161 samples\n",
      "2020-10-04 13:44:16,592 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2020-10-04 13:46:49,174 root   WARNING  Examples of protected class ('age', 15) exhausted before convergence after 691 samples\n",
      "2020-10-04 14:02:48,220 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2020-10-04 14:03:43,924 root   WARNING  Examples of protected class ('race', 'nan') exhausted before convergence after 2271 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############-------] 2020-10-04 14:04:26.739811 - 4 of 6 reports (66.67% complete) - Running robustness evaluation for model: mlp, estimated time is 75 seconds\n",
      "[################----] 2020-10-04 14:05:44.046731 - 5 of 6 reports (83.33% complete) - Running explainability evaluation for model: mlp, estimated time is 75 seconds\n",
      "[####################] 2020-10-04 14:07:25.433319 - 6 of 6 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "with open('trust-scan-def.yaml', \"w\") as f:\n",
    "    scan.save(f)\n",
    "results = scan.run(write_reports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the Results\n",
    "\n",
    "The results can be viewed in the Certifai console using the CLI command `certifai console`, run from this folder. \n",
    "Go to `http://localhost:8000` in your browser. \n",
    "\n",
    "The results can also be analyzed in a notebook. See the [fifth notebook](patient-readmission-trust-results.ipynb) for how to load and work with the results of the trust score scan in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
