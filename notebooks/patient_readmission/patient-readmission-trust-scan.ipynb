{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90d890b7e4ff4dbbb27e2b59e50e942c7bb0078c"
   },
   "source": [
    "Copyright (c) 2020. Cognitive Scale Inc. All rights reserved.\n",
    "Licensed under CognitiveScale Example Code [License](https://github.com/CognitiveScale/cortex-certifai-examples/blob/7998b8a481fccd467463deb1fc46d19622079b0e/LICENSE.md)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This fourth notebook shows how to scan models for their trust scores using Certifai, using a previously created scan definition as the starting point. \n",
    "\n",
    "If you have not already done so, please run the [first notebook](patient-readmission-train.ipynb) to train the models to be explained and the [second notebook](patient-readmission-explain-scan.ipynb) to create the `explain-scan-def.yaml` scan definition.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load the previously saved `explain-scan-def.yaml` scan definition and models\n",
    "2. Modify this scan definition to scan for the trust scores (fairness, explainability, robustness)\n",
    "4. View the results in the Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "fba09007b76132d605daa699b39a5fc92ff87ee1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiGroupingFeature, \n",
    "                                      CertifaiPredictorWrapper, CertifaiModel, \n",
    "                                      CertifaiDataset, CertifaiDatasetSource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4490db86a8bb5bdc7bc81d17da9f0a3b690b90c"
   },
   "source": [
    "# Loading the Certifai Scan object\n",
    "\n",
    "In this section, we load the previously defined scan definition to use as a starting point. This is a convenience that avoids us having to recreate information about the prediction task, datasets and feature schema. \n",
    "\n",
    "Load the scan definition from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 13:15:32,644 root   WARNING  The model 'logit' was locally defined and doesn't have a valid 'predict_endpoint'. Before running a scan with this model, you should deploy the model and update the 'logit' models metadata in the scan definition.\n",
      "2020-11-02 13:15:32,644 root   WARNING  The model 'mlp' was locally defined and doesn't have a valid 'predict_endpoint'. Before running a scan with this model, you should deploy the model and update the 'mlp' models metadata in the scan definition.\n"
     ]
    }
   ],
   "source": [
    "scan = CertifaiScanBuilder.from_file('explain-scan-def.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're running with local models in the notebook, we need to reload the models and reassociate them with the scan. If the models were running externally, we would instead update the scan definition with the correct predict_endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['logit', 'mlp']:\n",
    "    with open(f'readmission_{model_name}.pkl', 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "        model = CertifaiPredictorWrapper(saved.get('model'))\n",
    "        scan.remove_model(model_name)\n",
    "        scan.add_model(CertifaiModel(model_name, local_predictor=model))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run the Trust Scan\n",
    "\n",
    "In this Section, we're now going to modify the scan definition to run a scan for fairness, explainability and robustness. We could include explanations in this new scan, but for now we'll omit them. \n",
    "\n",
    "First, remove the explanation evaluation and add fairness, explainability and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.remove_evaluation_type('explanation')\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('explainability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this analysis, 'fairness' is less about discrimination against protected groups, but can be useful in detecting how specific groups may be more biased to unfavorable predictions. \n",
    "\n",
    "To scan for fairness, we need to provide some additional information on the groups to be scanned. We'll choose race, gender and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('gender'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results need to be interpreted with care.  [Racial/Ethnic Disparities in Readmissions in US Hospitals: The Role of Insurance Coverage](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5946640/) shows that lower readmission rates may not always be construed as a good outcome, and could relate to a lack of insurance coverage and poor access to care. \n",
    "\n",
    "Further, for a use case such as this, using an alternative measure such as equalized odds or equal opportunity instead of the default 'burden' based fairness metric may be appropriate. These measures will show where the model has different success rates in prediction across groups and can be measured with Certifai providing ground truth is available. The [fairness metrics notebook](https://github.com/CognitiveScale/cortex-certifai-examples/blob/master/notebooks/fairness_metrics/FairnessMetrics.ipynb) illustrates using alternative metrics.\n",
    "\n",
    "When running a fairness analysis, it is important that there are sufficient samples of each class within the grouping features. We can check this and other potential issues using a preflight analysis. It will also give us a time estimate for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preflight Scan\n",
      "[--------------------] 2020-11-02 13:15:38.681479 - 0 of 8 checks (0.0% complete) - Running model nondeterminism preflight check for model logit\n",
      "[##------------------] 2020-11-02 13:15:38.717233 - 1 of 8 checks (12.5% complete) - Running scan time estimate preflight check for model logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 13:15:40,726 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#####---------------] 2020-11-02 13:16:30.810780 - 2 of 8 checks (25.0% complete) - Running unknown outcome class preflight check for model logit\n",
      "[#######-------------] 2020-11-02 13:16:30.831550 - 3 of 8 checks (37.5% complete) - Running fairness class samples preflight check for model logit\n",
      "[##########----------] 2020-11-02 13:16:31.255051 - 4 of 8 checks (50.0% complete) - Finished all preflight checks for model logit\n",
      "[##########----------] 2020-11-02 13:16:31.255189 - 4 of 8 checks (50.0% complete) - Running model nondeterminism preflight check for model mlp\n",
      "[############--------] 2020-11-02 13:16:31.306776 - 5 of 8 checks (62.5% complete) - Running scan time estimate preflight check for model mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 13:16:33,496 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###############-----] 2020-11-02 13:17:28.652213 - 6 of 8 checks (75.0% complete) - Running unknown outcome class preflight check for model mlp\n",
      "[#################---] 2020-11-02 13:17:28.673776 - 7 of 8 checks (87.5% complete) - Running fairness class samples preflight check for model mlp\n",
      "[####################] 2020-11-02 13:17:29.055711 - 8 of 8 checks (100.0% complete) - Finished all preflight checks for model mlp\n",
      "{'logit': {'errors': [],\n",
      "           'messages': ['Passed model non determinism check',\n",
      "                        'Expected time for fairness analysis is 387 seconds',\n",
      "                        'Expected time for robustness analysis is 48 seconds',\n",
      "                        'Expected time for explainability analysis is 48 '\n",
      "                        'seconds',\n",
      "                        'Model logit passed time estimation check',\n",
      "                        'Passed unknown outcome classes check'],\n",
      "           'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                        \"size for 'Unknown/Invalid' with 3 examples\"]},\n",
      " 'mlp': {'errors': [],\n",
      "         'messages': ['Passed model non determinism check',\n",
      "                      'Expected time for fairness analysis is 422 seconds',\n",
      "                      'Expected time for robustness analysis is 51 seconds',\n",
      "                      'Expected time for explainability analysis is 51 seconds',\n",
      "                      'Model mlp passed time estimation check',\n",
      "                      'Passed unknown outcome classes check'],\n",
      "         'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                      \"size for 'Unknown/Invalid' with 3 examples\"]}}\n"
     ]
    }
   ],
   "source": [
    "preflight_result = scan.run_preflight()\n",
    "pprint.pprint(preflight_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the warnings, 'gender' has a small sample size (3) for 'Unknown/Invalid'. We will address this by dropping those rows, given they are a tiny proportion and are not a useful class for analysis. \n",
    "\n",
    "In other cases (e.g. for underrepresented age ranges), a better approach is to combine smaller classes into one larger class using bucketing. This is illustrated in the [Practical Issues](https://github.com/CognitiveScale/cortex-certifai-examples/blob/master/notebooks/practical_issues/PracticalIssues.ipynb) notebook.\n",
    "\n",
    "Drop rows with 'gender_Unknown/Invalid' and replace the evaluation dataset in the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetic_data_processed.csv')\n",
    "scan.remove_dataset('evaluation')\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                                CertifaiDatasetSource.dataframe(df[df['gender_Unknown/Invalid'] == 0]))\n",
    "scan.add_dataset(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan definition and run the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 13:17:29,748 root   WARNING  The dataset 'evaluation' was loaded from a dataframe and cannot be represented within a scan definition. A default value will be used in the exported YAML for the dataset. Before running a scan with the exported YAML, you should persist the dataset within a file and update the 'evaluation' datasets metadata.\n",
      "2020-11-02 13:17:29,753 root   WARNING  The model 'logit' was locally defined and cannot be represented within a scan definition because it doesn't have a 'predict_endpoint'. A default value of '<UNKNOWN_ENDPOINT>' will be used in the exported YAML for the models 'predict_endpoint'. Before running a scan with the exported YAML, you should deploy the model and update the 'logit' models metadata in the scan definition.\n",
      "2020-11-02 13:17:29,754 root   WARNING  The model 'mlp' was locally defined and cannot be represented within a scan definition because it doesn't have a 'predict_endpoint'. A default value of '<UNKNOWN_ENDPOINT>' will be used in the exported YAML for the models 'predict_endpoint'. Before running a scan with the exported YAML, you should deploy the model and update the 'mlp' models metadata in the scan definition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'readmission' and scan_id: 'eafa5614d892', total estimated time is 17 minutes\n",
      "[--------------------] 2020-11-02 13:17:35.132169 - 0 of 6 reports (0.0% complete) - Running fairness evaluation for model: logit, estimated time is 387 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 13:32:07,644 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2020-11-02 13:33:04,993 root   WARNING  Examples of protected class ('age', 15) exhausted before convergence after 691 samples\n",
      "2020-11-02 13:45:29,402 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2020-11-02 13:48:11,161 root   WARNING  Examples of protected class ('race', 'nan') exhausted before convergence after 2271 samples\n",
      "2020-11-02 13:48:11,163 root   WARNING  Examples of protected class ('race', 'Hispanic') exhausted before convergence after 2037 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###-----------------] 2020-11-02 13:48:51.449418 - 1 of 6 reports (16.67% complete) - Running robustness evaluation for model: logit, estimated time is 48 seconds\n",
      "[######--------------] 2020-11-02 13:49:43.340452 - 2 of 6 reports (33.33% complete) - Running explainability evaluation for model: logit, estimated time is 48 seconds\n",
      "[##########----------] 2020-11-02 13:50:52.592953 - 3 of 6 reports (50.0% complete) - Running fairness evaluation for model: mlp, estimated time is 422 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 13:56:25,022 root   WARNING  Examples of protected class ('age', 5) exhausted before convergence after 161 samples\n",
      "2020-11-02 14:11:24,090 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2020-11-02 14:12:36,193 root   WARNING  Examples of protected class ('age', 15) exhausted before convergence after 691 samples\n",
      "2020-11-02 14:24:00,122 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2020-11-02 14:24:32,709 root   WARNING  Examples of protected class ('race', 'nan') exhausted before convergence after 2271 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############-------] 2020-11-02 14:25:14.558623 - 4 of 6 reports (66.67% complete) - Running robustness evaluation for model: mlp, estimated time is 51 seconds\n",
      "[################----] 2020-11-02 14:26:08.660852 - 5 of 6 reports (83.33% complete) - Running explainability evaluation for model: mlp, estimated time is 51 seconds\n",
      "[####################] 2020-11-02 14:27:17.787133 - 6 of 6 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "with open('trust-scan-def.yaml', \"w\") as f:\n",
    "    scan.save(f)\n",
    "results = scan.run(write_reports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the Results\n",
    "\n",
    "The results can be viewed in the Certifai console using the CLI command `certifai console`, run from this folder. \n",
    "Go to `http://localhost:8000` in your browser. \n",
    "\n",
    "The results can also be analyzed in a notebook. See the [fifth notebook](patient-readmission-trust-results.ipynb) for how to load and work with the results of the trust score scan in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
