{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90d890b7e4ff4dbbb27e2b59e50e942c7bb0078c"
   },
   "source": [
    "Copyright (c) 2020. Cognitive Scale Inc. All rights reserved.\n",
    "Licensed under CognitiveScale Example Code [License](https://github.com/CognitiveScale/cortex-certifai-examples/blob/7998b8a481fccd467463deb1fc46d19622079b0e/LICENSE.md)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This fourth notebook shows how to scan models for their trust scores using Certifai, using a previously created scan definition as the starting point. \n",
    "\n",
    "If you have not already done so, please run the [first notebook](patient-readmission-train.ipynb) to train the models to be explained and the [second notebook](patient-readmission-explain-scan.ipynb) to create the `explain-scan-def.yaml` scan definition.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load the previously saved `explain-scan-def.yaml` scan definition and models\n",
    "2. Modify this scan definition to scan for the trust scores (fairness, explainability, robustness)\n",
    "4. View the results in the Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "fba09007b76132d605daa699b39a5fc92ff87ee1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiGroupingFeature, \n",
    "                                      CertifaiGroupingBucket, CertifaiPredictorWrapper, CertifaiModel, \n",
    "                                      CertifaiDataset, CertifaiDatasetSource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4490db86a8bb5bdc7bc81d17da9f0a3b690b90c"
   },
   "source": [
    "# Loading the Certifai Scan object\n",
    "\n",
    "In this section, we load the previously defined scan definition to use as a starting point. This is a convenience that avoids us having to recreate information about the prediction task, datasets and feature schema. \n",
    "\n",
    "Load the scan definition from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 07:59:39,701 root   WARNING  The model 'logit' was locally defined and doesn't have a valid 'predict_endpoint'. Before running a scan with this model, you should deploy the model and update the 'logit' models metadata in the scan definition.\n",
      "2021-05-13 07:59:39,702 root   WARNING  The model 'mlp' was locally defined and doesn't have a valid 'predict_endpoint'. Before running a scan with this model, you should deploy the model and update the 'mlp' models metadata in the scan definition.\n"
     ]
    }
   ],
   "source": [
    "scan = CertifaiScanBuilder.from_file('explain-scan-def.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're running with local models in the notebook, we need to reload the models and reassociate them with the scan. If the models were running externally, we would instead update the scan definition with the correct predict_endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['logit', 'mlp']:\n",
    "    with open(f'readmission_{model_name}.pkl', 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "        model = CertifaiPredictorWrapper(saved.get('model'))\n",
    "        scan.remove_model(model_name)\n",
    "        scan.add_model(CertifaiModel(model_name, local_predictor=model))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run the Trust Scan\n",
    "\n",
    "In this Section, we're now going to modify the scan definition to run a scan for fairness, explainability and robustness. We could include explanations in this new scan, but for now we'll omit them. \n",
    "\n",
    "First, remove the explanation evaluation and add fairness, explainability and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.remove_evaluation_type('explanation')\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('explainability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this analysis, 'fairness' is less about discrimination against protected groups, but can be useful in detecting how specific groups may be more biased to unfavorable predictions. \n",
    "\n",
    "To scan for fairness, we need to provide some additional information on the groups to be scanned. We'll choose race, gender and age.\n",
    "\n",
    "For age, we need to create buckets because it is an integer field with values between 5 and 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('gender'))\n",
    "\n",
    "buckets = []\n",
    "buckets.append(CertifaiGroupingBucket(\"05\", max=5))\n",
    "for b in range(10, 91, 10):\n",
    "    buckets.append(CertifaiGroupingBucket(f\"{b}\", max=b))\n",
    "buckets.append(CertifaiGroupingBucket(\"95\"))\n",
    "\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age', buckets=buckets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results need to be interpreted with care.  [Racial/Ethnic Disparities in Readmissions in US Hospitals: The Role of Insurance Coverage](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5946640/) shows that lower readmission rates may not always be construed as a good outcome, and could relate to a lack of insurance coverage and poor access to care. \n",
    "\n",
    "Further, for a use case such as this, using an alternative measure such as equalized odds or equal opportunity instead of the default 'burden' based fairness metric may be appropriate. These measures will show where the model has different success rates in prediction across groups and can be measured with Certifai providing ground truth is available. The [fairness metrics notebook](https://github.com/CognitiveScale/cortex-certifai-examples/blob/master/notebooks/fairness_metrics/FairnessMetrics.ipynb) illustrates using alternative metrics.\n",
    "\n",
    "When running a fairness analysis, it is important that there are sufficient samples of each class within the grouping features. We can check this and other potential issues using a preflight analysis. It will also give us a time estimate for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preflight Scan\n",
      "[--------------------] 2021-05-13 08:01:43.096590 - 0 of 10 checks (0.0% complete) - Running model nondeterminism preflight check for model logit\n",
      "[##------------------] 2021-05-13 08:01:43.159509 - 1 of 10 checks (10.0% complete) - Running scan time estimate preflight check for model logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 08:01:43,548 root   WARNING  Bucket 10 of fairness grouping feature age is unrepresented in the data\n",
      "2021-05-13 08:01:45,158 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest included class size is for gender='Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####----------------] 2021-05-13 08:02:37.704647 - 2 of 10 checks (20.0% complete) - Running one hot checker class preflight check for model logit\n",
      "[######--------------] 2021-05-13 08:02:37.705070 - 3 of 10 checks (30.0% complete) - Running unknown outcome class preflight check for model logit\n",
      "[########------------] 2021-05-13 08:02:37.737028 - 4 of 10 checks (40.0% complete) - Running fairness class samples preflight check for model logit\n",
      "[##########----------] 2021-05-13 08:02:38.318612 - 5 of 10 checks (50.0% complete) - Finished all preflight checks for model logit\n",
      "[##########----------] 2021-05-13 08:02:38.318837 - 5 of 10 checks (50.0% complete) - Running model nondeterminism preflight check for model mlp\n",
      "[############--------] 2021-05-13 08:02:38.380013 - 6 of 10 checks (60.0% complete) - Running scan time estimate preflight check for model mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 08:02:38,776 root   WARNING  Bucket 10 of fairness grouping feature age is unrepresented in the data\n",
      "2021-05-13 08:02:40,384 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest included class size is for gender='Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##############------] 2021-05-13 08:03:37.153546 - 7 of 10 checks (70.0% complete) - Running one hot checker class preflight check for model mlp\n",
      "[################----] 2021-05-13 08:03:37.153961 - 8 of 10 checks (80.0% complete) - Running unknown outcome class preflight check for model mlp\n",
      "[##################--] 2021-05-13 08:03:37.183810 - 9 of 10 checks (90.0% complete) - Running fairness class samples preflight check for model mlp\n",
      "[####################] 2021-05-13 08:03:37.770318 - 10 of 10 checks (100.0% complete) - Finished all preflight checks for model mlp\n",
      "{'logit': {'errors': [],\n",
      "           'messages': ['Passed model non determinism check',\n",
      "                        'Expected time for fairness analysis is 388 seconds',\n",
      "                        'Expected time for robustness analysis is 48 seconds',\n",
      "                        'Expected time for explainability analysis is 48 '\n",
      "                        'seconds',\n",
      "                        'Model logit passed time estimation check',\n",
      "                        'Passed unknown outcome classes check'],\n",
      "           'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                        \"size for 'Unknown/Invalid' with 3 examples\",\n",
      "                        \"Fairness grouping attribute 'age' has small sample \"\n",
      "                        \"size for '10' with 0 examples\"]},\n",
      " 'mlp': {'errors': [],\n",
      "         'messages': ['Passed model non determinism check',\n",
      "                      'Expected time for fairness analysis is 494 seconds',\n",
      "                      'Expected time for robustness analysis is 58 seconds',\n",
      "                      'Expected time for explainability analysis is 52 seconds',\n",
      "                      'Model mlp passed time estimation check',\n",
      "                      'Passed unknown outcome classes check'],\n",
      "         'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                      \"size for 'Unknown/Invalid' with 3 examples\",\n",
      "                      \"Fairness grouping attribute 'age' has small sample size \"\n",
      "                      \"for '10' with 0 examples\"]}}\n"
     ]
    }
   ],
   "source": [
    "preflight_result = scan.run_preflight()\n",
    "pprint.pprint(preflight_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the warnings, 'gender' has a small sample size (3) for 'Unknown/Invalid'. We will address this by dropping those rows, given they are a tiny proportion and are not a useful class for analysis. \n",
    "\n",
    "In other cases (e.g. for underrepresented age ranges), a better approach is to combine smaller classes into one larger class using bucketing. This is illustrated in the [Practical Issues](https://github.com/CognitiveScale/cortex-certifai-examples/blob/master/notebooks/practical_issues/PracticalIssues.ipynb) notebook.\n",
    "\n",
    "Drop rows with 'gender_Unknown/Invalid' and replace the evaluation dataset in the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetic_data_processed.csv')\n",
    "scan.remove_dataset('evaluation')\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                                CertifaiDatasetSource.dataframe(df[df['gender_Unknown/Invalid'] == 0]))\n",
    "scan.add_dataset(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan definition and run the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 08:07:15,995 root   WARNING  The dataset 'evaluation' was loaded from a dataframe and cannot be represented within a scan definition. A default value will be used in the exported YAML for the dataset. Before running a scan with the exported YAML, you should persist the dataset within a file and update the 'evaluation' datasets metadata.\n",
      "2021-05-13 08:07:15,998 root   WARNING  The model 'logit' was locally defined and cannot be represented within a scan definition because it doesn't have a 'predict_endpoint'. A default value of '<UNKNOWN_ENDPOINT>' will be used in the exported YAML for the models 'predict_endpoint'. Before running a scan with the exported YAML, you should deploy the model and update the 'logit' models metadata in the scan definition.\n",
      "2021-05-13 08:07:15,999 root   WARNING  The model 'mlp' was locally defined and cannot be represented within a scan definition because it doesn't have a 'predict_endpoint'. A default value of '<UNKNOWN_ENDPOINT>' will be used in the exported YAML for the models 'predict_endpoint'. Before running a scan with the exported YAML, you should deploy the model and update the 'mlp' models metadata in the scan definition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'readmission' and scan_id: '8738ee6b298a', total estimated time is 19 minutes\n",
      "[--------------------] 2021-05-13 08:07:19.029479 - 0 of 6 reports (0.0% complete) - Running fairness evaluation for model: logit, estimated time is 388 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 08:07:19,385 root   WARNING  Bucket Unknown/Invalid of fairness grouping feature gender is unrepresented in the data\n",
      "2021-05-13 08:07:19,434 root   WARNING  Bucket 10 of fairness grouping feature age is unrepresented in the data\n",
      "2021-05-13 08:09:50,644 root   WARNING  Examples of protected class ('age', '05') exhausted before convergence after 161 samples\n",
      "2021-05-13 08:20:06,956 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2021-05-13 08:21:11,916 root   WARNING  Examples of protected class ('age', '20') exhausted before convergence after 691 samples\n",
      "2021-05-13 08:36:00,064 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2021-05-13 08:39:59,063 root   WARNING  Examples of protected class ('race', 'Hispanic') exhausted before convergence after 2037 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###-----------------] 2021-05-13 08:43:05.583217 - 1 of 6 reports (16.67% complete) - Running robustness evaluation for model: logit, estimated time is 48 seconds\n",
      "[######--------------] 2021-05-13 08:44:12.087204 - 2 of 6 reports (33.33% complete) - Running explainability evaluation for model: logit, estimated time is 48 seconds\n",
      "[##########----------] 2021-05-13 08:45:34.975498 - 3 of 6 reports (50.0% complete) - Running fairness evaluation for model: mlp, estimated time is 494 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 08:45:35,451 root   WARNING  Bucket Unknown/Invalid of fairness grouping feature gender is unrepresented in the data\n",
      "2021-05-13 08:45:35,514 root   WARNING  Bucket 10 of fairness grouping feature age is unrepresented in the data\n",
      "2021-05-13 08:49:19,685 root   WARNING  Examples of protected class ('age', '05') exhausted before convergence after 161 samples\n",
      "2021-05-13 09:02:59,019 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2021-05-13 09:04:14,235 root   WARNING  Examples of protected class ('age', '20') exhausted before convergence after 690 samples\n",
      "2021-05-13 09:19:18,745 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############-------] 2021-05-13 09:21:00.866015 - 4 of 6 reports (66.67% complete) - Running robustness evaluation for model: mlp, estimated time is 58 seconds\n",
      "[################----] 2021-05-13 09:24:24.680384 - 5 of 6 reports (83.33% complete) - Running explainability evaluation for model: mlp, estimated time is 52 seconds\n",
      "[####################] 2021-05-13 09:25:52.309533 - 6 of 6 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "with open('trust-scan-def.yaml', \"w\") as f:\n",
    "    scan.save(f)\n",
    "results = scan.run(write_reports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the Results\n",
    "\n",
    "The results can be viewed in the Certifai console using the CLI command `certifai console`, run from this folder. \n",
    "Go to `http://localhost:8000` in your browser. \n",
    "\n",
    "The results can also be analyzed in a notebook. See the [fifth notebook](patient-readmission-trust-results.ipynb) for how to load and work with the results of the trust score scan in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
