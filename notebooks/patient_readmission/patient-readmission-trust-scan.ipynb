{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90d890b7e4ff4dbbb27e2b59e50e942c7bb0078c"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This fourth notebook shows how to scan models for their trust scores using Certifai, using a previously created scan definition as the starting point. \n",
    "\n",
    "If you have not already done so, please run the [first notebook](patient-readmission-train.ipynb) to train the models to be explained and the [second notebook](patient-readmission-explain-scan.ipynb) to create the `explain-scan-def.yaml` scan definition.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load the previously saved `explain-scan-def.yaml` scan definition and models\n",
    "2. Modify this scan definition to scan for the trust scores (fairness, explainability, robustness)\n",
    "4. View the results in the Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "fba09007b76132d605daa699b39a5fc92ff87ee1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiGroupingFeature, \n",
    "                                      CertifaiPredictorWrapper, CertifaiModel, \n",
    "                                      CertifaiDataset, CertifaiDatasetSource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4490db86a8bb5bdc7bc81d17da9f0a3b690b90c"
   },
   "source": [
    "# Loading the Certifai Scan object\n",
    "\n",
    "In this section, we load the previously defined scan definition to use as a starting point. This is a convenience that avoids us having to recreate information about the prediction task, datasets and feature schema. \n",
    "\n",
    "Load the scan definition from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = CertifaiScanBuilder.from_file('explain-scan-def.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're running with local models in the notebook, we need to reload the models and reassociate them with the scan. This isn't necessary if the models are running externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "\n",
    "for model_name in ['logit', 'mlp']:\n",
    "    with open(f'readmission_{model_name}.pkl', 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "        model = CertifaiPredictorWrapper(saved.get('model'), encoder=Encoder())\n",
    "        scan.remove_model(model_name)\n",
    "        scan.add_model(CertifaiModel(model_name, local_predictor=model))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run the Trust Scan\n",
    "\n",
    "In this Section, we're now going to modify the scan definition to run a scan for fairness, explainability and robustness. We could include explanations in this new scan, but for now we'll omit them. \n",
    "\n",
    "First, remove the explanation evaluation and add fairness, explainability and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.remove_evaluation_type('explanation')\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('explainability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this analysis, 'fairness' is less about discrimination against protected groups, but can be useful in detecting how specific groups may be more biased to unfavorable predictions. \n",
    "\n",
    "To scan for fairness, we need to provide some additional information on the groups to be scanned. We'll choose race, gender and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('gender'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results need to be interpreted with care.  [Racial/Ethnic Disparities in Readmissions in US Hospitals: The Role of Insurance Coverage](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5946640/) shows that lower readmission rates may not always be construed as a good outcome, and could relate to a lack of insurance coverage and poor access to care. \n",
    "\n",
    "Further, for a use case such as this, using an alternative measure such as equalized odds or equal opportunity instead of the default 'burden' based fairness metric may be appropriate. These measures will show where the model has different success rates in prediction across groups and can be measured with Certifai providing ground truth is available. The [fairness metrics notebook](https://github.com/CognitiveScale/cortex-certifai-examples/blob/master/notebooks/fairness_metrics/FairnessMetrics.ipynb) illustrates using alternative metrics.\n",
    "\n",
    "When running a fairness analysis, it is important that there are sufficient samples of each class within the grouping features. We can check this and other potential issues using a preflight analysis. It will also give us a time estimate for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preflight Scan\n",
      "[--------------------] 2020-10-05 11:27:19.280342 - 0 of 8 checks (0.0% complete) - Running model nondeterminism preflight check for model logit\n",
      "[##------------------] 2020-10-05 11:27:19.323434 - 1 of 8 checks (12.5% complete) - Running scan time estimate preflight check for model logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-05 11:27:21,543 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#####---------------] 2020-10-05 11:28:17.745457 - 2 of 8 checks (25.0% complete) - Running unknown outcome class preflight check for model logit\n",
      "[#######-------------] 2020-10-05 11:28:17.767609 - 3 of 8 checks (37.5% complete) - Running fairness class samples preflight check for model logit\n",
      "[##########----------] 2020-10-05 11:28:18.137250 - 4 of 8 checks (50.0% complete) - Finished all preflight checks for model logit\n",
      "[##########----------] 2020-10-05 11:28:18.137420 - 4 of 8 checks (50.0% complete) - Running model nondeterminism preflight check for model mlp\n",
      "[############--------] 2020-10-05 11:28:18.180570 - 5 of 8 checks (62.5% complete) - Running scan time estimate preflight check for model mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-05 11:28:20,422 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'Unknown/Invalid' with 3 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###############-----] 2020-10-05 11:29:19.132059 - 6 of 8 checks (75.0% complete) - Running unknown outcome class preflight check for model mlp\n",
      "[#################---] 2020-10-05 11:29:19.154725 - 7 of 8 checks (87.5% complete) - Running fairness class samples preflight check for model mlp\n",
      "[####################] 2020-10-05 11:29:19.553497 - 8 of 8 checks (100.0% complete) - Finished all preflight checks for model mlp\n",
      "{'logit': {'errors': [],\n",
      "           'messages': ['Passed model non determinism check',\n",
      "                        'Expected time for fairness analysis is 514 seconds',\n",
      "                        'Expected time for robustness analysis is 60 seconds',\n",
      "                        'Expected time for explainability analysis is 56 '\n",
      "                        'seconds',\n",
      "                        'Model logit passed time estimation check',\n",
      "                        'Passed unknown outcome classes check'],\n",
      "           'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                        \"size for 'Unknown/Invalid' with 3 examples\"]},\n",
      " 'mlp': {'errors': [],\n",
      "         'messages': ['Passed model non determinism check',\n",
      "                      'Expected time for fairness analysis is 558 seconds',\n",
      "                      'Expected time for robustness analysis is 65 seconds',\n",
      "                      'Expected time for explainability analysis is 59 seconds',\n",
      "                      'Model mlp passed time estimation check',\n",
      "                      'Passed unknown outcome classes check'],\n",
      "         'warnings': [\"Fairness grouping attribute 'gender' has small sample \"\n",
      "                      \"size for 'Unknown/Invalid' with 3 examples\"]}}\n"
     ]
    }
   ],
   "source": [
    "preflight_result = scan.run_preflight()\n",
    "pprint.pprint(preflight_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the warnings, 'gender' has a small sample size (3) for 'Unknown/Invalid'. We will address this by dropping those rows, given they are a tiny proportion and are not a useful class for analysis. \n",
    "\n",
    "In other cases (e.g. for underrepresented age ranges), a better approach is to combine smaller classes into one larger class using bucketing. This is illustrated in the [Practical Issues](cortex-certifai-examples/notebooks/practical_issues/PracticalIssues.ipynb) notebook.\n",
    "\n",
    "Drop rows with 'gender_Unknown/Invalid' and replace the evaluation dataset in the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetic_data_processed.csv')\n",
    "scan.remove_dataset('evaluation')\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                                CertifaiDatasetSource.dataframe(df[df['gender_Unknown/Invalid'] == 0]))\n",
    "scan.add_dataset(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan definition and run the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'readmission' and scan_id: '5fe6e58129e1', total estimated time is 22 minutes\n",
      "[--------------------] 2020-10-05 11:29:25.819966 - 0 of 6 reports (0.0% complete) - Running fairness evaluation for model: logit, estimated time is 514 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-05 11:48:36,038 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2020-10-05 11:50:09,159 root   WARNING  Examples of protected class ('age', 15) exhausted before convergence after 691 samples\n",
      "2020-10-05 12:10:38,304 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2020-10-05 12:12:50,460 root   WARNING  Examples of protected class ('age', 25) exhausted before convergence after 1657 samples\n",
      "2020-10-05 12:16:37,666 root   WARNING  Examples of protected class ('race', 'Hispanic') exhausted before convergence after 2037 samples\n",
      "2020-10-05 12:17:36,724 root   WARNING  Examples of protected class ('race', 'nan') exhausted before convergence after 2271 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###-----------------] 2020-10-05 12:18:25.051782 - 1 of 6 reports (16.67% complete) - Running robustness evaluation for model: logit, estimated time is 60 seconds\n",
      "[######--------------] 2020-10-05 12:20:15.577666 - 2 of 6 reports (33.33% complete) - Running explainability evaluation for model: logit, estimated time is 56 seconds\n",
      "[##########----------] 2020-10-05 12:21:42.948512 - 3 of 6 reports (50.0% complete) - Running fairness evaluation for model: mlp, estimated time is 558 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-05 12:28:29,628 root   WARNING  Examples of protected class ('age', 5) exhausted before convergence after 161 samples\n",
      "2020-10-05 12:47:01,630 root   WARNING  Examples of protected class ('race', 'Asian') exhausted before convergence after 641 samples\n",
      "2020-10-05 12:49:04,186 root   WARNING  Examples of protected class ('age', 15) exhausted before convergence after 691 samples\n",
      "2020-10-05 13:06:48,342 root   WARNING  Examples of protected class ('race', 'Other') exhausted before convergence after 1505 samples\n",
      "2020-10-05 13:08:04,402 root   WARNING  Examples of protected class ('race', 'nan') exhausted before convergence after 2271 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############-------] 2020-10-05 13:08:48.375841 - 4 of 6 reports (66.67% complete) - Running robustness evaluation for model: mlp, estimated time is 65 seconds\n",
      "[################----] 2020-10-05 13:10:01.534529 - 5 of 6 reports (83.33% complete) - Running explainability evaluation for model: mlp, estimated time is 59 seconds\n",
      "[####################] 2020-10-05 13:11:26.829221 - 6 of 6 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "with open('trust-scan-def.yaml', \"w\") as f:\n",
    "    scan.save(f)\n",
    "results = scan.run(write_reports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the Results\n",
    "\n",
    "The results can be viewed in the Certifai console using the CLI command `certifai console`, run from this folder. \n",
    "Go to `http://localhost:8000` in your browser. \n",
    "\n",
    "The results can also be analyzed in a notebook. See the [fifth notebook](patient-readmission-trust-results.ipynb) for how to load and work with the results of the trust score scan in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
