{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020. Cognitive Scale Inc. All rights reserved.\n",
    "Licensed under CognitiveScale Example Code [License](https://github.com/CognitiveScale/cortex-certifai-examples/blob/7998b8a481fccd467463deb1fc46d19622079b0e/LICENSE.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a scan programatically for a regression use case\n",
    "\n",
    "In this notebook we'll go through the end-to-end process of building a local model for a regression use case and running a Certifai Scan said models to analyze Robustness, Fairness, Explainability, Explanation, and Perfomance. The models will be predicting the final settled claim amount for auto insurance claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from certifai.common.utils.encoding import CatEncoder\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe\n",
    "from certifai.scanner.explanation_utils import explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7623698392411863\n",
      "0.7422361025338485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations. Duality gap: 9789028.07067509, tolerance: 62512.58675713785\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets for test/train split\n",
    "base_path = '..'\n",
    "all_data_file = f\"{base_path}/datasets/auto_insurance_claims_dataset.csv\"\n",
    "explanation_data_file = f\"{base_path}/datasets/auto_insurance_explan.csv\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "cat_columns = [\n",
    "    'State Code',\n",
    "    'Coverage',\n",
    "    'Education',\n",
    "    'EmploymentStatus',\n",
    "    'Gender',\n",
    "    'Location Code',\n",
    "    'Marital Status',\n",
    "    'Policy',\n",
    "    'Claim Reason',\n",
    "    'Sales Channel',\n",
    "    'Vehicle Class',\n",
    "    'Vehicle Size',\n",
    "]\n",
    "label_column = \"Total Claim Amount\"\n",
    "\n",
    "\n",
    "Y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "\n",
    "encoder = CatEncoder(cat_columns, X)\n",
    "\n",
    "# Train models\n",
    "linl1_model = Lasso(alpha=1e-4, random_state=RANDOM_SEED)\n",
    "linl1_model.fit(encoder(X_train.values), Y_train)\n",
    "linl1_r2_score = r2_score(Y_test, linl1_model.predict(encoder(X_test.values)))\n",
    "print(linl1_r2_score)\n",
    "\n",
    "svm_model = LinearSVR(random_state=RANDOM_SEED)\n",
    "svm_model.fit(encoder(X_train.values), Y_train)\n",
    "svm_r2_score = r2_score(Y_test, svm_model.predict(encoder(X_test.values)))\n",
    "print(svm_r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the models for use by Certifai as a local model\n",
    "linl1_model_proxy = CertifaiPredictorWrapper(linl1_model, encoder=encoder)\n",
    "svm_model_proxy = CertifaiPredictorWrapper(svm_model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scan object from scratch using the ScanBuilder class\n",
    "\n",
    "# Here we define the favorable direction to be increasing, and consider\n",
    "# a change of 0.5 standard deviations to be significant \n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.regression(True, 0.5),\n",
    "                              prediction_description='Amount of Settled Claim')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "\n",
    "# Add our local models\n",
    "first_model = CertifaiModel('LinL1', local_predictor=linl1_model_proxy)\n",
    "scan.add_model(first_model)\n",
    "\n",
    "second_model = CertifaiModel('SVM', local_predictor=svm_model_proxy)\n",
    "scan.add_model(second_model)\n",
    "\n",
    "\n",
    "# Add datasets to the scan\n",
    "eval_dataset = CertifaiDataset('evaluation', CertifaiDatasetSource.csv(all_data_file))\n",
    "scan.add_dataset(eval_dataset)\n",
    "scan.evaluation_dataset_id = eval_dataset.id\n",
    "\n",
    "explan_dataset = CertifaiDataset('explanation', CertifaiDatasetSource.csv(explanation_data_file))\n",
    "scan.add_dataset(explan_dataset)\n",
    "scan.explanation_dataset_id = explan_dataset.id\n",
    "\n",
    "# Here we are using a dataframe (from our test/train split) instead of a CSV file for the performance evaluation\n",
    "test_df = pd.concat([X_test, Y_test], axis=1)\n",
    "test_dataset = CertifaiDataset('test', CertifaiDatasetSource.dataframe(test_df))\n",
    "scan.add_dataset(test_dataset)\n",
    "scan.test_dataset_id = test_dataset.id\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = 'Total Claim Amount'\n",
    "\n",
    "\n",
    "# Setup an evaluation that includes Robustness, Explainability, Explanations, Fairness and Performance\n",
    "scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('explainability')\n",
    "scan.add_evaluation_type('explanation')\n",
    "\n",
    "# We'll look at disparity between groups defined by Marital Status and Gender in the fairness evaluation\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('Marital Status'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('Gender'))\n",
    "\n",
    "# We'll compute the r-squared metric in the performance evaluation and in doing so\n",
    "# verify that it matches the value computed earlier during testing\n",
    "scan.add_evaluation_type('performance')\n",
    "scan.add_metric(CertifaiModelMetric('r2', 'r-squared'))\n",
    "scan.atx_performance_metric = 'r2'\n",
    "\n",
    "\n",
    "# Set the output path that reports should be written to\n",
    "scan.output_path = '../local_reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preflight Scan\n",
      "[--------------------] 2020-08-31 13:34:46.338564 - 0 of 8 checks (0.0% complete) - Running model nondeterminism preflight check for model LinL1\n",
      "[##------------------] 2020-08-31 13:34:46.390290 - 1 of 8 checks (12.5% complete) - Running unknown outcome class preflight check for model LinL1\n",
      "[#####---------------] 2020-08-31 13:34:46.390466 - 2 of 8 checks (25.0% complete) - Running scan time estimate preflight check for model LinL1\n",
      "[#######-------------] 2020-08-31 13:35:30.132985 - 3 of 8 checks (37.5% complete) - Running fairness class samples preflight check for model LinL1\n",
      "[##########----------] 2020-08-31 13:35:30.147065 - 4 of 8 checks (50.0% complete) - Finished all preflight checks for model LinL1\n",
      "[##########----------] 2020-08-31 13:35:30.147242 - 4 of 8 checks (50.0% complete) - Running model nondeterminism preflight check for model SVM\n",
      "[############--------] 2020-08-31 13:35:30.201300 - 5 of 8 checks (62.5% complete) - Running unknown outcome class preflight check for model SVM\n",
      "[###############-----] 2020-08-31 13:35:30.201494 - 6 of 8 checks (75.0% complete) - Running scan time estimate preflight check for model SVM\n",
      "[#################---] 2020-08-31 13:36:14.332227 - 7 of 8 checks (87.5% complete) - Running fairness class samples preflight check for model SVM\n",
      "[####################] 2020-08-31 13:36:14.348183 - 8 of 8 checks (100.0% complete) - Finished all preflight checks for model SVM\n",
      "{'LinL1': {'errors': [],\n",
      "           'messages': ['Passed model non determinism check',\n",
      "                        'Passed unknown outcome classes check',\n",
      "                        'Expected time for robustness analysis is 53 seconds',\n",
      "                        'Expected time for explainability analysis is 53 seconds',\n",
      "                        'Expected time for explanation analysis is 44 seconds',\n",
      "                        'Expected time for fairness analysis is 139 seconds',\n",
      "                        'Expected time for performance analysis is 10 seconds',\n",
      "                        'Model LinL1 passed time estimation check',\n",
      "                        'Passed fairness class samples check'],\n",
      "           'warnings': []},\n",
      " 'SVM': {'errors': [],\n",
      "         'messages': ['Passed model non determinism check',\n",
      "                      'Passed unknown outcome classes check',\n",
      "                      'Expected time for robustness analysis is 53 seconds',\n",
      "                      'Expected time for explainability analysis is 53 seconds',\n",
      "                      'Expected time for explanation analysis is 44 seconds',\n",
      "                      'Expected time for fairness analysis is 140 seconds',\n",
      "                      'Expected time for performance analysis is 10 seconds',\n",
      "                      'Model SVM passed time estimation check',\n",
      "                      'Passed fairness class samples check'],\n",
      "         'warnings': []}}\n"
     ]
    }
   ],
   "source": [
    "# Run a preflight scan\n",
    "# The preflight scan will run various checks against each model to alert about possible situations\n",
    "# that may impact your scan, as well as produce a time estimate for your scan. A preflight report\n",
    "# will be written per model that contains various data from the preflight scan, which will be\n",
    "# used to offer time estimates when running future scans.\n",
    "preflight_result = scan.run_preflight()\n",
    "\n",
    "pprint.pprint(preflight_result, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'test_user_case' and scan_id: 'a51d36316a84', total estimated time is 10 minutes\n",
      "[--------------------] 2020-08-31 13:36:14.593370 - 0 of 10 reports (0.0% complete) - Running robustness evaluation for model: LinL1, estimated time is 53 seconds\n",
      "[##------------------] 2020-08-31 13:37:03.426579 - 1 of 10 reports (10.0% complete) - Running explainability evaluation for model: LinL1, estimated time is 53 seconds\n",
      "[####----------------] 2020-08-31 13:38:42.393185 - 2 of 10 reports (20.0% complete) - Running explanation evaluation for model: LinL1, estimated time is 44 seconds\n",
      "[######--------------] 2020-08-31 13:39:59.921306 - 3 of 10 reports (30.0% complete) - Running fairness evaluation for model: LinL1, estimated time is 139 seconds\n",
      "[########------------] 2020-08-31 13:42:14.903681 - 4 of 10 reports (40.0% complete) - Running performance evaluation for model: LinL1, estimated time is 10 seconds\n",
      "[##########----------] 2020-08-31 13:42:14.915923 - 5 of 10 reports (50.0% complete) - Running robustness evaluation for model: SVM, estimated time is 53 seconds\n",
      "[############--------] 2020-08-31 13:43:04.360021 - 6 of 10 reports (60.0% complete) - Running explainability evaluation for model: SVM, estimated time is 53 seconds\n",
      "[##############------] 2020-08-31 13:44:46.066077 - 7 of 10 reports (70.0% complete) - Running explanation evaluation for model: SVM, estimated time is 44 seconds\n",
      "[################----] 2020-08-31 13:45:58.637803 - 8 of 10 reports (80.0% complete) - Running fairness evaluation for model: SVM, estimated time is 140 seconds\n",
      "[##################--] 2020-08-31 13:48:00.137813 - 9 of 10 reports (90.0% complete) - Running performance evaluation for model: SVM, estimated time is 10 seconds\n",
      "[####################] 2020-08-31 13:48:00.149139 - 10 of 10 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'local_reports' directory relative to this notebook. This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "result = scan.run(write_reports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>ATX</th>\n",
       "      <th>explainability</th>\n",
       "      <th>fairness</th>\n",
       "      <th>performance</th>\n",
       "      <th>robustness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinL1</th>\n",
       "      <td>LinL1</td>\n",
       "      <td>90.461219</td>\n",
       "      <td>99.430894</td>\n",
       "      <td>97.114499</td>\n",
       "      <td>76.236984</td>\n",
       "      <td>89.06250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>SVM</td>\n",
       "      <td>89.181263</td>\n",
       "      <td>98.693878</td>\n",
       "      <td>97.088815</td>\n",
       "      <td>74.223610</td>\n",
       "      <td>86.71875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      context        ATX  explainability   fairness  performance  robustness\n",
       "LinL1   LinL1  90.461219       99.430894  97.114499    76.236984    89.06250\n",
       "SVM       SVM  89.181263       98.693878  97.088815    74.223610    86.71875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the scores for each model and evaluation type, as well as the overall ATX score\n",
    "atx_df = construct_scores_dataframe(scores(\"atx\", result))\n",
    "display(atx_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>robustness</th>\n",
       "      <th>robustness lower bound</th>\n",
       "      <th>robustness upper bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinL1</th>\n",
       "      <td>LinL1</td>\n",
       "      <td>89.06250</td>\n",
       "      <td>87.65625</td>\n",
       "      <td>90.46875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>SVM</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>85.31250</td>\n",
       "      <td>88.12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      context  robustness  robustness lower bound  robustness upper bound\n",
       "LinL1   LinL1    89.06250                87.65625                90.46875\n",
       "SVM       SVM    86.71875                85.31250                88.12500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the results of the performance evaluation for both models\n",
    "robustness_df = construct_scores_dataframe(scores(\"robustness\", result))\n",
    "display(robustness_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>explainability</th>\n",
       "      <th>Num features (1)</th>\n",
       "      <th>Num features (10)</th>\n",
       "      <th>Num features (2)</th>\n",
       "      <th>Num features (3)</th>\n",
       "      <th>Num features (4)</th>\n",
       "      <th>Num features (5)</th>\n",
       "      <th>Num features (6)</th>\n",
       "      <th>Num features (7)</th>\n",
       "      <th>Num features (8)</th>\n",
       "      <th>Num features (9)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinL1</th>\n",
       "      <td>LinL1</td>\n",
       "      <td>99.430894</td>\n",
       "      <td>97.967480</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>1.626016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>SVM</td>\n",
       "      <td>98.693878</td>\n",
       "      <td>86.938776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.061224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      context  explainability  Num features (1)  Num features (10)  \\\n",
       "LinL1   LinL1       99.430894         97.967480           0.406504   \n",
       "SVM       SVM       98.693878         86.938776           0.000000   \n",
       "\n",
       "       Num features (2)  Num features (3)  Num features (4)  Num features (5)  \\\n",
       "LinL1          1.626016               0.0               0.0               0.0   \n",
       "SVM           13.061224               0.0               0.0               0.0   \n",
       "\n",
       "       Num features (6)  Num features (7)  Num features (8)  Num features (9)  \n",
       "LinL1               0.0               0.0               0.0               0.0  \n",
       "SVM                 0.0               0.0               0.0               0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the results of the explainability evaluation for both models\n",
    "# The table displays the distributions over the number of features that must be changed to alter predictions\n",
    "explainability_df = construct_scores_dataframe(scores(\"explainability\", result))\n",
    "display(explainability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>type</th>\n",
       "      <th>overall fairness</th>\n",
       "      <th>Feature (Gender)</th>\n",
       "      <th>Group details (F)</th>\n",
       "      <th>Group details (M)</th>\n",
       "      <th>Feature (Marital Status)</th>\n",
       "      <th>Group details (Divorced)</th>\n",
       "      <th>Group details (Married)</th>\n",
       "      <th>Group details (Single)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinL1 (burden)</th>\n",
       "      <td>LinL1</td>\n",
       "      <td>burden</td>\n",
       "      <td>97.114499</td>\n",
       "      <td>97.963882</td>\n",
       "      <td>0.170481</td>\n",
       "      <td>0.163670</td>\n",
       "      <td>97.740154</td>\n",
       "      <td>0.170462</td>\n",
       "      <td>0.161783</td>\n",
       "      <td>0.168504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM (burden)</th>\n",
       "      <td>SVM</td>\n",
       "      <td>burden</td>\n",
       "      <td>97.088815</td>\n",
       "      <td>97.403870</td>\n",
       "      <td>0.175995</td>\n",
       "      <td>0.167114</td>\n",
       "      <td>98.129803</td>\n",
       "      <td>0.175060</td>\n",
       "      <td>0.168074</td>\n",
       "      <td>0.171106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               context    type  overall fairness  Feature (Gender)  \\\n",
       "LinL1 (burden)   LinL1  burden         97.114499         97.963882   \n",
       "SVM (burden)       SVM  burden         97.088815         97.403870   \n",
       "\n",
       "                Group details (F)  Group details (M)  \\\n",
       "LinL1 (burden)           0.170481           0.163670   \n",
       "SVM (burden)             0.175995           0.167114   \n",
       "\n",
       "                Feature (Marital Status)  Group details (Divorced)  \\\n",
       "LinL1 (burden)                 97.740154                  0.170462   \n",
       "SVM (burden)                   98.129803                  0.175060   \n",
       "\n",
       "                Group details (Married)  Group details (Single)  \n",
       "LinL1 (burden)                 0.161783                0.168504  \n",
       "SVM (burden)                   0.168074                0.171106  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the fairness results of fairness evaluation for both models\n",
    "# The table displays the fairness scores across different groups within the \"Marital Status\" and \"Gender\" features. \n",
    "# You can set `include_confidence=True` to include confidence intervals for each burden value\n",
    "fairness_df = construct_scores_dataframe(scores(\"fairness\", result), include_confidence=False)\n",
    "display(fairness_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinL1 r2_score:  0.7623698392411863\n",
      "SVM r2_score:  0.7422361025338485\n"
     ]
    }
   ],
   "source": [
    "# Here we will display the r-squared value computed during the performance evaluation\n",
    "print('LinL1 r2_score: ', result['performance']['LinL1']['performance_metrics'][0]['value'])\n",
    "print('SVM r2_score: ', result['performance']['SVM']['performance_metrics'][0]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input: ['NE' 387.3647046 'Basic' 'College' 'Retired' 'M' 28142 'Suburban'\n",
      " 'Married' 105 18 50 '0' '1' 'Personal L3' 'Hail' 'Agent' 'Sports Car'\n",
      " 'Medsize']\n",
      "\n",
      "prediction increased\n",
      "['NE' 387.3647046 'Basic' 'College' 'Retired' 'M' 28142 'Suburban'\n",
      " 'Married' 128 18 50 '0' '1' 'Personal L3' 'Hail' 'Agent' 'Sports Car'\n",
      " 'Medsize']\n",
      "\n",
      "prediction decreased\n",
      "['NE' 387.3647046 'Basic' 'College' 'Retired' 'M' 28142 'Rural' 'Married'\n",
      " 105 18 50 '0' '1' 'Personal L3' 'Hail' 'Agent' 'Sports Car' 'Medsize']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Certifai's explanation utilities we can programatically explore counterfactuals produced\n",
    "# during the explanation evaluation. Here we will, just look at a single observation\n",
    "linl1_explanations = explanations(result, 'LinL1')\n",
    "svm_explanations = explanations(result, 'SVM')\n",
    "\n",
    "\n",
    "first = linl1_explanations['LinL1'][0]\n",
    "print('Original input:', first.instance)\n",
    "print()\n",
    "\n",
    "# Note, that for this regression use case a counterfactual will produced in the\n",
    "# favorable direction (increasing) and in the unfavorable direction (decreasing)\n",
    "for cf in first.explanation.best_individuals:\n",
    "    print(cf.counterfactual_type)\n",
    "    print(cf.data)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
