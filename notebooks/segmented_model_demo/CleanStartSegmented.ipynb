{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a scan programatically\n",
    "\n",
    "In this notebook we'll build up a scan definition from first principles, against a local model trained within the\n",
    "notebook.  We will then run that scan and save its results.  Finally we will extract the scan defintion as YAML, which could be used to run the same scan (potentially on revised models or datasets) via the Certifai stand-alone scanner.\n",
    "\n",
    "MODIFIED A LITTLE AS A TESTBED FOT SEGMENTED MODELS - not illustrative of anything aprt fropm how to go about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from copy import copy\n",
    "\n",
    "from certifai.common.utils import set_verbose\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe\n",
    "\n",
    "# set verboseness to limit logging\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special import - \n",
    "# for multiprocessing to work in a Notebook,  pickled classes must be in a separate package or notebook\n",
    "# hence, the encoder class has to be somewhere other than the current notebook\n",
    "# from ipynb.fs.defs.cat_encoder import CatEncoder # <- doesn't work on Azure Notebooks\n",
    "# %run cat_encoder.py # <- doesn't work because code doesn't remain external\n",
    "\n",
    "# Azure Notebooks workaround - \n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('.')))\n",
    "from cat_encoder import CatEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Support Vector Machine' accuracy is 0.75\n",
      "Model 'Logistic classifier' accuracy is 0.76\n"
     ]
    }
   ],
   "source": [
    "# Example will use a simple logistic classifier on the German Credit dataset\n",
    "base_path = os.path.join('data')\n",
    "all_data_file = f\"{base_path}/german_credit_eval.csv\"\n",
    "\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "cat_columns = [\n",
    "    'checkingstatus',\n",
    "    'history',\n",
    "    'purpose',\n",
    "    'savings',\n",
    "    'employ',\n",
    "    'status',\n",
    "    'others',\n",
    "    'property',\n",
    "    'age',\n",
    "    'otherplans',\n",
    "    'housing',\n",
    "    'job',\n",
    "    'telephone',\n",
    "    'foreign'\n",
    "    ]\n",
    "\n",
    "label_column = 'outcome'\n",
    "\n",
    "# SEGMENT MODEL TEST\n",
    "# Add a randomized column that will select between two 'segment' models on a per-row basis\n",
    "# Note - we're not really doing any REAL segmentation here - this is just a structural example\n",
    "# to allow a model to be chosen based on the value of a column - for this test Im just randomly\n",
    "# setting the selection column which will choose between a logistic and an SVM model\n",
    "df[\"model_selector\"] = np.random.choice([ \"logistic\",\"SVM\"], len(df))\n",
    "cat_columns.append('model_selector')\n",
    "\n",
    "# Save this so we have a CSV that matches are augmented data and contains a segment label\n",
    "augmented_data_file = 'augmented_data'\n",
    "df.to_csv(augmented_data_file, index=False)\n",
    "\n",
    "# Separate outcome\n",
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Note - to support python multi-processing in the context of a notebook the encoder MUST\n",
    "# be in a separate file, which is why `CatEncoder` is defined outside of this notebook\n",
    "encoder = CatEncoder(cat_columns, X)\n",
    "\n",
    "def build_model(data, name, model_family, test=None):\n",
    "    if test is None:\n",
    "        test = data\n",
    "        \n",
    "    if model_family == 'SVM':\n",
    "        parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.1, .5, 1, 2, 4, 10], 'gamma':['auto']}\n",
    "        m = svm.SVC()\n",
    "    elif model_family == 'logistic':\n",
    "        parameters = {'C': (0.5, 1.0, 2.0), 'solver': ['lbfgs'], 'max_iter': [1000]}\n",
    "        m = LogisticRegression()\n",
    "    model = GridSearchCV(m, parameters, cv=3)\n",
    "    model.fit(data[0], data[1])\n",
    "\n",
    "    # Assess on the test data\n",
    "    accuracy = model.score(test[0], test[1].values)\n",
    "    print(f\"Model '{name}' accuracy is {accuracy}\")\n",
    "    return model\n",
    "\n",
    "svm_model = build_model((encoder(X_train.values), y_train),\n",
    "                        'Support Vector Machine',\n",
    "                        'SVM',\n",
    "                        test=(encoder(X_test.values), y_test))\n",
    "\n",
    "logistic_model = build_model((encoder(X_train.values), y_train),\n",
    "                        'Logistic classifier',\n",
    "                        'logistic',\n",
    "                        test=(encoder(X_test.values), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_selector_SVM': {'idx': 62, 'model': None},\n",
       " 'model_selector_logistic': {'idx': 63, 'model': None}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SEGMENT MODEL TEST\n",
    "\n",
    "# Create a joint model that delegates to the appropriate segment model.  Note - I had to put this into a seperate\n",
    "# file because of the issue with Python multi-processing and pickling with notebooks\n",
    "\n",
    "from joint_model import JointModel\n",
    "segment_selection_model_column_mapping = encoder.cat_indexes_of_feature('model_selector')\n",
    "segment_selection_model_column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic segment predictions match\n",
      "SVM segment predictions match\n"
     ]
    }
   ],
   "source": [
    "# set the respective models to corresponding \n",
    "# one-hot encoded segmented feature columns(from the `segment_selection_model_column_mapping`)\n",
    "\n",
    "segment_selection_model_column_mapping['model_selector_SVM']['model'] = svm_model\n",
    "segment_selection_model_column_mapping['model_selector_logistic']['model'] = logistic_model\n",
    "joint_model = JointModel(segment_selection_model_column_mapping)\n",
    "\n",
    "# The model's work in the one-hot encoded space tso encode the data\n",
    "encoded_X = encoder(X.values)\n",
    "\n",
    "# FOLLOWING is just to test we got it right - not needed for real usage\n",
    "# Test the predicts match what we expect\n",
    "# First out joint model that should delegate appropriately\n",
    "all_joint_predictions = joint_model.predict(encoded_X)\n",
    "\n",
    "# Now the two segemnt models separately on their segments\n",
    "logistic_indexes = X['model_selector'] == 'logistic'\n",
    "logistic_rows = encoded_X[logistic_indexes]\n",
    "SVM_indexes = X['model_selector'] == 'SVM'\n",
    "SVM_rows = encoded_X[SVM_indexes]\n",
    "logistic_predictions = logistic_model.predict(logistic_rows)\n",
    "SVM_predictions = svm_model.predict(SVM_rows)\n",
    "\n",
    "# Validate we got the same results\n",
    "if np.all(logistic_predictions == all_joint_predictions[np.where(logistic_indexes.values)]):\n",
    "    print(\"Logistic segment predictions match\")\n",
    "else:\n",
    "    print(\"Logistic segment predictions do NOT match!!\")\n",
    "if np.all(SVM_predictions == all_joint_predictions[np.where(SVM_indexes.values)]):\n",
    "    print(\"SVM segment predictions match\")\n",
    "else:\n",
    "    print(\"SVM segment predictions do NOT match!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the joint model up for use by Certifai as a local model\n",
    "joint_model_proxy = CertifaiPredictorWrapper(joint_model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 19:39:19,693 root   INFO     Validating license...\n",
      "2020-05-15 19:39:19,695 root   INFO     License is valid - expires: n/a\n",
      "2020-05-15 19:39:19,718 root   INFO     Generated unique scan id: 8360aa4dafb8\n",
      "2020-05-15 19:39:19,719 root   INFO     Validating input data...\n",
      "2020-05-15 19:39:19,720 root   INFO     Creating dataset with id: evaluation\n",
      "2020-05-15 19:39:19,758 root   INFO     Inferring dataset features and applying user overrides\n",
      "2020-05-15 19:39:19,761 root   INFO     Integer-valued feature 'duration' inferred to be numeric (sample cardinality 33)\n",
      "2020-05-15 19:39:19,765 root   INFO     Integer-valued feature 'amount' inferred to be numeric (sample cardinality 921)\n",
      "2020-05-15 19:39:19,767 root   INFO     Integer-valued feature 'installment' inferred to be categorical (sample cardinality 4)\n",
      "2020-05-15 19:39:19,769 root   INFO     Integer-valued feature 'residence' inferred to be categorical (sample cardinality 4)\n",
      "2020-05-15 19:39:19,772 root   INFO     Integer-valued feature 'cards' inferred to be categorical (sample cardinality 4)\n",
      "2020-05-15 19:39:19,775 root   INFO     Integer-valued feature 'liable' inferred to be categorical (sample cardinality 2)\n",
      "2020-05-15 19:39:19,783 root   INFO     Integer-valued feature 'outcome' inferred to be categorical (sample cardinality 2)\n",
      "2020-05-15 19:39:19,787 root   WARNING  Couldn't find column by label as index: `outcome`, in df.columns: `Index(['checkingstatus', 'duration', 'history', 'purpose', 'amount', 'savings',\n",
      "       'employ', 'installment', 'status', 'others', 'residence', 'property',\n",
      "       'age', 'otherplans', 'housing', 'cards', 'job', 'liable', 'telephone',\n",
      "       'foreign', 'model_selector'],\n",
      "      dtype='object')`.\n",
      "2020-05-15 19:39:19,788 root   INFO     Inferring dataset features and applying user overrides\n",
      "2020-05-15 19:39:19,791 root   INFO     Integer-valued feature 'duration' inferred to be numeric (sample cardinality 33)\n",
      "2020-05-15 19:39:19,794 root   INFO     Integer-valued feature 'amount' inferred to be numeric (sample cardinality 921)\n",
      "2020-05-15 19:39:19,797 root   INFO     Integer-valued feature 'installment' inferred to be categorical (sample cardinality 4)\n",
      "2020-05-15 19:39:19,799 root   INFO     Integer-valued feature 'residence' inferred to be categorical (sample cardinality 4)\n",
      "2020-05-15 19:39:19,803 root   INFO     Integer-valued feature 'cards' inferred to be categorical (sample cardinality 4)\n",
      "2020-05-15 19:39:19,811 root   INFO     Integer-valued feature 'liable' inferred to be categorical (sample cardinality 2)\n",
      "2020-05-15 19:39:19,825 root   INFO     Input validation complete\n",
      "2020-05-15 19:39:19,826 root   INFO     Beginning to perform evaluations\n",
      "2020-05-15 19:39:19,828 root   INFO     performing evaluation: fairness, for model: Joint\n",
      "2020-05-15 19:39:19,875 root   INFO     Validating license...\n",
      "2020-05-15 19:39:19,879 root   INFO     License is valid - expires: n/a\n",
      "2020-05-15 19:39:19,881 root   INFO     Running counterfactual analysis for multiclass classification with class structure ClassStructure.PARTITIONED (analysis = Fairness)\n",
      "2020-05-15 19:39:19,884 root   INFO     Hyper-params for experiment: {'tournsize': 3, 'population': 4000, 'indpb': 0.05, 'CXPB': 0.5, 'MUTPB': 0.2, 'generation': 180, 'evolution': 100, 'N_CYCLES': 1, 'early_stopping_delta': 0.0001, 'early_stopping_epochs': 3, 'percent_class_seeding': 5, 'sampling_Z': 1.96, 'sampling_boundary': 0.01, 'sampling_min_n': 100, '2pt_orig_weight': 0.025, 'shrinkage_weight': 0.5, 'annealing_weight': 0.45, 'annealing_rate': 1.25, 'elitism': 0.04, 'dimensional_normalization': True, 'num_counterfactuals': 1}\n",
      "2020-05-15 19:39:19,886 root   INFO     Running without scaler, using identity scaler.\n",
      "2020-05-15 19:39:19,916 root   WARNING  Insufficient examples of some fairness classes to guarantee convergence (smallest class size is for 'male : divorced/separated' with 50 samples)\n",
      "2020-05-15 19:39:19,943 root   INFO     Running analysis variant 'prediction more beneficial'\n",
      "2020-05-15 19:39:21,447 root   INFO     Running with explanation reduction ON\n",
      "2020-05-15 19:39:21,448 root   INFO     Total dataset size is 1000\n",
      "2020-05-15 19:39:41,503 root   INFO     Batch run time per generation for instances 0 to 113: 0.09436\n",
      "2020-05-15 19:39:41,505 root   INFO     Current max sampling error 0.37267797699493616 (max for early stop 0.005102040816326531)\n",
      "2020-05-15 19:39:41,506 root   INFO     Current min non-exhausted protected class samples 25 (min for early stop 100)\n",
      "2020-05-15 19:39:57,753 root   INFO     Batch run time per generation for instances 114 to 227: 0.07172\n",
      "2020-05-15 19:39:57,754 root   WARNING  Examples of protected class (8, 'male : divorced/separated') exhausted before convergence after 50 samples\n",
      "2020-05-15 19:39:57,754 root   INFO     Current max sampling error 0.28756268695177367 (max for early stop 0.005102040816326531)\n",
      "2020-05-15 19:39:57,755 root   INFO     Current min non-exhausted protected class samples 53 (min for early stop 100)\n",
      "2020-05-15 19:40:14,849 root   INFO     Batch run time per generation for instances 228 to 392: 0.08109\n",
      "2020-05-15 19:40:14,850 root   WARNING  Examples of protected class (8, 'male : married/widowed') exhausted before convergence after 92 samples\n",
      "2020-05-15 19:40:14,852 root   INFO     Current max sampling error 0.20504587271373786 (max for early stop 0.005102040816326531)\n",
      "2020-05-15 19:40:14,853 root   INFO     Current min non-exhausted protected class samples 118 (min for early stop 100)\n",
      "2020-05-15 19:40:31,914 root   INFO     Batch run time per generation for instances 393 to 499: 0.07772\n",
      "2020-05-15 19:40:31,915 root   INFO     Current max sampling error 0.16545973484836277 (max for early stop 0.005102040816326531)\n",
      "2020-05-15 19:40:31,916 root   INFO     Current min non-exhausted protected class samples 155 (min for early stop 100)\n",
      "2020-05-15 19:40:48,616 root   INFO     Batch run time per generation for instances 500 to 628: 0.07599\n",
      "2020-05-15 19:40:48,617 root   WARNING  Examples of protected class (12, '<= 25 years') exhausted before convergence after 190 samples\n",
      "2020-05-15 19:40:48,617 root   INFO     Current max sampling error 0.13612668118484744 (max for early stop 0.005102040816326531)\n",
      "2020-05-15 19:40:48,618 root   INFO     Current min non-exhausted protected class samples 243 (min for early stop 100)\n",
      "2020-05-15 19:41:04,288 root   INFO     Batch run time per generation for instances 629 to 773: 0.07199\n",
      "2020-05-15 19:41:04,289 root   INFO     Current max sampling error 0.11775025256370084 (max for early stop 0.005102040816326531)\n",
      "2020-05-15 19:41:04,290 root   INFO     Current min non-exhausted protected class samples 305 (min for early stop 100)\n",
      "2020-05-15 19:41:20,565 root   INFO     Batch run time per generation for instances 774 to 988: 0.07075\n",
      "2020-05-15 19:41:20,565 root   WARNING  Examples of protected class (8, 'female : divorced/separated/married') exhausted before convergence after 310 samples\n",
      "2020-05-15 19:41:20,566 root   INFO     Current max sampling error 0.0982726912818543 (max for early stop 0.005102040816326531)\n",
      "2020-05-15 19:41:20,574 root   INFO     Current min non-exhausted protected class samples 537 (min for early stop 100)\n",
      "2020-05-15 19:41:21,898 root   INFO     Batch run time per generation for instances 989 to 999: 0.10973\n",
      "2020-05-15 19:41:21,899 root   WARNING  Examples of protected class (12, '> 25 years') exhausted before convergence after 810 samples\n",
      "2020-05-15 19:41:21,900 root   WARNING  Examples of protected class (8, 'male : single') exhausted before convergence after 548 samples\n",
      "2020-05-15 19:41:21,901 root   INFO     Early stopping after running 1000 instances\n",
      "2020-05-15 19:41:21,910 root   INFO     Estimating sample variance (2001 bags)\n",
      "2020-05-15 19:41:28,560 root   INFO     Group Burden is [{'feature': 'age', 'groups': [{'label': '<= 25 years', 'value': 0.08902439024390243, 'value_confidence_95_lower': 0.07575757575757576, 'value_confidence_95_upper': 0.1051912568306011, 'values': [0.08902439024390243], 'values_confidence_95_lower': [0.07575757575757576], 'values_confidence_95_upper': [0.1051912568306011], 'values_names': ['burden']}, {'label': '> 25 years', 'value': 0.052360774818401935, 'value_confidence_95_lower': 0.0460772104607721, 'value_confidence_95_upper': 0.05897435897435897, 'values': [0.052360774818401935], 'values_confidence_95_lower': [0.0460772104607721], 'values_confidence_95_upper': [0.05897435897435897], 'values_names': ['burden']}]}, {'feature': 'status', 'groups': [{'label': 'female : divorced/separated/married', 'value': 0.0828125, 'value_confidence_95_lower': 0.07108626198083066, 'value_confidence_95_upper': 0.09584664536741214, 'values': [0.0828125], 'values_confidence_95_lower': [0.07108626198083066], 'values_confidence_95_upper': [0.09584664536741214], 'values_names': ['burden']}, {'label': 'male : divorced/separated', 'value': 0.10416666666666667, 'value_confidence_95_lower': 0.07386363636363637, 'value_confidence_95_upper': 0.13709677419354838, 'values': [0.10416666666666667], 'values_confidence_95_lower': [0.07386363636363637], 'values_confidence_95_upper': [0.13709677419354838], 'values_names': ['burden']}, {'label': 'male : married/widowed', 'value': 0.043209876543209874, 'value_confidence_95_lower': 0.0273972602739726, 'value_confidence_95_upper': 0.060526315789473685, 'values': [0.043209876543209874], 'values_confidence_95_lower': [0.0273972602739726], 'values_confidence_95_upper': [0.060526315789473685], 'values_names': ['burden']}, {'label': 'male : single', 'value': 0.0446113074204947, 'value_confidence_95_lower': 0.03773584905660377, 'value_confidence_95_upper': 0.05188679245283019, 'values': [0.0446113074204947], 'values_confidence_95_lower': [0.03773584905660377], 'values_confidence_95_upper': [0.05188679245283019], 'values_names': ['burden']}]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 19:41:28,561 root   INFO     Feature fairnesses are [{'feature': 'age', 'value': 73.79134860050891, 'value_confidence_95_lower': 64.88156539649846, 'value_confidence_95_upper': 83.83467458395187}, {'feature': 'status', 'value': 72.1861625931583, 'value_confidence_95_lower': 62.03935286030913, 'value_confidence_95_upper': 81.96230153091673}]\n",
      "2020-05-15 19:41:28,561 root   INFO     Model used: Joint\n",
      "2020-05-15 19:41:28,562 root   INFO     CERScore is 1.0530973451327434\n",
      "2020-05-15 19:41:28,563 root   INFO     NCERScore is 0.0743237051272755\n",
      "2020-05-15 19:41:28,563 root   INFO     Normalization constant is 14.169064140833248\n",
      "2020-05-15 19:41:28,564 root   INFO     Total run time in seconds: 122.02108907699585\n",
      "2020-05-15 19:41:28,565 root   INFO     Total Samples: 1767\n",
      "2020-05-15 19:41:28,565 root   INFO     Average feature reduction applied to counterfactuals: 0.0\n",
      "2020-05-15 19:41:28,703 root   INFO     Completed fairness report for model Joint\n",
      "2020-05-15 19:41:28,708 root   INFO     Creating ATX report for model: Joint\n",
      "2020-05-15 19:41:28,711 root   INFO     Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "# Create the scan object from scratch using the ScanBuilder class\n",
    "\n",
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='Loan granted', favorable=True),\n",
    "        CertifaiOutcomeValue(2, name='Loan denied')\n",
    "    ]),\n",
    "    prediction_description='Determine whether a loan should be granted')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "\n",
    "# Add our local models\n",
    "first_model = CertifaiModel('Joint',\n",
    "                            local_predictor=joint_model_proxy)\n",
    "scan.add_model(first_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.csv(augmented_data_file))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "# Setup an evaluation for fairness on the above dataset using the model\n",
    "# We'll look at disparity between groups defined by marital status and age\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('age'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('status'))\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = 'outcome'\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>type</th>\n",
       "      <th>overall fairness</th>\n",
       "      <th>Feature (age)</th>\n",
       "      <th>Group details (&lt;= 25 years)</th>\n",
       "      <th>Group details (&gt; 25 years)</th>\n",
       "      <th>Feature (status)</th>\n",
       "      <th>Group details (female : divorced/separated/married)</th>\n",
       "      <th>Group details (male : divorced/separated)</th>\n",
       "      <th>Group details (male : married/widowed)</th>\n",
       "      <th>Group details (male : single)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Joint (burden)</th>\n",
       "      <td>Joint</td>\n",
       "      <td>burden</td>\n",
       "      <td>69.645913</td>\n",
       "      <td>73.791349</td>\n",
       "      <td>0.089024</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>72.186163</td>\n",
       "      <td>0.082812</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.04321</td>\n",
       "      <td>0.044611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               context    type  overall fairness  Feature (age)  \\\n",
       "Joint (burden)   Joint  burden         69.645913      73.791349   \n",
       "\n",
       "                Group details (<= 25 years)  Group details (> 25 years)  \\\n",
       "Joint (burden)                     0.089024                    0.052361   \n",
       "\n",
       "                Feature (status)  \\\n",
       "Joint (burden)         72.186163   \n",
       "\n",
       "                Group details (female : divorced/separated/married)  \\\n",
       "Joint (burden)                                           0.082812     \n",
       "\n",
       "                Group details (male : divorced/separated)  \\\n",
       "Joint (burden)                                   0.104167   \n",
       "\n",
       "                Group details (male : married/widowed)  \\\n",
       "Joint (burden)                                 0.04321   \n",
       "\n",
       "                Group details (male : single)  \n",
       "Joint (burden)                       0.044611  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The result is a dictionary keyed on analysis, containing reports keyed on model id\n",
    "# The console app is the recommended way to view these, by saving the results to file\n",
    "# (see previous cell), but programmatic analysis of the result here is also possible\n",
    "df = construct_scores_dataframe(scores('fairness', result), include_confidence=False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
