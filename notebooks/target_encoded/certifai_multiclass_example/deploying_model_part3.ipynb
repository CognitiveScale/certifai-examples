{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020. Cognitive Scale Inc. All rights reserved.\n",
    "Licensed under CognitiveScale Example Code [License](https://github.com/CognitiveScale/cortex-certifai-examples/blob/7998b8a481fccd467463deb1fc46d19622079b0e/LICENSE.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying model as a service\n",
    "\n",
    "- For the sake of this tutorial we will be deploying the previously built model (in part3 of the example) to Azure Cloud using Azure Cloud Instance (ACI)\n",
    "- Most of the steps are common for almost all could service providers\n",
    "- Important thing to note is the scoring script (score.py) which defines `request/response` schema needed for Certifai  scan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the section below we will:\n",
    "\n",
    "1. Configure Azure workspace\n",
    "2. Register model (built in part3 of the example) to the workspace\n",
    "3. Create a prediction environment in the remote Azure workspace (created above) and\n",
    "4. Deploy model (predict) as web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using an Azure Machine Learning Notebook VM, you can skip over the `Configure and Initialize Azure workspace` section. Otherwise, make sure you go through the [configuration-notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) to create an Azure workspace. Creating remote environments/dependencies will be covered in the notebook\n",
    "\n",
    "**Please Note**: to step through this notebook, make sure you have necessary dependencies installed locally (using `certifai_azure_model_env.yml`)\n",
    "- python=3.8\n",
    "- scikit-learn=1.0.2\n",
    "- numpy=1.21.5\n",
    "- pandas\n",
    "- azureml-sdk=1.47.0\n",
    "- matplotlib\n",
    "- jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:04.898468Z",
     "iopub.status.busy": "2023-01-20T02:10:04.898142Z",
     "iopub.status.idle": "2023-01-20T02:10:05.422042Z",
     "shell.execute_reply": "2023-01-20T02:10:05.421064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(joblib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Initialize Azure workspace\n",
    "\n",
    "- Follow the instructions listed here [creating and managing azure-ml workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace) to create an azure-ml workspace\n",
    "\n",
    "**Once you have the workspace created easiest way to run through remaining steps is to download the `config.json` to the current directory and replace the exisiting config.json**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace%28class%29?view=azure-ml-py) object from the persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:05.458467Z",
     "iopub.status.busy": "2023-01-20T02:10:05.458100Z",
     "iopub.status.idle": "2023-01-20T02:10:10.194437Z",
     "shell.execute_reply": "2023-01-20T02:10:10.193690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import AzureCliAuthentication, ServicePrincipalAuthentication\n",
    "\n",
    "def get_azure_authentication():\n",
    "    # dynamically decide how to perform authentication\n",
    "    use_cli_authentication = 'AML_USE_CLI_AUTH' in os.environ\n",
    "    use_service_principal_authentication = 'AML_USE_SP_AUTH' in os.environ\n",
    "\n",
    "    if use_cli_authenticationt:\n",
    "        return AzureCliAuthentication()\n",
    "    elif use_service_principal_authentication:\n",
    "        assert 'AML_TENANT_ID' in os.environ, 'AML_TENANT_ID is expected when authenticating via service principal (SP)'\n",
    "        assert 'AML_PRINCIPAL_ID' in os.environ, 'AML_PRINCIPAL_ID is expected when authenticating via service principal (SP)'\n",
    "        assert 'AML_PRINCIPAL_PASS' in os.environ, 'AML_PRINCIPAL_PASS is expected when authenticating via service principal (SP)'\n",
    "        return ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\n",
    "                                              service_principal_id=os.environ['AML_PRINCIPAL_ID'],\n",
    "                                              service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\n",
    "    else:\n",
    "        # default to not providing authentication, suitable for either\n",
    "        # interactive login or use within a AzureML notebook\n",
    "        return None\n",
    "\n",
    "ws = Workspace.from_config(auth=get_azure_authentication())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:10.198425Z",
     "iopub.status.busy": "2023-01-20T02:10:10.197951Z",
     "iopub.status.idle": "2023-01-20T02:10:10.482966Z",
     "shell.execute_reply": "2023-01-20T02:10:10.482089Z"
    }
   },
   "outputs": [],
   "source": [
    "# deps test model for model serialization/de-serialization\n",
    "import sklearn as sklearn_version_test\n",
    "assert sklearn_version_test.__version__ >= '0.23.1', 'scikit-learn version mismatch, `pip install scikit-learn>=0.23.1` to install right sklearn version for this notebook'\n",
    "assert np.__version__                   >= '1.16.2' , 'numpy version mismatch, `pip install numpy>=1.16.2` to install right numpy version for this notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:10.486972Z",
     "iopub.status.busy": "2023-01-20T02:10:10.486660Z",
     "iopub.status.idle": "2023-01-20T02:10:10.544168Z",
     "shell.execute_reply": "2023-01-20T02:10:10.543412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models/german_credit_multiclass.joblib\n",
      "Pipeline(steps=[('full_pipeline',\n",
      "                 Pipeline(steps=[('scaler', StandardScaler())])),\n",
      "                ('model',\n",
      "                 GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
      "                              param_grid={'C': (0.5, 1.0, 2.0),\n",
      "                                          'max_iter': [1000],\n",
      "                                          'solver': ['lbfgs']}))])\n"
     ]
    }
   ],
   "source": [
    "# load model \n",
    "model_path = 'models/german_credit_multiclass.joblib'\n",
    "print(f'loading {model_path}')\n",
    "try:\n",
    "    model = joblib.load(model_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(f'model `{model_path}` not found. Looks like model has not been trained or file location is wrong')\n",
    "    raise Exception(str(e))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model to created  workspace\n",
    "\n",
    "- Register a file or folder as a model by calling [Model.register()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-).\n",
    "\n",
    "- In addition to the content of the model file itself (model + scaler object), our registered model will also store model metadata like model description, tags, etc. -- that will be useful when managing and deploying models in our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:10.547461Z",
     "iopub.status.busy": "2023-01-20T02:10:10.547203Z",
     "iopub.status.idle": "2023-01-20T02:10:14.724761Z",
     "shell.execute_reply": "2023-01-20T02:10:14.723167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model german_credit_target_encoded_multiclass\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "target_en_multiclass_german_credit = Model.register(model_path=model_path,\n",
    "                       model_name='german_credit_target_encoded_multiclass',\n",
    "                       tags={'area': \"banking credit risk\", 'type': \"multi-class\"},\n",
    "                       description=\"Logistic Classifier model to predict credit loan approved/denied/further Inspection\",\n",
    "                       workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom prediction environment inside azure-ml workspace\n",
    "\n",
    "If we want control over how our model is run, if it uses another framework, or if it has special runtime requirements, we can instead specify our own environment and scoring method. Custom environments can be used for any model we want to deploy.\n",
    "\n",
    "Specify the model's runtime environment by creating an [Environment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment%28class%29?view=azure-ml-py) object and providing the [CondaDependencies](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py) needed by the model\n",
    "\n",
    "In this example we will create a conda environment for our german credit model from file **myenv.yml** and register it to our workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:14.730654Z",
     "iopub.status.busy": "2023-01-20T02:10:14.730292Z",
     "iopub.status.idle": "2023-01-20T02:10:14.735621Z",
     "shell.execute_reply": "2023-01-20T02:10:14.734801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: project_environment\n",
      "dependencies:\n",
      "  - python=3.8.13\n",
      "  - pip=22.2.2\n",
      "  - scikit-learn=1.0.2\n",
      "  - numpy=1.21.5\n",
      "  - joblib=1.1.1\n",
      "  - pip:\n",
      "    - azureml-defaults\n",
      "    - inference-schema[numpy-support]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"myenv.yml\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:14.739428Z",
     "iopub.status.busy": "2023-01-20T02:10:14.739133Z",
     "iopub.status.idle": "2023-01-20T02:10:15.581611Z",
     "shell.execute_reply": "2023-01-20T02:10:15.580854Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"assetId\": \"azureml://locations/eastus2/workspaces/b608c37e-b4dd-47f2-8aed-a9dbe0bd7a52/environments/target-en-multiclass/versions/4\",\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20221010.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"target-en-multiclass\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8.13\",\n",
       "                \"pip=22.2.2\",\n",
       "                \"scikit-learn=1.0.2\",\n",
       "                \"numpy=1.21.5\",\n",
       "                \"joblib=1.1.1\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"inference-schema[numpy-support]\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"project_environment\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"4\"\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "environment = Environment(\"target-en-multiclass\")\n",
    "environment.python.conda_dependencies = CondaDependencies(\"myenv.yml\")\n",
    "environment.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certifai model invoke request/response schema\n",
    "\n",
    "Make a note of the request/response schema in `score.py`\n",
    "\n",
    "- Certifai invokes model with the json schema:\n",
    "\n",
    "```\n",
    "{\n",
    "\t\"payload\": {\n",
    "\t\t\"instances\": [\n",
    "\t\t\t[6, 107, 88, 0, 0, 36.8, 0.727, 31],\n",
    "\t\t\t[5, 100, 80, 0, 0, 31.9, 0.61, 33]\n",
    "\t\t]\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "**where individual list of values correspond to a `row` in the dataset**\n",
    "\n",
    "- Certifai expects model responses with the json schema: \n",
    "\n",
    "```\n",
    "{\n",
    "\t\"payload\": {\n",
    "\t\t\"predictions\": [1, 0]\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "**where `predictions` correspond to an ordered list of model predict responses**\n",
    "\n",
    "**Important**: Certifai needs batch-predictions enabled for serving models in-order to be performant. It invokes model  predicts with batches of size 4K.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration and deploy webservice\n",
    "\n",
    "**Inference Configuration** will contain:\n",
    "\n",
    "1. Scoring script\n",
    "2. Environment (created above)\n",
    "\n",
    "We create the scoring script, called **score.py**. The web service call uses this script to show how to use the model.\n",
    "\n",
    "We include below two required functions in the scoring script:\n",
    "\n",
    "1. The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started.\n",
    "\n",
    "2. The `run(data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are also supported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:15.585471Z",
     "iopub.status.busy": "2023-01-20T02:10:15.585175Z",
     "iopub.status.idle": "2023-01-20T02:10:15.737546Z",
     "shell.execute_reply": "2023-01-20T02:10:15.735939Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import json\r\n",
      "import numpy as np\r\n",
      "import joblib\r\n",
      "import traceback\r\n",
      "\r\n",
      "\r\n",
      "def init():\r\n",
      "\tglobal model\r\n",
      "\t# AZUREML_MODEL_DIR is an environment variable created during deployment.\r\n",
      "\t# It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\r\n",
      "\t# For multiple models, it points to the folder containing all deployed models (./azureml-models)\r\n",
      "\tmodel_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'german_credit_multiclass.joblib')\r\n",
      "\t# deserialize the model_obj file back into a sklearn model and scaler object\r\n",
      "\tmodel  = joblib.load(model_path)\r\n",
      "\tprint(model)\r\n",
      "\r\n",
      "\r\n",
      "def run(data):\r\n",
      "\ttry: \r\n",
      "\t\t# certifai invokes model with the json schema -> {\"payload\": {\"instances\": [ [6,107,88,0,0,36.8,0.727,31], [5,100,80,0,0,31.9,0.61,33] ]}}\r\n",
      "\t\tdata  = json.loads(data).get('payload', {}).get('instances', [])\r\n",
      "\t\tdata  = np.array(data, dtype=object)\r\n",
      "\t\tdata  = data if data.ndim == 2 else np.reshape(data, (1, -1))\r\n",
      "\t\tresult = model.predict(data)\r\n",
      "\t\t# you can return any datatype as long as it is JSON-serializable\r\n",
      "\t\t# certifai expects model response with the json schema -> {\"payload\": {\"predictions\": [1,0]} }\r\n",
      "\r\n",
      "\t\treturn {\"payload\":{ \"predictions\": result.tolist()} }\r\n",
      "\texcept Exception as e:\r\n",
      "\t\terror = str(e)\r\n",
      "\t\tprint(traceback.format_exc())\r\n",
      "\t\treturn error\r\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the registered model in the custom environment by providing an [InferenceConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py) object to [Model.deploy()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config--deployment-config-none--deployment-target-none-). In this case we are also using the [AciWebservice.deploy_configuration()](https://docs.microsoft.com/python/api/azureml-core/azureml.core.webservice.aci.aciwebservice#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none--dns-name-label-none--) method to generate a custom deploy configuration\n",
    "        \n",
    "**Note**: This step can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:15.742501Z",
     "iopub.status.busy": "2023-01-20T02:10:15.742069Z",
     "iopub.status.idle": "2023-01-20T02:10:15.756039Z",
     "shell.execute_reply": "2023-01-20T02:10:15.755182Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "inference_config= InferenceConfig(entry_script=\"score.py\",\n",
    "                                   environment=environment,source_directory=\"scripts\")\n",
    "\n",
    "service_name = 'te-multiclass-gc-service'\n",
    "aci_deployment_config = AciWebservice.deploy_configuration(auth_enabled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:10:15.759524Z",
     "iopub.status.busy": "2023-01-20T02:10:15.759239Z",
     "iopub.status.idle": "2023-01-20T02:13:53.544168Z",
     "shell.execute_reply": "2023-01-20T02:13:53.543361Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/61zjyp_54tv05lfbp3vwp72c0000gq/T/ipykernel_51813/165639150.py:1: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoint-sdk-v2 /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration. \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  service = Model.deploy(ws, service_name, [target_en_multiclass_german_credit],inference_config=inference_config,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2023-01-19 20:10:20-06:00 Creating Container Registry if not exists.\n",
      "2023-01-19 20:10:20-06:00 Registering the environment.\n",
      "2023-01-19 20:10:21-06:00 Use the existing image.\n",
      "2023-01-19 20:10:21-06:00 Generating deployment configuration.\n",
      "2023-01-19 20:10:21-06:00 Submitting deployment to compute.\n",
      "2023-01-19 20:10:24-06:00 Checking the status of deployment te-multiclass-gc-service..\n",
      "2023-01-19 20:13:46-06:00 Checking the status of inference endpoint te-multiclass-gc-service.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "service = Model.deploy(ws, service_name, [target_en_multiclass_german_credit],inference_config=inference_config,\n",
    "                       deployment_config=aci_deployment_config, overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:13:53.547818Z",
     "iopub.status.busy": "2023-01-20T02:13:53.547545Z",
     "iopub.status.idle": "2023-01-20T02:13:55.067164Z",
     "shell.execute_reply": "2023-01-20T02:13:55.066184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /azureml-envs/azureml_284ce0c20ba2a5a5ddac8507bf73cab5/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_284ce0c20ba2a5a5ddac8507bf73cab5/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_284ce0c20ba2a5a5ddac8507bf73cab5/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_284ce0c20ba2a5a5ddac8507bf73cab5/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2023-01-20T02:13:13,659694200+00:00 - iot-server/run \n",
      "2023-01-20T02:13:13,660125300+00:00 - rsyslog/run \n",
      "bash: /azureml-envs/azureml_284ce0c20ba2a5a5ddac8507bf73cab5/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "2023-01-20T02:13:13,696737400+00:00 - nginx/run \n",
      "2023-01-20T02:13:13,706774600+00:00 - gunicorn/run \n",
      "2023-01-20T02:13:13,712935300+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:13,715693500+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-20T02:13:13,794497200+00:00 | gunicorn/run | AzureML Container Runtime Information\n",
      "2023-01-20T02:13:13,814083000+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-20T02:13:13,890809900+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:13,900218500+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:13,916837700+00:00 | gunicorn/run | AzureML image information: openmpi4.1.0-ubuntu20.04, Materializaton Build:20221010.v9\n",
      "2023-01-20T02:13:13,919925000+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:13,990201100+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:13,996643200+00:00 | gunicorn/run | PATH environment variable: /azureml-envs/azureml_284ce0c20ba2a5a5ddac8507bf73cab5/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "2023-01-20T02:13:14,003422467+00:00 | gunicorn/run | PYTHONPATH environment variable: \n",
      "2023-01-20T02:13:14,008277584+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:14,011244916+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\n",
      "\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "/bin/bash: /azureml-envs/azureml_284ce0c20ba2a5a5ddac8507bf73cab5/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2023-01-20T02:13:14,602448153+00:00 - iot-server/finish 1 0\n",
      "2023-01-20T02:13:14,610189869+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "adal==1.2.7\n",
      "argcomplete==2.0.0\n",
      "attrs==22.2.0\n",
      "azure-common==1.1.28\n",
      "azure-core==1.26.2\n",
      "azure-graphrbac==0.61.1\n",
      "azure-identity==1.7.0\n",
      "azure-mgmt-authorization==3.0.0\n",
      "azure-mgmt-containerregistry==10.0.0\n",
      "azure-mgmt-core==1.3.2\n",
      "azure-mgmt-keyvault==10.1.0\n",
      "azure-mgmt-resource==21.2.1\n",
      "azure-mgmt-storage==20.1.0\n",
      "azureml-core==1.48.0\n",
      "azureml-dataprep==4.8.4\n",
      "azureml-dataprep-native==38.0.0\n",
      "azureml-dataprep-rslex==2.15.2\n",
      "azureml-dataset-runtime==1.48.0\n",
      "azureml-defaults==1.48.0\n",
      "azureml-inference-server-http==0.7.7\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==4.0.1\n",
      "cachetools==5.2.1\n",
      "certifi @ file:///croot/certifi_1671487769961/work/certifi\n",
      "cffi==1.15.1\n",
      "charset-normalizer==3.0.1\n",
      "click==8.1.3\n",
      "cloudpickle==2.2.0\n",
      "contextlib2==21.6.0\n",
      "cryptography==38.0.4\n",
      "distro==1.8.0\n",
      "docker==6.0.1\n",
      "dotnetcore2==3.1.23\n",
      "Flask==2.2.2\n",
      "Flask-Cors==3.0.10\n",
      "fusepy==3.0.1\n",
      "google-api-core==2.11.0\n",
      "google-auth==2.16.0\n",
      "googleapis-common-protos==1.58.0\n",
      "gunicorn==20.1.0\n",
      "humanfriendly==10.0\n",
      "idna==3.4\n",
      "importlib-metadata==6.0.0\n",
      "importlib-resources==5.10.2\n",
      "inference-schema==1.5.1\n",
      "isodate==0.6.1\n",
      "itsdangerous==2.1.2\n",
      "jeepney==0.8.0\n",
      "Jinja2==3.1.2\n",
      "jmespath==1.0.1\n",
      "joblib @ file:///croot/joblib_1666298844297/work\n",
      "jsonpickle==2.2.0\n",
      "jsonschema==4.17.3\n",
      "knack==0.10.1\n",
      "MarkupSafe==2.1.2\n",
      "mkl-fft==1.3.1\n",
      "mkl-random @ file:///tmp/build/80754af9/mkl_random_1626186064646/work\n",
      "mkl-service==2.4.0\n",
      "msal==1.20.0\n",
      "msal-extensions==0.3.1\n",
      "msrest==0.7.1\n",
      "msrestazure==0.6.4\n",
      "ndg-httpsclient==0.5.1\n",
      "numpy @ file:///opt/conda/conda-bld/numpy_and_numpy_base_1653915516269/work\n",
      "oauthlib==3.2.2\n",
      "opencensus==0.11.0\n",
      "opencensus-context==0.1.3\n",
      "opencensus-ext-azure==1.1.7\n",
      "packaging==21.3\n",
      "paramiko==2.12.0\n",
      "pathspec==0.10.3\n",
      "pkginfo==1.9.6\n",
      "pkgutil_resolve_name==1.3.10\n",
      "portalocker==2.6.0\n",
      "protobuf==4.21.12\n",
      "psutil==5.9.4\n",
      "pyarrow==9.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.21\n",
      "Pygments==2.14.0\n",
      "PyJWT==2.6.0\n",
      "PyNaCl==1.5.0\n",
      "pyOpenSSL==22.1.0\n",
      "pyparsing==3.0.9\n",
      "pyrsistent==0.19.3\n",
      "PySocks==1.7.1\n",
      "python-dateutil==2.8.2\n",
      "pytz==2022.7.1\n",
      "PyYAML==6.0\n",
      "requests==2.28.2\n",
      "requests-oauthlib==1.3.1\n",
      "rsa==4.9\n",
      "scikit-learn @ file:///tmp/build/80754af9/scikit-learn_1642617107864/work\n",
      "scipy==1.9.3\n",
      "SecretStorage==3.3.3\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "tabulate==0.9.0\n",
      "threadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\n",
      "typing_extensions==4.4.0\n",
      "urllib3==1.26.14\n",
      "websocket-client==1.4.2\n",
      "Werkzeug==2.2.2\n",
      "wrapt==1.12.1\n",
      "zipp==3.11.0\n",
      "\n",
      "2023-01-20T02:13:16,214194191+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:16,216464391+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-20T02:13:16,218306291+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2023-01-20T02:13:16,219806391+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-20T02:13:16,288117591+00:00 | gunicorn/run | \n",
      "2023-01-20T02:13:19,214124891+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\n",
      "\n",
      "Azure ML Inferencing HTTP server v0.7.7\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: /var/azureml-app/scripts/score.py\n",
      "Model Directory: /var/azureml-app/azureml-models/german_credit_target_encoded_multiclass/8\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.7.7\n",
      "CORS for the specified origins: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://0.0.0.0:31311 (69)\n",
      "Using worker: sync\n",
      "Booting worker with pid: 119\n",
      "Initializing logger\n",
      "2023-01-20 02:13:21,399 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2023-01-20 02:13:21,400 | root | INFO | Starting up app insight hooks\n",
      "2023-01-20 02:13:21,839 | root | INFO | Found user script at /var/azureml-app/scripts/score.py\n",
      "2023-01-20 02:13:21,840 | root | INFO | run() is not decorated. Server will invoke it with the input in JSON string.\n",
      "2023-01-20 02:13:21,840 | root | INFO | Invoking user's init function\n",
      "00000000-0000-0000-0000-000000000000,in init\n",
      "00000000-0000-0000-0000-000000000000,azure dir:\n",
      "00000000-0000-0000-0000-000000000000,/var/azureml-app/azureml-models/german_credit_target_encoded_multiclass/8\n",
      "00000000-0000-0000-0000-000000000000,model_path:\n",
      "00000000-0000-0000-0000-000000000000,/var/azureml-app/azureml-models/german_credit_target_encoded_multiclass/8/german_credit_multiclass.joblib\n",
      "00000000-0000-0000-0000-000000000000,Pipeline(steps=[('full_pipeline',\n",
      "                 Pipeline(steps=[('scaler', StandardScaler())])),\n",
      "                ('model',\n",
      "                 GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
      "                              param_grid={'C': (0.5, 1.0, 2.0),\n",
      "                                          'max_iter': [1000],\n",
      "                                          'solver': ['lbfgs']}))])\n",
      "2023-01-20 02:13:23,190 | root | INFO | Users's init has completed successfully\n",
      "2023-01-20 02:13:23,198 | root | INFO | Swaggers are prepared for the following versions: [2, 3].\n",
      "2023-01-20 02:13:23,198 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2023-01-20 02:13:23,200 | root | INFO | AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\n",
      "2023-01-20 02:13:46,403 | root | INFO | 200\n",
      "127.0.0.1 - - [20/Jan/2023:02:13:46 +0000] \"GET /swagger.json HTTP/1.0\" 200 2281 \"-\" \"Go-http-client/1.1\"\n",
      "2023-01-20 02:13:53,556 | root | INFO | 200\n",
      "127.0.0.1 - - [20/Jan/2023:02:13:53 +0000] \"GET /swagger.json HTTP/1.0\" 200 2281 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice\n",
    "\n",
    "- create the data instances to test with\n",
    "- invoke the service endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:13:55.072529Z",
     "iopub.status.busy": "2023-01-20T02:13:55.072090Z",
     "iopub.status.idle": "2023-01-20T02:13:55.091430Z",
     "shell.execute_reply": "2023-01-20T02:13:55.090518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "base_path = '../..'\n",
    "all_data_file = f\"{base_path}/datasets/german_credit_eval_multiclass_encoded.csv\"\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "X_test = df.drop('outcome',axis=1).values\n",
    "\n",
    "sample_input = json.dumps({\n",
    "\"payload\": {\n",
    "    \"instances\": \n",
    "        X_test[:10].tolist()\n",
    "}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:13:55.096272Z",
     "iopub.status.busy": "2023-01-20T02:13:55.095834Z",
     "iopub.status.idle": "2023-01-20T02:13:55.654749Z",
     "shell.execute_reply": "2023-01-20T02:13:55.653444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0:00:00.549564\n",
      "{'payload': {'predictions': [1, 2, 1, 1, 1, 1, 1, 1, 1, 1]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "          }\n",
    "\n",
    "response = requests.post(\n",
    "    service.scoring_uri, data=sample_input, headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.elapsed)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:13:55.660760Z",
     "iopub.status.busy": "2023-01-20T02:13:55.660268Z",
     "iopub.status.idle": "2023-01-20T02:13:55.755109Z",
     "shell.execute_reply": "2023-01-20T02:13:55.753814Z"
    }
   },
   "outputs": [],
   "source": [
    "local_scan_definition_file = 'target_encoded_gcredit_multiclass_scan_def.yaml'\n",
    "import yaml\n",
    "\n",
    "with open(local_scan_definition_file) as file:\n",
    "    scan_def = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update scan definition yaml\n",
    "\n",
    "- update the model `predict-endpoint` with the scoring uri of the deployed web service\n",
    "- add the auth header deatils (if auth enabled in service)\n",
    "- save the scan definition yaml for remote scanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:13:55.761299Z",
     "iopub.status.busy": "2023-01-20T02:13:55.760652Z",
     "iopub.status.idle": "2023-01-20T02:13:55.766250Z",
     "shell.execute_reply": "2023-01-20T02:13:55.765402Z"
    }
   },
   "outputs": [],
   "source": [
    "# update the predict-endpoint with scoring uri\n",
    "scan_def['models'][0]['predict_endpoint'] = service.scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:13:55.770890Z",
     "iopub.status.busy": "2023-01-20T02:13:55.770510Z",
     "iopub.status.idle": "2023-01-20T02:13:55.776915Z",
     "shell.execute_reply": "2023-01-20T02:13:55.776161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if web-service auth is enabled un-comment the below code snippet'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add header details\n",
    "scan_def['model_headers'] = {}\n",
    "scan_def['model_headers']['default'] = [{'name': 'Content-Type', 'value':'application/json'},\n",
    "                                        {'name': 'accept',        'value':'application/json'} ]\n",
    "\n",
    "'''if web-service auth is enabled un-comment the below code snippet'''\n",
    "\n",
    "\n",
    "# scan_def['model_headers']['defined'] = [{'model_id': 'german_credit_multiclass',\n",
    "#                                          'name': 'Authorization', \n",
    "#                                          'value':'Bearer <INSERT_TOKEN_HERE>'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-20T02:13:55.780375Z",
     "iopub.status.busy": "2023-01-20T02:13:55.780111Z",
     "iopub.status.idle": "2023-01-20T02:13:55.807763Z",
     "shell.execute_reply": "2023-01-20T02:13:55.807086Z"
    }
   },
   "outputs": [],
   "source": [
    "# save yaml to disk\n",
    "with open(local_scan_definition_file, 'w') as file:\n",
    "    yaml.dump(scan_def, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
