{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020. Cognitive Scale Inc. All rights reserved.\n",
    "Licensed under CognitiveScale Example Code [License](https://github.com/CognitiveScale/cortex-certifai-examples/blob/7998b8a481fccd467463deb1fc46d19622079b0e/LICENSE.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Cortex Certifai fairness evaluation on xgboost model to predict adult income\n",
    "\n",
    "- Description: Each dataset row represents the attribute values for de-identified individual. The models predict the income bracket of the person as <=50K or >=50K\n",
    "- Dataset Source: UCI [Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/census+income)\n",
    "- In the example below we show how to create an xgboost model and evaluate fairness using Cortex Certifai\n",
    "- Example can be worked locally by installing the dependencies listed below\n",
    "- dependencies\n",
    "    - python>=3.6.2,<=3.7\n",
    "    - scikit-learn=0.20.3\n",
    "    - xgboost (`conda install -c conda-forge xgboost`)\n",
    "    - numpy=1.16.2\n",
    "    - pandas\n",
    "    - ipython\n",
    "    - matplotlib\n",
    "    - jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neccessary imports\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special import - \n",
    "# for multiprocessing to work in a Notebook,  pickled classes must be in a separate package or notebook\n",
    "# hence, the model encoder(s),decoder class has to be somewhere other than the current notebook\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('.')))\n",
    "from cat_encoder import CatEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into dataframe\n",
    "df = pd.read_csv('adult_income-prepped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate outcome\n",
    "label_column = 'income'\n",
    "y = df[label_column]\n",
    "X_raw = df.drop(label_column, axis=1)\n",
    "\n",
    "# remove some additional non helpful columns\n",
    "rm=[\"fnlwgt\", \"capital-loss\"]\n",
    "dropped_indexes_list = [i for i,col in enumerate(X_raw.columns.to_list()) if col in rm]\n",
    "final_list=X_raw.columns.to_list()\n",
    "for i in rm:   \n",
    "    final_list.remove(i)\n",
    "X = X_raw[final_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test set from the cleaned dataframe(after removing non-useful columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder for categorical columns\n",
    "from cat_encoder import CatEncoder\n",
    "cat_columns = [\n",
    "   'workclass', \n",
    "   'education', \n",
    "   'marital-status', \n",
    "   'occupation', \n",
    "   'relationship',\n",
    "   'race',\n",
    "   'gender',\n",
    "   'native-country'\n",
    "           ]\n",
    "encoder = CatEncoder(cat_columns, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set hyperparams and start model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams for training xgboost model\n",
    "params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode training data to be used to for model training \n",
    "data_dmatrix = xgb.DMatrix(data=encoder(X_train.values),label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the xgboost model\n",
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcuate model accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493192752584706"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy on test-set. using 0.46 as threshold for scoring\n",
    "threshold = 0.46\n",
    "dtest = xgb.DMatrix(encoder(X_test.values))\n",
    "preds = xg_reg.predict(dtest)\n",
    "best_preds = map(lambda x: int(x > threshold), preds)\n",
    "acc = accuracy_score(y_test, list(best_preds))\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cortex certifai updates required before initiating scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrapping model to create xgboost.Dmatrix obj from numpy arrays for certifai predicts\n",
    "\n",
    "- cortex certifai invokes model (black-box) predicts using numpy-arrays from evaluation dataset provided\n",
    "- since xgboost model requires Dmatrix obj for prediction we create a `TransformedPredict` wrapper class\n",
    "- `TransformedPredict` wrapper creates Dmatrix object before returning calling wrapped model's (here xgboost) predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting override_model_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile override_model_predict.py\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "class TransformedPredict:\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "    def predict(self,arr):\n",
    "        dtest = xgb.DMatrix(data=arr)\n",
    "        return self.model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft scoring models additionally need to provide a decoder callable to get outcomes\n",
    "\n",
    "- soft scoring models like xgboost return scores (e.g. probability) that needs to be passed through a threshold filter to get final outcomes\n",
    "- just as we did above to create a threshold to filter binary outcomes for calculating accuracy metrics, we create a `Decoder` class with overridden `__call__` method to add decoding rules for xgboost model scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting decoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile decoder.py\n",
    "import numpy as np\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self,threshold):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        if not isinstance(x, np.ndarray):\n",
    "             x = np.array(x)\n",
    "        return (x > self.threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to verify model predicts with new wrapper model class == model predicts from raw model\n",
    "from decoder import Decoder\n",
    "from override_model_predict import TransformedPredict\n",
    "decoder = Decoder(threshold)\n",
    "transformed_model = TransformedPredict(xg_reg)\n",
    "assert (decoder(transformed_model.predict(encoder(X_test.values))) == \n",
    "        decoder(xg_reg.predict(xgb.DMatrix(encoder(X_test.values))))).all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using cortex certifai scan api's to set up model scanning\n",
    "\n",
    "- before running below section make sure you have necessary packages for cortex certifai installed\n",
    "- copy the toolkit path to `certifai_toolkit_path` variable and run the below cell to install the required packages to initiate a certifai model scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "certifai_toolkit_path = 'path_to_certifai_toolkit'\n",
    "!find $certifai_toolkit_path/packages/all       -type f ! -name \"*console-*\" ! -name \"*client-*\" | xargs -I % sh -c 'pip install % ' ;\n",
    "!find $certifai_toolkit_path/packages/python3.6 -type f   -name \"*engine-*\"                      | xargs -I % sh -c 'pip install % ' ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.13'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check version of certifai installed\n",
    "from certifai.scanner.version import  get_version\n",
    "get_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for building certifai scan\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a CertifaiPredictiorWrapper object from transformed model created above\n",
    "\n",
    "- this predictiorWrapper object will be used by certifai to perform model predicts as constructor to CertifaiModel\n",
    "- run the assert test below to confirm predictions from raw model and certifaiWrapped model are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbg_certifai_wrapped_model = CertifaiPredictorWrapper(transformed_model,encoder=encoder,decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to assert wrapped certifai model predicts == raw model predicts\n",
    "assert (xbg_certifai_wrapped_model.model.predict(X_test.values) == \n",
    "        decoder(xg_reg.predict(xgb.DMatrix(encoder(X_test.values))))).all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a certifai evaluation dataset\n",
    "\n",
    "- earlier we modified our dataset to drop certain non useful columns\n",
    "- and we ran our encoder on the cleaned dataset\n",
    "- we will pass the cleaned dataframe (with the removed columns) to certifai for evaluation\n",
    "- this is needed since the dropped columns are non-encoded and are essentially not required by model for predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned dataframe `X_raw[final_list]` or X\n",
    "dataframe_certifai = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------] 2020-05-29 19:22:35.121020 - 0 of 1 reports (0.0% complete) - Starting scan with model_use_case_id: 'test_user_case' and scan_id: '40a8f16e11d7'\n",
      "[--------------------] 2020-05-29 19:22:35.121231 - 0 of 1 reports (0.0% complete) - Running fairness evaluation for model: XGB\n",
      "[####################] 2020-05-29 19:32:51.242402 - 1 of 1 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "# Create the scan object from scratch using the ScanBuilder class\n",
    "\n",
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='income > 50K', favorable=True),\n",
    "        CertifaiOutcomeValue(0, name='income < 50K')\n",
    "    ]),\n",
    "    prediction_description='Determine whether income greater than 50K or less')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "\n",
    "# Add our local models\n",
    "first_model = CertifaiModel('XGB',\n",
    "                            local_predictor=xbg_certifai_wrapped_model)\n",
    "scan.add_model(first_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.dataframe(dataframe_certifai))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "# Setup an evaluation for fairness on the above dataset using the model\n",
    "# We'll look at disparity between groups defined by marital status and age\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race'))\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('gender'))\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = 'income'\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>type</th>\n",
       "      <th>overall fairness</th>\n",
       "      <th>Feature (gender)</th>\n",
       "      <th>Group details (gender_Female)</th>\n",
       "      <th>Group details (gender_Male)</th>\n",
       "      <th>Feature (race)</th>\n",
       "      <th>Group details (race_Amer-Indian-Eskimo)</th>\n",
       "      <th>Group details (race_Asian-Pac-Islander)</th>\n",
       "      <th>Group details (race_Black)</th>\n",
       "      <th>Group details (race_Other)</th>\n",
       "      <th>Group details (race_White)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB (burden)</th>\n",
       "      <td>XGB</td>\n",
       "      <td>burden</td>\n",
       "      <td>69.203043</td>\n",
       "      <td>69.203043</td>\n",
       "      <td>0.230045</td>\n",
       "      <td>0.121931</td>\n",
       "      <td>92.218841</td>\n",
       "      <td>0.1799</td>\n",
       "      <td>0.134161</td>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.180491</td>\n",
       "      <td>0.184763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             context    type  overall fairness  Feature (gender)  \\\n",
       "XGB (burden)     XGB  burden         69.203043         69.203043   \n",
       "\n",
       "              Group details (gender_Female)  Group details (gender_Male)  \\\n",
       "XGB (burden)                       0.230045                     0.121931   \n",
       "\n",
       "              Feature (race)  Group details (race_Amer-Indian-Eskimo)  \\\n",
       "XGB (burden)       92.218841                                   0.1799   \n",
       "\n",
       "              Group details (race_Asian-Pac-Islander)  \\\n",
       "XGB (burden)                                 0.134161   \n",
       "\n",
       "              Group details (race_Black)  Group details (race_Other)  \\\n",
       "XGB (burden)                      0.1916                    0.180491   \n",
       "\n",
       "              Group details (race_White)  \n",
       "XGB (burden)                    0.184763  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = construct_scores_dataframe(scores('fairness', result), include_confidence=False)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
